wandb: Currently logged in as: hbgallella to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /media/backup_16TB/sean/VJEPA/jepa/wandb/run-20250601_112848-up6z6vd6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run voxel-jepa
wandb: ⭐️ View project at https://wandb.ai/hbgallella/voxel-jepa-pretraining
wandb: 🚀 View run at https://wandb.ai/hbgallella/voxel-jepa-pretraining/runs/up6z6vd6
/media/backup_16TB/sean/VJEPA/jepa/app/vjepa/utils.py:209: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler() if mixed_precision else None
/media/backup_16TB/sean/VJEPA/jepa/app/vjepa/train_v2.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(dtype=dtype, enabled=mixed_precision):
/home/sean/anaconda3/envs/jepa/lib/python3.9/contextlib.py:87: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
wandb: uploading wandb-summary.json; uploading output.log; uploading config.yaml
wandb: uploading output.log
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:  enc-grad-norm ▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          epoch ▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▇▇▇▇▇████
wandb:   gpu-time(ms) ▁▁▂▁▁▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂█▂▂▃▃▂▂▃▂▂▁▂▂▂▂▂▃▂▃▃
wandb:            itr ▅▇█▁▃▆▇▂▄▇▁▅▆▇▄▆▆▃▂▄██▆▄▇▄█▃▇█▃▅▂▁▂█▆▅█▆
wandb:           loss ▇▇▇▆▇▂▂▁▅▂▃▃▃▃▁▂▂▃▃▂▃▃▄▇▆▅▆▆▅▆▇▆▆▇█▆▆▆▅▇
wandb:      loss-jepa █▇▆▆▆▆▄▃▂▂▂▁▃▃▃▃▃▂▃▄▄▃▃▃▅▅▆▇█▅▆▅▆▅▆▇▇▅▆▆
wandb: pred-grad-norm ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▅▄▆▄▃▃▃▃▄▄▃▃▄▅▅▄▆▇▅▆▆▇█
wandb:       reg-loss █▁▁▃▄▆▆▄▅▄▅▄▅▅▃▃▃▄▄▃▁▃▂▅▄▅▆▆▆▅▅▄▄▄▃▄▄▄▄▃
wandb:  wall-time(ms) ▁▂▁▁▂▂▂▁▁▂▂▁▁▂▂▁▁▂▂▁▂▂▂▂▁▂▂█▂▂▂▁▂▂▁▁▁▁▂▂
wandb: 
wandb: Run summary:
wandb:  enc-grad-norm 0.26447
wandb:          epoch 150
wandb:   gpu-time(ms) 352.77423
wandb:            itr 1265
wandb:           loss 0.16786
wandb:      loss-jepa 0.16786
wandb: pred-grad-norm 0.36262
wandb:       reg-loss 0.73438
wandb:  wall-time(ms) 357.51891
wandb: 
wandb: 🚀 View run voxel-jepa at: https://wandb.ai/hbgallella/voxel-jepa-pretraining/runs/up6z6vd6
wandb: ⭐️ View project at: https://wandb.ai/hbgallella/voxel-jepa-pretraining
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250601_112848-up6z6vd6/logs
[rank0]:[W602 06:25:56.192577918 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
