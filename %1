wandb: Currently logged in as: hbgallella to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /media/backup_16TB/sean/VJEPA/jepa/wandb/run-20250601_112848-up6z6vd6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run voxel-jepa
wandb: â­ï¸ View project at https://wandb.ai/hbgallella/voxel-jepa-pretraining
wandb: ğŸš€ View run at https://wandb.ai/hbgallella/voxel-jepa-pretraining/runs/up6z6vd6
/media/backup_16TB/sean/VJEPA/jepa/app/vjepa/utils.py:209: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler() if mixed_precision else None
/media/backup_16TB/sean/VJEPA/jepa/app/vjepa/train_v2.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(dtype=dtype, enabled=mixed_precision):
/home/sean/anaconda3/envs/jepa/lib/python3.9/contextlib.py:87: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
wandb: uploading wandb-summary.json; uploading output.log; uploading config.yaml
wandb: uploading output.log
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:  enc-grad-norm â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   gpu-time(ms) â–â–â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–ˆâ–‚â–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–ƒ
wandb:            itr â–…â–‡â–ˆâ–â–ƒâ–†â–‡â–‚â–„â–‡â–â–…â–†â–‡â–„â–†â–†â–ƒâ–‚â–„â–ˆâ–ˆâ–†â–„â–‡â–„â–ˆâ–ƒâ–‡â–ˆâ–ƒâ–…â–‚â–â–‚â–ˆâ–†â–…â–ˆâ–†
wandb:           loss â–‡â–‡â–‡â–†â–‡â–‚â–‚â–â–…â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–â–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–„â–‡â–†â–…â–†â–†â–…â–†â–‡â–†â–†â–‡â–ˆâ–†â–†â–†â–…â–‡
wandb:      loss-jepa â–ˆâ–‡â–†â–†â–†â–†â–„â–ƒâ–‚â–‚â–‚â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–…â–…â–†â–‡â–ˆâ–…â–†â–…â–†â–…â–†â–‡â–‡â–…â–†â–†
wandb: pred-grad-norm â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–„â–…â–„â–†â–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–„â–…â–…â–„â–†â–‡â–…â–†â–†â–‡â–ˆ
wandb:       reg-loss â–ˆâ–â–â–ƒâ–„â–†â–†â–„â–…â–„â–…â–„â–…â–…â–ƒâ–ƒâ–ƒâ–„â–„â–ƒâ–â–ƒâ–‚â–…â–„â–…â–†â–†â–†â–…â–…â–„â–„â–„â–ƒâ–„â–„â–„â–„â–ƒ
wandb:  wall-time(ms) â–â–‚â–â–â–‚â–‚â–‚â–â–â–‚â–‚â–â–â–‚â–‚â–â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–ˆâ–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–‚â–‚
wandb: 
wandb: Run summary:
wandb:  enc-grad-norm 0.26447
wandb:          epoch 150
wandb:   gpu-time(ms) 352.77423
wandb:            itr 1265
wandb:           loss 0.16786
wandb:      loss-jepa 0.16786
wandb: pred-grad-norm 0.36262
wandb:       reg-loss 0.73438
wandb:  wall-time(ms) 357.51891
wandb: 
wandb: ğŸš€ View run voxel-jepa at: https://wandb.ai/hbgallella/voxel-jepa-pretraining/runs/up6z6vd6
wandb: â­ï¸ View project at: https://wandb.ai/hbgallella/voxel-jepa-pretraining
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250601_112848-up6z6vd6/logs
[rank0]:[W602 06:25:56.192577918 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
