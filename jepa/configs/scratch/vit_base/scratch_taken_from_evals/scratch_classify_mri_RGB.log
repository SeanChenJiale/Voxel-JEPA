Port 42005 is in use. Picking a free port automatically.
51243
INFO:root:called-params /media/backup_16TB/sean/VJEPA/jepa/configs/scratch/vit_base/scratch_taken_from_evals/scratch_classify_mri_RGB.yaml
INFO:root:loaded params...
{   'data': {   'dataset_train': '/media/backup_16TB/sean/VJEPA/jepa/configs/evals_a6000_csv/eval_AD/k400_AD_train.csv',
                'dataset_type': 'VideoDataset',
                'dataset_val': '/media/backup_16TB/sean/VJEPA/jepa/configs/evals_a6000_csv/eval_AD/k400_AD_test.csv',
                'frame_step': 4,
                'frames_per_clip': 16,
                'num_classes': 2,
                'num_segments': 1,
                'num_views_per_segment': 1,
                'num_workers': 16},
    'data_aug': {   'auto_augment': False,
                    'motion_shift': False,
                    'random_resize_aspect_ratio': [0.75, 1.35],
                    'random_resize_scale': [0.3, 1],
                    'reprob': 0.0},
    'eval_name': 'video_classification_frozen',
    'logging': {   'project': 'voxel-jepa-ad-bind-base-scratch',
                   'run_name': 'K400_RGB'},
    'nodes': 1,
    'optimization': {   'attend_across_segments': False,
                        'batch_size': 20,
                        'final_lr': 0.0,
                        'lr': 0.0005,
                        'num_epochs': 100,
                        'resolution': 224,
                        'start_lr': 0.0005,
                        'use_bfloat16': True,
                        'warmup': 0.0,
                        'weight_decay': 0.005},
    'port': 42005,
    'pretrain': {   'checkpoint': 'pass',
                    'checkpoint_key': 'target_encoder',
                    'clip_duration': None,
                    'folder': '/media/backup_16TB/sean/VJEPA/a6000_output/vit_base/K400/scratch',
                    'frames_per_clip': 16,
                    'model_name': 'vit_base',
                    'patch_size': 16,
                    'strategy': 'skip_1',
                    'tight_silu': False,
                    'tubelet_size': 2,
                    'uniform_power': True,
                    'use_sdpa': True,
                    'use_silu': False,
                    'write_tag': 'K400_RGB'},
    'resume_checkpoint': False,
    'tag': 'vit_base_classify_mri_rgb',
    'tasks_per_node': 1}
INFO:root:Running... (rank: 0/1)
INFO:root:Running evaluation: video_classification_frozen
========================
========================
IN CLASSIFICATION FINE TUNING
========================
========================
INFO:root:Initialized (rank/world-size) 0/1
> /media/backup_16TB/sean/VJEPA/jepa/app_scratch/video_classification_frozen/eval.py(236)main()
-> for name, param in encoder.named_parameters():
(Pdb) pos_embed with shape torch.Size([1, 1568, 768])
init_data: VideoDataset
INFO:root:VideoDataset dataset created
INFO:root:VideoDataset unsupervised data loader created
init_data: VideoDataset
INFO:root:VideoDataset dataset created
INFO:root:VideoDataset unsupervised data loader created
INFO:root:Dataloader created... iterations per epoch: 325
Encoder: 149 total params, 148 trainable, All requires_grad=True: False
Non-trainable params (first 5): [('model.pos_embed', torch.Size([1, 1568, 768]))]
classifier: 17 total params, 17 trainable, All requires_grad=True: True
INFO:root:Using AdamW
> /media/backup_16TB/sean/VJEPA/jepa/app_scratch/video_classification_frozen/eval.py(348)main()
-> def save_encoder_checkpoint(epoch):
(Pdb) INFO:root:Epoch 1
NaN or Inf in gradient of model.patch_embed.proj.weight
NaN or Inf in gradient of model.patch_embed.proj.bias
NaN or Inf in gradient of model.blocks.0.norm1.weight
NaN or Inf in gradient of model.blocks.0.norm1.bias
NaN or Inf in gradient of model.blocks.0.attn.qkv.weight
NaN or Inf in gradient of model.blocks.0.attn.qkv.bias
NaN or Inf in gradient of model.blocks.0.attn.proj.weight
NaN or Inf in gradient of model.blocks.0.attn.proj.bias
NaN or Inf in gradient of model.blocks.0.norm2.weight
NaN or Inf in gradient of model.blocks.0.norm2.bias
NaN or Inf in gradient of model.blocks.0.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.0.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.0.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.0.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.1.norm1.weight
NaN or Inf in gradient of model.blocks.1.norm1.bias
NaN or Inf in gradient of model.blocks.1.attn.qkv.weight
NaN or Inf in gradient of model.blocks.1.attn.qkv.bias
NaN or Inf in gradient of model.blocks.1.attn.proj.weight
NaN or Inf in gradient of model.blocks.1.attn.proj.bias
NaN or Inf in gradient of model.blocks.1.norm2.weight
NaN or Inf in gradient of model.blocks.1.norm2.bias
NaN or Inf in gradient of model.blocks.1.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.1.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.1.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.1.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.2.norm1.weight
NaN or Inf in gradient of model.blocks.2.norm1.bias
NaN or Inf in gradient of model.blocks.2.attn.qkv.weight
NaN or Inf in gradient of model.blocks.2.attn.qkv.bias
NaN or Inf in gradient of model.blocks.2.attn.proj.weight
NaN or Inf in gradient of model.blocks.2.attn.proj.bias
NaN or Inf in gradient of model.blocks.2.norm2.weight
NaN or Inf in gradient of model.blocks.2.norm2.bias
NaN or Inf in gradient of model.blocks.2.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.2.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.2.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.2.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.3.norm1.weight
NaN or Inf in gradient of model.blocks.3.norm1.bias
NaN or Inf in gradient of model.blocks.3.attn.qkv.weight
NaN or Inf in gradient of model.blocks.3.attn.qkv.bias
NaN or Inf in gradient of model.blocks.3.attn.proj.weight
NaN or Inf in gradient of model.blocks.3.attn.proj.bias
NaN or Inf in gradient of model.blocks.3.norm2.weight
NaN or Inf in gradient of model.blocks.3.norm2.bias
NaN or Inf in gradient of model.blocks.3.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.3.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.3.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.3.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.4.norm1.weight
NaN or Inf in gradient of model.blocks.4.norm1.bias
NaN or Inf in gradient of model.blocks.4.attn.qkv.weight
NaN or Inf in gradient of model.blocks.4.attn.qkv.bias
NaN or Inf in gradient of model.blocks.4.attn.proj.weight
NaN or Inf in gradient of model.blocks.4.attn.proj.bias
NaN or Inf in gradient of model.blocks.4.norm2.weight
NaN or Inf in gradient of model.blocks.4.norm2.bias
NaN or Inf in gradient of model.blocks.4.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.4.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.4.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.4.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.5.norm1.weight
NaN or Inf in gradient of model.blocks.5.norm1.bias
NaN or Inf in gradient of model.blocks.5.attn.qkv.weight
NaN or Inf in gradient of model.blocks.5.attn.qkv.bias
NaN or Inf in gradient of model.blocks.5.attn.proj.weight
NaN or Inf in gradient of model.blocks.5.attn.proj.bias
NaN or Inf in gradient of model.blocks.5.norm2.weight
NaN or Inf in gradient of model.blocks.5.norm2.bias
NaN or Inf in gradient of model.blocks.5.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.5.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.5.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.5.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.6.norm1.weight
NaN or Inf in gradient of model.blocks.6.norm1.bias
NaN or Inf in gradient of model.blocks.6.attn.qkv.weight
NaN or Inf in gradient of model.blocks.6.attn.qkv.bias
NaN or Inf in gradient of model.blocks.6.attn.proj.weight
NaN or Inf in gradient of model.blocks.6.attn.proj.bias
NaN or Inf in gradient of model.blocks.6.norm2.weight
NaN or Inf in gradient of model.blocks.6.norm2.bias
NaN or Inf in gradient of model.blocks.6.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.6.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.6.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.6.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.7.norm1.weight
NaN or Inf in gradient of model.blocks.7.norm1.bias
NaN or Inf in gradient of model.blocks.7.attn.qkv.weight
NaN or Inf in gradient of model.blocks.7.attn.qkv.bias
NaN or Inf in gradient of model.blocks.7.attn.proj.weight
NaN or Inf in gradient of model.blocks.7.attn.proj.bias
NaN or Inf in gradient of model.blocks.7.norm2.weight
NaN or Inf in gradient of model.blocks.7.norm2.bias
NaN or Inf in gradient of model.blocks.7.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.7.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.7.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.7.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.8.norm1.weight
NaN or Inf in gradient of model.blocks.8.norm1.bias
NaN or Inf in gradient of model.blocks.8.attn.qkv.weight
NaN or Inf in gradient of model.blocks.8.attn.qkv.bias
NaN or Inf in gradient of model.blocks.8.attn.proj.weight
NaN or Inf in gradient of model.blocks.8.attn.proj.bias
NaN or Inf in gradient of model.blocks.8.norm2.weight
NaN or Inf in gradient of model.blocks.8.norm2.bias
NaN or Inf in gradient of model.blocks.8.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.8.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.8.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.8.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.9.norm1.weight
NaN or Inf in gradient of model.blocks.9.norm1.bias
NaN or Inf in gradient of model.blocks.9.attn.qkv.weight
NaN or Inf in gradient of model.blocks.9.attn.qkv.bias
NaN or Inf in gradient of model.blocks.9.attn.proj.weight
NaN or Inf in gradient of model.blocks.9.attn.proj.bias
NaN or Inf in gradient of model.blocks.9.norm2.weight
NaN or Inf in gradient of model.blocks.9.norm2.bias
NaN or Inf in gradient of model.blocks.9.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.9.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.9.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.9.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.10.norm1.weight
NaN or Inf in gradient of model.blocks.10.norm1.bias
NaN or Inf in gradient of model.blocks.10.attn.qkv.weight
NaN or Inf in gradient of model.blocks.10.attn.qkv.bias
NaN or Inf in gradient of model.blocks.10.attn.proj.weight
NaN or Inf in gradient of model.blocks.10.attn.proj.bias
NaN or Inf in gradient of model.blocks.10.norm2.weight
NaN or Inf in gradient of model.blocks.10.norm2.bias
NaN or Inf in gradient of model.blocks.10.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.10.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.10.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.10.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.11.norm1.weight
NaN or Inf in gradient of model.blocks.11.norm1.bias
NaN or Inf in gradient of model.blocks.11.attn.qkv.weight
NaN or Inf in gradient of model.blocks.11.attn.qkv.bias
NaN or Inf in gradient of model.blocks.11.attn.proj.weight
NaN or Inf in gradient of model.blocks.11.attn.proj.bias
NaN or Inf in gradient of model.blocks.11.norm2.weight
NaN or Inf in gradient of model.blocks.11.norm2.bias
NaN or Inf in gradient of model.blocks.11.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.11.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.11.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.11.mlp.fc2.bias
NaN or Inf in gradient of model.norm.weight
NaN or Inf in gradient of model.norm.bias
NaN or Inf in gradient of module.pooler.query_tokens
NaN or Inf in gradient of module.pooler.cross_attention_block.norm1.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.norm1.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.q.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.q.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.kv.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.kv.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.norm2.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.norm2.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc1.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc1.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc2.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc2.bias
NaN or Inf in gradient of module.linear.weight
NaN or Inf in gradient of module.linear.bias
INFO:root:[    0] 50.000% (loss: 0.726) [mem: 1.17e+04]
INFO:root:[   20] 62.381% (loss: 0.965) [mem: 1.24e+04]
INFO:root:[   40] 65.610% (loss: 0.816) [mem: 1.24e+04]
INFO:root:[   60] 66.230% (loss: 0.796) [mem: 1.24e+04]
INFO:root:[   80] 69.383% (loss: 0.735) [mem: 1.24e+04]
INFO:root:[  100] 71.485% (loss: 0.683) [mem: 1.24e+04]
INFO:root:[  120] 73.306% (loss: 0.646) [mem: 1.24e+04]
INFO:root:[  140] 74.610% (loss: 0.617) [mem: 1.24e+04]
INFO:root:[  160] 76.118% (loss: 0.584) [mem: 1.24e+04]
INFO:root:[  180] 77.597% (loss: 0.555) [mem: 1.24e+04]
INFO:root:[  200] 78.756% (loss: 0.528) [mem: 1.24e+04]
INFO:root:[  220] 79.457% (loss: 0.515) [mem: 1.24e+04]
INFO:root:[  240] 79.979% (loss: 0.500) [mem: 1.24e+04]
INFO:root:[  260] 80.747% (loss: 0.483) [mem: 1.24e+04]
INFO:root:[  280] 81.566% (loss: 0.465) [mem: 1.24e+04]
INFO:root:[  300] 82.392% (loss: 0.462) [mem: 1.24e+04]
INFO:root:[  320] 83.131% (loss: 0.446) [mem: 1.24e+04]
INFO:root:[    0] 95.000% (loss: 0.226) [mem: 1.24e+04]
INFO:root:[   20] 94.762% (loss: 0.192) [mem: 2.30e+04]
INFO:root:[   40] 95.244% (loss: 0.168) [mem: 2.30e+04]
INFO:root:[   60] 94.836% (loss: 0.181) [mem: 2.30e+04]
INFO:root:[   80] 95.309% (loss: 0.161) [mem: 2.30e+04]
INFO:root:[    1] train: 83.275% test: 95.417% (loss: 0.443, grad_norm: nan)
Epoch 1: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 2
INFO:root:[    0] 90.000% (loss: 0.296) [mem: 2.30e+04]
INFO:root:[   20] 93.571% (loss: 0.171) [mem: 2.30e+04]
INFO:root:[   40] 92.805% (loss: 0.213) [mem: 2.30e+04]
INFO:root:[   60] 93.197% (loss: 0.208) [mem: 2.30e+04]
INFO:root:[   80] 93.827% (loss: 0.194) [mem: 2.30e+04]
INFO:root:[  100] 93.465% (loss: 0.209) [mem: 2.30e+04]
INFO:root:[  120] 93.595% (loss: 0.209) [mem: 2.30e+04]
INFO:root:[  140] 93.759% (loss: 0.199) [mem: 2.30e+04]
INFO:root:[  160] 93.975% (loss: 0.195) [mem: 2.30e+04]
INFO:root:[  180] 94.199% (loss: 0.194) [mem: 2.30e+04]
INFO:root:[  200] 94.154% (loss: 0.195) [mem: 2.30e+04]
INFO:root:[  220] 94.525% (loss: 0.190) [mem: 2.30e+04]
INFO:root:[  240] 94.772% (loss: 0.198) [mem: 2.30e+04]
INFO:root:[  260] 94.808% (loss: 0.214) [mem: 2.30e+04]
INFO:root:[  280] 95.053% (loss: 0.215) [mem: 2.30e+04]
INFO:root:[  300] 95.100% (loss: 0.229) [mem: 2.30e+04]
INFO:root:[  320] 95.296% (loss: 0.226) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.048% (loss: 0.132) [mem: 2.30e+04]
INFO:root:[   40] 99.146% (loss: 0.117) [mem: 2.30e+04]
INFO:root:[   60] 99.180% (loss: 0.087) [mem: 2.30e+04]
INFO:root:[   80] 99.074% (loss: 0.094) [mem: 2.30e+04]
INFO:root:[    2] train: 95.354% test: 99.107% (loss: 0.223, grad_norm: 97881.844)
Epoch 2: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 3
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 96.667% (loss: 0.508) [mem: 2.30e+04]
INFO:root:[   40] 97.439% (loss: 0.324) [mem: 2.30e+04]
INFO:root:[   60] 97.541% (loss: 0.287) [mem: 2.30e+04]
INFO:root:[   80] 97.284% (loss: 0.317) [mem: 2.30e+04]
INFO:root:[  100] 97.525% (loss: 0.263) [mem: 2.30e+04]
INFO:root:[  120] 97.521% (loss: 0.291) [mem: 2.30e+04]
INFO:root:[  140] 97.553% (loss: 0.284) [mem: 2.30e+04]
INFO:root:[  160] 97.826% (loss: 0.252) [mem: 2.30e+04]
NaN or Inf in gradient of module.linear.weight
INFO:root:[  180] 97.459% (loss: 0.309) [mem: 2.30e+04]
INFO:root:[  200] 97.239% (loss: 0.312) [mem: 2.30e+04]
INFO:root:[  220] 97.127% (loss: 0.298) [mem: 2.30e+04]
INFO:root:[  240] 96.929% (loss: 0.306) [mem: 2.30e+04]
INFO:root:[  260] 96.916% (loss: 0.293) [mem: 2.30e+04]
INFO:root:[  280] 96.904% (loss: 0.303) [mem: 2.30e+04]
INFO:root:[  300] 96.960% (loss: 0.300) [mem: 2.30e+04]
INFO:root:[  320] 96.947% (loss: 0.295) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.524% (loss: 0.047) [mem: 2.30e+04]
INFO:root:[   40] 99.634% (loss: 0.046) [mem: 2.30e+04]
INFO:root:[   60] 99.508% (loss: 0.048) [mem: 2.30e+04]
INFO:root:[   80] 99.383% (loss: 0.044) [mem: 2.30e+04]
INFO:root:[    3] train: 96.953% test: 99.405% (loss: 0.296, grad_norm: inf)
Epoch 3: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 4
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 92.857% (loss: 0.597) [mem: 2.30e+04]
INFO:root:[   40] 94.634% (loss: 0.365) [mem: 2.30e+04]
INFO:root:[   60] 95.574% (loss: 0.349) [mem: 2.30e+04]
INFO:root:[   80] 95.741% (loss: 0.341) [mem: 2.30e+04]
INFO:root:[  100] 96.337% (loss: 0.297) [mem: 2.30e+04]
INFO:root:[  120] 96.570% (loss: 0.276) [mem: 2.30e+04]
INFO:root:[  140] 96.844% (loss: 0.254) [mem: 2.30e+04]
INFO:root:[  160] 97.081% (loss: 0.233) [mem: 2.30e+04]
INFO:root:[  180] 97.099% (loss: 0.256) [mem: 2.30e+04]
INFO:root:[  200] 97.090% (loss: 0.259) [mem: 2.30e+04]
INFO:root:[  220] 97.036% (loss: 0.256) [mem: 2.30e+04]
INFO:root:[  240] 96.992% (loss: 0.261) [mem: 2.30e+04]
INFO:root:[  260] 97.146% (loss: 0.248) [mem: 2.30e+04]
INFO:root:[  280] 97.242% (loss: 0.242) [mem: 2.30e+04]
INFO:root:[  300] 97.093% (loss: 0.260) [mem: 2.30e+04]
INFO:root:[  320] 97.118% (loss: 0.254) [mem: 2.30e+04]
INFO:root:[    0] 95.000% (loss: 0.646) [mem: 2.30e+04]
INFO:root:[   20] 94.524% (loss: 0.671) [mem: 2.30e+04]
INFO:root:[   40] 94.878% (loss: 0.646) [mem: 2.30e+04]
INFO:root:[   60] 94.918% (loss: 0.648) [mem: 2.30e+04]
INFO:root:[   80] 95.494% (loss: 0.598) [mem: 2.30e+04]
INFO:root:[    4] train: 97.138% test: 95.655% (loss: 0.254, grad_norm: 66512.695)
Epoch 4: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 5
INFO:root:[    0] 95.000% (loss: 0.655) [mem: 2.30e+04]
INFO:root:[   20] 96.905% (loss: 0.382) [mem: 2.30e+04]
INFO:root:[   40] 96.707% (loss: 0.420) [mem: 2.30e+04]
INFO:root:[   60] 97.377% (loss: 0.317) [mem: 2.30e+04]
INFO:root:[   80] 97.346% (loss: 0.323) [mem: 2.30e+04]
INFO:root:[  100] 97.129% (loss: 0.327) [mem: 2.30e+04]
INFO:root:[  120] 97.025% (loss: 0.328) [mem: 2.30e+04]
INFO:root:[  140] 97.092% (loss: 0.319) [mem: 2.30e+04]
INFO:root:[  160] 97.205% (loss: 0.307) [mem: 2.30e+04]
INFO:root:[  180] 97.293% (loss: 0.291) [mem: 2.30e+04]
INFO:root:[  200] 97.338% (loss: 0.287) [mem: 2.30e+04]
INFO:root:[  220] 97.466% (loss: 0.268) [mem: 2.30e+04]
INFO:root:[  240] 97.427% (loss: 0.268) [mem: 2.30e+04]
INFO:root:[  260] 97.433% (loss: 0.279) [mem: 2.30e+04]
INFO:root:[  280] 97.527% (loss: 0.268) [mem: 2.30e+04]
INFO:root:[  300] 97.575% (loss: 0.261) [mem: 2.30e+04]
INFO:root:[  320] 97.586% (loss: 0.260) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.286% (loss: 0.049) [mem: 2.30e+04]
INFO:root:[   40] 99.512% (loss: 0.033) [mem: 2.30e+04]
INFO:root:[   60] 99.262% (loss: 0.052) [mem: 2.30e+04]
INFO:root:[   80] 99.321% (loss: 0.049) [mem: 2.30e+04]
INFO:root:[    5] train: 97.599% test: 99.345% (loss: 0.259, grad_norm: 104785.883)
Epoch 5: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 6
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 97.619% (loss: 0.252) [mem: 2.30e+04]
INFO:root:[   40] 98.171% (loss: 0.174) [mem: 2.30e+04]
INFO:root:[   60] 97.951% (loss: 0.205) [mem: 2.30e+04]
INFO:root:[   80] 97.778% (loss: 0.218) [mem: 2.30e+04]
INFO:root:[  100] 97.822% (loss: 0.220) [mem: 2.30e+04]
INFO:root:[  120] 97.603% (loss: 0.240) [mem: 2.30e+04]
INFO:root:[  140] 97.447% (loss: 0.234) [mem: 2.30e+04]
INFO:root:[  160] 97.422% (loss: 0.241) [mem: 2.30e+04]
INFO:root:[  180] 97.541% (loss: 0.235) [mem: 2.30e+04]
INFO:root:[  200] 97.587% (loss: 0.236) [mem: 2.30e+04]
INFO:root:[  220] 97.602% (loss: 0.234) [mem: 2.30e+04]
INFO:root:[  240] 97.739% (loss: 0.225) [mem: 2.30e+04]
INFO:root:[  260] 97.797% (loss: 0.224) [mem: 2.30e+04]
INFO:root:[  280] 97.865% (loss: 0.220) [mem: 2.30e+04]
INFO:root:[  300] 97.874% (loss: 0.223) [mem: 2.30e+04]
INFO:root:[  320] 97.944% (loss: 0.217) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 98.810% (loss: 0.116) [mem: 2.30e+04]
INFO:root:[   40] 98.902% (loss: 0.135) [mem: 2.30e+04]
INFO:root:[   60] 98.770% (loss: 0.136) [mem: 2.30e+04]
INFO:root:[   80] 98.889% (loss: 0.119) [mem: 2.30e+04]
INFO:root:[    6] train: 97.969% test: 98.929% (loss: 0.215, grad_norm: 64397.906)
Epoch 6: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 7
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 98.095% (loss: 0.255) [mem: 2.30e+04]
INFO:root:[   40] 97.561% (loss: 0.253) [mem: 2.30e+04]
INFO:root:[   60] 97.377% (loss: 0.263) [mem: 2.30e+04]
INFO:root:[   80] 97.716% (loss: 0.221) [mem: 2.30e+04]
INFO:root:[  100] 97.574% (loss: 0.229) [mem: 2.30e+04]
INFO:root:[  120] 97.107% (loss: 0.265) [mem: 2.30e+04]
INFO:root:[  140] 97.482% (loss: 0.233) [mem: 2.30e+04]
INFO:root:[  160] 97.391% (loss: 0.248) [mem: 2.30e+04]
INFO:root:[  180] 97.569% (loss: 0.226) [mem: 2.30e+04]
INFO:root:[  200] 97.736% (loss: 0.212) [mem: 2.30e+04]
INFO:root:[  220] 97.805% (loss: 0.205) [mem: 2.30e+04]
INFO:root:[  240] 97.884% (loss: 0.201) [mem: 2.30e+04]
INFO:root:[  260] 97.989% (loss: 0.192) [mem: 2.30e+04]
INFO:root:[  280] 98.025% (loss: 0.198) [mem: 2.30e+04]
INFO:root:[  300] 98.040% (loss: 0.195) [mem: 2.30e+04]
INFO:root:[  320] 98.115% (loss: 0.189) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.048% (loss: 0.141) [mem: 2.30e+04]
INFO:root:[   40] 98.780% (loss: 0.153) [mem: 2.30e+04]
INFO:root:[   60] 98.361% (loss: 0.219) [mem: 2.30e+04]
INFO:root:[   80] 98.519% (loss: 0.200) [mem: 2.30e+04]
INFO:root:[    7] train: 98.138% test: 98.571% (loss: 0.186, grad_norm: 77109.008)
Epoch 7: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 8
INFO:root:[    0] 100.000% (loss: 0.020) [mem: 2.30e+04]
INFO:root:[   20] 98.571% (loss: 0.196) [mem: 2.30e+04]
INFO:root:[   40] 98.780% (loss: 0.131) [mem: 2.30e+04]
INFO:root:[   60] 98.443% (loss: 0.202) [mem: 2.30e+04]
INFO:root:[   80] 98.580% (loss: 0.168) [mem: 2.30e+04]
INFO:root:[  100] 98.762% (loss: 0.144) [mem: 2.30e+04]
INFO:root:[  120] 98.760% (loss: 0.151) [mem: 2.30e+04]
INFO:root:[  140] 98.652% (loss: 0.146) [mem: 2.30e+04]
INFO:root:[  160] 98.665% (loss: 0.154) [mem: 2.30e+04]
INFO:root:[  180] 98.702% (loss: 0.159) [mem: 2.30e+04]
INFO:root:[  200] 98.632% (loss: 0.165) [mem: 2.30e+04]
INFO:root:[  220] 98.597% (loss: 0.166) [mem: 2.30e+04]
INFO:root:[  240] 98.651% (loss: 0.162) [mem: 2.30e+04]
INFO:root:[  260] 98.602% (loss: 0.166) [mem: 2.30e+04]
INFO:root:[  280] 98.630% (loss: 0.164) [mem: 2.30e+04]
INFO:root:[  300] 98.638% (loss: 0.163) [mem: 2.30e+04]
INFO:root:[  320] 98.707% (loss: 0.156) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.524% (loss: 0.057) [mem: 2.30e+04]
INFO:root:[   40] 99.268% (loss: 0.079) [mem: 2.30e+04]
INFO:root:[   60] 98.852% (loss: 0.126) [mem: 2.30e+04]
INFO:root:[   80] 99.012% (loss: 0.109) [mem: 2.30e+04]
INFO:root:[    8] train: 98.707% test: 99.048% (loss: 0.154, grad_norm: 113925.352)
Epoch 8: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 9
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 98.333% (loss: 0.172) [mem: 2.30e+04]
INFO:root:[   40] 98.659% (loss: 0.152) [mem: 2.30e+04]
INFO:root:[   60] 98.115% (loss: 0.246) [mem: 2.30e+04]
INFO:root:[   80] 98.272% (loss: 0.207) [mem: 2.30e+04]
INFO:root:[  100] 97.921% (loss: 0.223) [mem: 2.30e+04]
INFO:root:[  120] 98.017% (loss: 0.202) [mem: 2.30e+04]
INFO:root:[  140] 98.156% (loss: 0.186) [mem: 2.30e+04]
INFO:root:[  160] 98.043% (loss: 0.209) [mem: 2.30e+04]
INFO:root:[  180] 98.011% (loss: 0.220) [mem: 2.30e+04]
INFO:root:[  200] 98.060% (loss: 0.211) [mem: 2.30e+04]
INFO:root:[  220] 98.100% (loss: 0.205) [mem: 2.30e+04]
INFO:root:[  240] 98.195% (loss: 0.199) [mem: 2.30e+04]
NaN or Inf in gradient of model.patch_embed.proj.weight
INFO:root:[  260] 98.199% (loss: 0.198) [mem: 2.30e+04]
INFO:root:[  280] 98.274% (loss: 0.191) [mem: 2.30e+04]
INFO:root:[  300] 98.256% (loss: 0.190) [mem: 2.30e+04]
INFO:root:[  320] 98.255% (loss: 0.195) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   40] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   60] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   80] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[    9] train: 98.262% test: 100.000% (loss: 0.192, grad_norm: inf)
Epoch 9: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 10
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.762% (loss: 0.047) [mem: 2.30e+04]
INFO:root:[   40] 99.512% (loss: 0.059) [mem: 2.30e+04]
INFO:root:[   60] 99.426% (loss: 0.065) [mem: 2.30e+04]
INFO:root:[   80] 98.951% (loss: 0.115) [mem: 2.30e+04]
INFO:root:[  100] 98.713% (loss: 0.143) [mem: 2.30e+04]
INFO:root:[  120] 98.719% (loss: 0.146) [mem: 2.30e+04]
INFO:root:[  140] 98.794% (loss: 0.136) [mem: 2.30e+04]
NaN or Inf in gradient of model.patch_embed.proj.weight
NaN or Inf in gradient of model.blocks.0.attn.qkv.weight
INFO:root:[  160] 98.727% (loss: 0.148) [mem: 2.30e+04]
INFO:root:[  180] 98.591% (loss: 0.153) [mem: 2.30e+04]
INFO:root:[  200] 98.607% (loss: 0.157) [mem: 2.30e+04]
INFO:root:[  220] 98.665% (loss: 0.148) [mem: 2.30e+04]
INFO:root:[  240] 98.444% (loss: 0.174) [mem: 2.30e+04]
INFO:root:[  260] 98.199% (loss: 0.181) [mem: 2.30e+04]
INFO:root:[  280] 97.918% (loss: 0.191) [mem: 2.30e+04]
INFO:root:[  300] 97.824% (loss: 0.189) [mem: 2.30e+04]
INFO:root:[  320] 97.741% (loss: 0.189) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.286% (loss: 0.128) [mem: 2.30e+04]
INFO:root:[   40] 99.024% (loss: 0.123) [mem: 2.30e+04]
INFO:root:[   60] 98.852% (loss: 0.127) [mem: 2.30e+04]
INFO:root:[   80] 98.951% (loss: 0.114) [mem: 2.30e+04]
INFO:root:[   10] train: 97.738% test: 98.988% (loss: 0.191, grad_norm: inf)
Epoch 10: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 11
INFO:root:[    0] 100.000% (loss: 0.023) [mem: 2.30e+04]
INFO:root:[   20] 96.905% (loss: 0.407) [mem: 2.30e+04]
INFO:root:[   40] 94.756% (loss: 0.416) [mem: 2.30e+04]
INFO:root:[   60] 93.607% (loss: 0.377) [mem: 2.30e+04]
INFO:root:[   80] 94.321% (loss: 0.378) [mem: 2.30e+04]
INFO:root:[  100] 94.505% (loss: 0.377) [mem: 2.30e+04]
INFO:root:[  120] 94.959% (loss: 0.341) [mem: 2.30e+04]
INFO:root:[  140] 95.177% (loss: 0.317) [mem: 2.30e+04]
INFO:root:[  160] 95.435% (loss: 0.306) [mem: 2.30e+04]
INFO:root:[  180] 95.635% (loss: 0.302) [mem: 2.30e+04]
NaN or Inf in gradient of model.patch_embed.proj.weight
INFO:root:[  200] 95.597% (loss: 0.328) [mem: 2.30e+04]
INFO:root:[  220] 95.588% (loss: 0.311) [mem: 2.30e+04]
INFO:root:[  240] 95.685% (loss: 0.303) [mem: 2.30e+04]
INFO:root:[  260] 95.383% (loss: 0.346) [mem: 2.30e+04]
INFO:root:[  280] 95.445% (loss: 0.335) [mem: 2.30e+04]
INFO:root:[  300] 95.415% (loss: 0.338) [mem: 2.30e+04]
INFO:root:[  320] 95.374% (loss: 0.339) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.002) [mem: 2.30e+04]
INFO:root:[   20] 98.095% (loss: 0.077) [mem: 2.30e+04]
INFO:root:[   40] 98.293% (loss: 0.071) [mem: 2.30e+04]
INFO:root:[   60] 97.377% (loss: 0.096) [mem: 2.30e+04]
INFO:root:[   80] 97.654% (loss: 0.080) [mem: 2.30e+04]
INFO:root:[   11] train: 95.368% test: 97.738% (loss: 0.337, grad_norm: inf)
Epoch 11: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 12
INFO:root:[    0] 100.000% (loss: 0.019) [mem: 2.30e+04]
INFO:root:[   20] 95.952% (loss: 0.276) [mem: 2.30e+04]
INFO:root:[   40] 96.341% (loss: 0.201) [mem: 2.30e+04]
INFO:root:[   60] 95.246% (loss: 0.237) [mem: 2.30e+04]
INFO:root:[   80] 94.691% (loss: 0.251) [mem: 2.30e+04]
INFO:root:[  100] 94.752% (loss: 0.238) [mem: 2.30e+04]
INFO:root:[  120] 94.504% (loss: 0.260) [mem: 2.30e+04]
INFO:root:[  140] 94.397% (loss: 0.258) [mem: 2.30e+04]
INFO:root:[  160] 94.224% (loss: 0.248) [mem: 2.30e+04]
INFO:root:[  180] 94.088% (loss: 0.251) [mem: 2.30e+04]
INFO:root:[  200] 94.403% (loss: 0.236) [mem: 2.30e+04]
INFO:root:[  220] 94.389% (loss: 0.232) [mem: 2.30e+04]
INFO:root:[  240] 94.564% (loss: 0.227) [mem: 2.30e+04]
INFO:root:[  260] 94.579% (loss: 0.237) [mem: 2.30e+04]
INFO:root:[  280] 94.555% (loss: 0.245) [mem: 2.30e+04]
INFO:root:[  300] 94.502% (loss: 0.242) [mem: 2.30e+04]
INFO:root:[  320] 94.564% (loss: 0.235) [mem: 2.30e+04]
INFO:root:[    0] 95.000% (loss: 0.066) [mem: 2.30e+04]
INFO:root:[   20] 95.476% (loss: 0.162) [mem: 2.30e+04]
INFO:root:[   40] 95.976% (loss: 0.161) [mem: 2.30e+04]
INFO:root:[   60] 95.164% (loss: 0.210) [mem: 2.30e+04]
INFO:root:[   80] 95.432% (loss: 0.197) [mem: 2.30e+04]
INFO:root:[   12] train: 94.583% test: 95.595% (loss: 0.236, grad_norm: 95500.070)
Epoch 12: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 13
INFO:root:[    0] 95.000% (loss: 0.485) [mem: 2.30e+04]
INFO:root:[   20] 94.762% (loss: 0.309) [mem: 2.30e+04]
INFO:root:[   40] 96.220% (loss: 0.211) [mem: 2.30e+04]
INFO:root:[   60] 96.393% (loss: 0.226) [mem: 2.30e+04]
INFO:root:[   80] 96.235% (loss: 0.247) [mem: 2.30e+04]
INFO:root:[  100] 95.000% (loss: 0.310) [mem: 2.30e+04]
INFO:root:[  120] 95.248% (loss: 0.295) [mem: 2.30e+04]
INFO:root:[  140] 95.390% (loss: 0.292) [mem: 2.30e+04]
INFO:root:[  160] 95.342% (loss: 0.284) [mem: 2.30e+04]
INFO:root:[  180] 95.138% (loss: 0.285) [mem: 2.30e+04]
INFO:root:[  200] 95.323% (loss: 0.278) [mem: 2.30e+04]
INFO:root:[  220] 95.271% (loss: 0.284) [mem: 2.30e+04]
INFO:root:[  240] 95.270% (loss: 0.284) [mem: 2.30e+04]
INFO:root:[  260] 95.019% (loss: 0.298) [mem: 2.30e+04]
INFO:root:[  280] 95.107% (loss: 0.287) [mem: 2.30e+04]
INFO:root:[  300] 95.233% (loss: 0.275) [mem: 2.30e+04]
INFO:root:[  320] 95.202% (loss: 0.275) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.004) [mem: 2.30e+04]
INFO:root:[   20] 97.381% (loss: 0.146) [mem: 2.30e+04]
INFO:root:[   40] 97.805% (loss: 0.118) [mem: 2.30e+04]
INFO:root:[   60] 97.787% (loss: 0.119) [mem: 2.30e+04]
INFO:root:[   80] 97.654% (loss: 0.129) [mem: 2.30e+04]
INFO:root:[   13] train: 95.215% test: 97.738% (loss: 0.275, grad_norm: 113348.180)
Epoch 13: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 14
INFO:root:[    0] 80.000% (loss: 1.054) [mem: 2.30e+04]
INFO:root:[   20] 91.905% (loss: 0.573) [mem: 2.30e+04]
INFO:root:[   40] 91.707% (loss: 0.540) [mem: 2.30e+04]
INFO:root:[   60] 93.033% (loss: 0.422) [mem: 2.30e+04]
INFO:root:[   80] 93.827% (loss: 0.346) [mem: 2.30e+04]
INFO:root:[  100] 93.960% (loss: 0.311) [mem: 2.30e+04]
INFO:root:[  120] 94.463% (loss: 0.275) [mem: 2.30e+04]
INFO:root:[  140] 94.858% (loss: 0.251) [mem: 2.30e+04]
INFO:root:[  160] 95.093% (loss: 0.257) [mem: 2.30e+04]
INFO:root:[  180] 94.227% (loss: 0.332) [mem: 2.30e+04]
INFO:root:[  200] 94.453% (loss: 0.311) [mem: 2.30e+04]
INFO:root:[  220] 94.457% (loss: 0.316) [mem: 2.30e+04]
INFO:root:[  240] 94.461% (loss: 0.313) [mem: 2.30e+04]
INFO:root:[  260] 94.579% (loss: 0.303) [mem: 2.30e+04]
INFO:root:[  280] 94.769% (loss: 0.289) [mem: 2.30e+04]
INFO:root:[  300] 94.900% (loss: 0.278) [mem: 2.30e+04]
INFO:root:[  320] 95.000% (loss: 0.269) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.286% (loss: 0.071) [mem: 2.30e+04]
INFO:root:[   40] 99.024% (loss: 0.069) [mem: 2.30e+04]
INFO:root:[   60] 98.279% (loss: 0.112) [mem: 2.30e+04]
INFO:root:[   80] 98.333% (loss: 0.107) [mem: 2.30e+04]
INFO:root:[   14] train: 95.031% test: 98.333% (loss: 0.267, grad_norm: 99614.078)
Epoch 14: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 15
INFO:root:[    0] 85.000% (loss: 0.706) [mem: 2.30e+04]
INFO:root:[   20] 95.000% (loss: 0.367) [mem: 2.30e+04]
INFO:root:[   40] 96.341% (loss: 0.249) [mem: 2.30e+04]
INFO:root:[   60] 96.148% (loss: 0.236) [mem: 2.30e+04]
INFO:root:[   80] 95.926% (loss: 0.261) [mem: 2.30e+04]
INFO:root:[  100] 95.891% (loss: 0.262) [mem: 2.30e+04]
INFO:root:[  120] 95.992% (loss: 0.246) [mem: 2.30e+04]
NaN or Inf in gradient of model.blocks.0.attn.qkv.weight
INFO:root:[  140] 96.099% (loss: 0.240) [mem: 2.30e+04]
INFO:root:[  160] 96.273% (loss: 0.241) [mem: 2.30e+04]
INFO:root:[  180] 96.243% (loss: 0.248) [mem: 2.30e+04]
INFO:root:[  200] 95.597% (loss: 0.282) [mem: 2.30e+04]
INFO:root:[  220] 95.679% (loss: 0.274) [mem: 2.30e+04]
INFO:root:[  240] 95.207% (loss: 0.277) [mem: 2.30e+04]
INFO:root:[  260] 94.789% (loss: 0.277) [mem: 2.30e+04]
INFO:root:[  280] 94.199% (loss: 0.297) [mem: 2.30e+04]
INFO:root:[  300] 94.103% (loss: 0.291) [mem: 2.30e+04]
INFO:root:[  320] 93.925% (loss: 0.297) [mem: 2.30e+04]
INFO:root:[    0] 90.000% (loss: 0.298) [mem: 2.30e+04]
INFO:root:[   20] 95.000% (loss: 0.179) [mem: 2.30e+04]
INFO:root:[   40] 95.854% (loss: 0.148) [mem: 2.30e+04]
INFO:root:[   60] 95.164% (loss: 0.176) [mem: 2.30e+04]
INFO:root:[   80] 95.432% (loss: 0.162) [mem: 2.30e+04]
INFO:root:[   15] train: 93.905% test: 95.595% (loss: 0.295, grad_norm: inf)
Epoch 15: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 16
INFO:root:[    0] 80.000% (loss: 0.927) [mem: 2.30e+04]
INFO:root:[   20] 92.381% (loss: 0.321) [mem: 2.30e+04]
INFO:root:[   40] 93.293% (loss: 0.250) [mem: 2.30e+04]
INFO:root:[   60] 93.115% (loss: 0.230) [mem: 2.30e+04]
INFO:root:[   80] 92.963% (loss: 0.220) [mem: 2.30e+04]
INFO:root:[  100] 92.772% (loss: 0.222) [mem: 2.30e+04]
INFO:root:[  120] 93.182% (loss: 0.209) [mem: 2.30e+04]
INFO:root:[  140] 92.801% (loss: 0.221) [mem: 2.30e+04]
INFO:root:[  160] 93.012% (loss: 0.220) [mem: 2.30e+04]
INFO:root:[  180] 93.315% (loss: 0.213) [mem: 2.30e+04]
INFO:root:[  200] 93.532% (loss: 0.207) [mem: 2.30e+04]
INFO:root:[  220] 93.122% (loss: 0.230) [mem: 2.30e+04]
INFO:root:[  240] 93.278% (loss: 0.230) [mem: 2.30e+04]
INFO:root:[  260] 93.333% (loss: 0.227) [mem: 2.30e+04]
INFO:root:[  280] 93.399% (loss: 0.225) [mem: 2.30e+04]
INFO:root:[  300] 93.422% (loss: 0.223) [mem: 2.30e+04]
INFO:root:[  320] 93.567% (loss: 0.215) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.002) [mem: 2.30e+04]
INFO:root:[   20] 97.619% (loss: 0.077) [mem: 2.30e+04]
INFO:root:[   40] 97.805% (loss: 0.072) [mem: 2.30e+04]
INFO:root:[   60] 97.131% (loss: 0.091) [mem: 2.30e+04]
INFO:root:[   80] 97.284% (loss: 0.089) [mem: 2.30e+04]
INFO:root:[   16] train: 93.551% test: 97.381% (loss: 0.215, grad_norm: 38519.508)
Epoch 16: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 17
INFO:root:[    0] 85.000% (loss: 0.244) [mem: 2.30e+04]
INFO:root:[   20] 94.524% (loss: 0.199) [mem: 2.30e+04]
INFO:root:[   40] 96.585% (loss: 0.210) [mem: 2.30e+04]
INFO:root:[   60] 95.574% (loss: 0.283) [mem: 2.30e+04]
INFO:root:[   80] 94.444% (loss: 0.348) [mem: 2.30e+04]
INFO:root:[  100] 94.059% (loss: 0.356) [mem: 2.30e+04]
NaN or Inf in gradient of model.patch_embed.proj.weight
NaN or Inf in gradient of model.patch_embed.proj.bias
NaN or Inf in gradient of model.blocks.0.norm1.weight
NaN or Inf in gradient of model.blocks.0.norm1.bias
NaN or Inf in gradient of model.blocks.0.attn.qkv.weight
NaN or Inf in gradient of model.blocks.0.attn.qkv.bias
NaN or Inf in gradient of model.blocks.0.attn.proj.weight
NaN or Inf in gradient of model.blocks.0.attn.proj.bias
NaN or Inf in gradient of model.blocks.0.norm2.weight
NaN or Inf in gradient of model.blocks.0.norm2.bias
NaN or Inf in gradient of model.blocks.0.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.0.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.0.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.0.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.1.norm1.weight
NaN or Inf in gradient of model.blocks.1.norm1.bias
NaN or Inf in gradient of model.blocks.1.attn.qkv.weight
NaN or Inf in gradient of model.blocks.1.attn.qkv.bias
NaN or Inf in gradient of model.blocks.1.attn.proj.weight
NaN or Inf in gradient of model.blocks.1.attn.proj.bias
NaN or Inf in gradient of model.blocks.1.norm2.weight
NaN or Inf in gradient of model.blocks.1.norm2.bias
NaN or Inf in gradient of model.blocks.1.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.1.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.1.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.1.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.2.norm1.weight
NaN or Inf in gradient of model.blocks.2.norm1.bias
NaN or Inf in gradient of model.blocks.2.attn.qkv.weight
NaN or Inf in gradient of model.blocks.2.attn.qkv.bias
NaN or Inf in gradient of model.blocks.2.attn.proj.weight
NaN or Inf in gradient of model.blocks.2.attn.proj.bias
NaN or Inf in gradient of model.blocks.2.norm2.weight
NaN or Inf in gradient of model.blocks.2.norm2.bias
NaN or Inf in gradient of model.blocks.2.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.2.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.2.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.2.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.3.norm1.weight
NaN or Inf in gradient of model.blocks.3.norm1.bias
NaN or Inf in gradient of model.blocks.3.attn.qkv.weight
NaN or Inf in gradient of model.blocks.3.attn.qkv.bias
NaN or Inf in gradient of model.blocks.3.attn.proj.weight
NaN or Inf in gradient of model.blocks.3.attn.proj.bias
NaN or Inf in gradient of model.blocks.3.norm2.weight
NaN or Inf in gradient of model.blocks.3.norm2.bias
NaN or Inf in gradient of model.blocks.3.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.3.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.3.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.3.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.4.norm1.weight
NaN or Inf in gradient of model.blocks.4.norm1.bias
NaN or Inf in gradient of model.blocks.4.attn.qkv.weight
NaN or Inf in gradient of model.blocks.4.attn.qkv.bias
NaN or Inf in gradient of model.blocks.4.attn.proj.weight
NaN or Inf in gradient of model.blocks.4.attn.proj.bias
NaN or Inf in gradient of model.blocks.4.norm2.weight
NaN or Inf in gradient of model.blocks.4.norm2.bias
NaN or Inf in gradient of model.blocks.4.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.4.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.4.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.4.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.5.norm1.weight
NaN or Inf in gradient of model.blocks.5.norm1.bias
NaN or Inf in gradient of model.blocks.5.attn.qkv.weight
NaN or Inf in gradient of model.blocks.5.attn.qkv.bias
NaN or Inf in gradient of model.blocks.5.attn.proj.weight
NaN or Inf in gradient of model.blocks.5.attn.proj.bias
NaN or Inf in gradient of model.blocks.5.norm2.weight
NaN or Inf in gradient of model.blocks.5.norm2.bias
NaN or Inf in gradient of model.blocks.5.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.5.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.5.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.5.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.6.norm1.weight
NaN or Inf in gradient of model.blocks.6.norm1.bias
NaN or Inf in gradient of model.blocks.6.attn.qkv.weight
NaN or Inf in gradient of model.blocks.6.attn.qkv.bias
NaN or Inf in gradient of model.blocks.6.attn.proj.weight
NaN or Inf in gradient of model.blocks.6.attn.proj.bias
NaN or Inf in gradient of model.blocks.6.norm2.weight
NaN or Inf in gradient of model.blocks.6.norm2.bias
NaN or Inf in gradient of model.blocks.6.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.6.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.6.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.6.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.7.norm1.weight
NaN or Inf in gradient of model.blocks.7.norm1.bias
NaN or Inf in gradient of model.blocks.7.attn.qkv.weight
NaN or Inf in gradient of model.blocks.7.attn.qkv.bias
NaN or Inf in gradient of model.blocks.7.attn.proj.weight
NaN or Inf in gradient of model.blocks.7.attn.proj.bias
NaN or Inf in gradient of model.blocks.7.norm2.weight
NaN or Inf in gradient of model.blocks.7.norm2.bias
NaN or Inf in gradient of model.blocks.7.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.7.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.7.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.7.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.8.norm1.weight
NaN or Inf in gradient of model.blocks.8.norm1.bias
NaN or Inf in gradient of model.blocks.8.attn.qkv.weight
NaN or Inf in gradient of model.blocks.8.attn.qkv.bias
NaN or Inf in gradient of model.blocks.8.attn.proj.weight
NaN or Inf in gradient of model.blocks.8.attn.proj.bias
NaN or Inf in gradient of model.blocks.8.norm2.weight
NaN or Inf in gradient of model.blocks.8.norm2.bias
NaN or Inf in gradient of model.blocks.8.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.8.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.8.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.8.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.9.norm1.weight
NaN or Inf in gradient of model.blocks.9.norm1.bias
NaN or Inf in gradient of model.blocks.9.attn.qkv.weight
NaN or Inf in gradient of model.blocks.9.attn.qkv.bias
NaN or Inf in gradient of model.blocks.9.attn.proj.weight
NaN or Inf in gradient of model.blocks.9.attn.proj.bias
NaN or Inf in gradient of model.blocks.9.norm2.weight
NaN or Inf in gradient of model.blocks.9.norm2.bias
NaN or Inf in gradient of model.blocks.9.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.9.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.9.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.9.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.10.norm1.weight
NaN or Inf in gradient of model.blocks.10.norm1.bias
NaN or Inf in gradient of model.blocks.10.attn.qkv.weight
NaN or Inf in gradient of model.blocks.10.attn.qkv.bias
NaN or Inf in gradient of model.blocks.10.attn.proj.weight
NaN or Inf in gradient of model.blocks.10.attn.proj.bias
NaN or Inf in gradient of model.blocks.10.norm2.weight
NaN or Inf in gradient of model.blocks.10.norm2.bias
NaN or Inf in gradient of model.blocks.10.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.10.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.10.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.10.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.11.norm1.weight
NaN or Inf in gradient of model.blocks.11.norm1.bias
NaN or Inf in gradient of model.blocks.11.attn.qkv.weight
NaN or Inf in gradient of model.blocks.11.attn.qkv.bias
NaN or Inf in gradient of model.blocks.11.attn.proj.weight
NaN or Inf in gradient of model.blocks.11.attn.proj.bias
NaN or Inf in gradient of model.blocks.11.norm2.weight
NaN or Inf in gradient of model.blocks.11.norm2.bias
NaN or Inf in gradient of model.blocks.11.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.11.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.11.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.11.mlp.fc2.bias
NaN or Inf in gradient of model.norm.weight
NaN or Inf in gradient of model.norm.bias
NaN or Inf in gradient of module.pooler.query_tokens
NaN or Inf in gradient of module.pooler.cross_attention_block.norm1.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.norm1.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.q.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.q.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.kv.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.kv.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.norm2.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.norm2.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc1.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc1.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc2.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc2.bias
NaN or Inf in gradient of module.linear.weight
NaN or Inf in gradient of module.linear.bias
INFO:root:[  120] 94.174% (loss: 0.328) [mem: 2.30e+04]
INFO:root:[  140] 94.504% (loss: 0.306) [mem: 2.30e+04]
INFO:root:[  160] 94.068% (loss: 0.306) [mem: 2.30e+04]
INFO:root:[  180] 93.950% (loss: 0.299) [mem: 2.30e+04]
INFO:root:[  200] 94.104% (loss: 0.285) [mem: 2.30e+04]
INFO:root:[  220] 94.186% (loss: 0.274) [mem: 2.30e+04]
INFO:root:[  240] 93.817% (loss: 0.286) [mem: 2.30e+04]
INFO:root:[  260] 93.774% (loss: 0.283) [mem: 2.30e+04]
INFO:root:[  280] 93.701% (loss: 0.280) [mem: 2.30e+04]
INFO:root:[  300] 93.904% (loss: 0.269) [mem: 2.30e+04]
INFO:root:[  320] 93.894% (loss: 0.275) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.002) [mem: 2.30e+04]
INFO:root:[   20] 99.286% (loss: 0.015) [mem: 2.30e+04]
INFO:root:[   40] 99.512% (loss: 0.012) [mem: 2.30e+04]
INFO:root:[   60] 99.344% (loss: 0.016) [mem: 2.30e+04]
INFO:root:[   80] 99.383% (loss: 0.015) [mem: 2.30e+04]
INFO:root:[   17] train: 93.860% test: 99.405% (loss: 0.276, grad_norm: nan)
Epoch 17: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 18
INFO:root:[    0] 100.000% (loss: 0.041) [mem: 2.30e+04]
INFO:root:[   20] 94.524% (loss: 0.133) [mem: 2.30e+04]
INFO:root:[   40] 95.366% (loss: 0.136) [mem: 2.30e+04]
INFO:root:[   60] 95.984% (loss: 0.132) [mem: 2.30e+04]
INFO:root:[   80] 95.309% (loss: 0.229) [mem: 2.30e+04]
INFO:root:[  100] 95.198% (loss: 0.239) [mem: 2.30e+04]
INFO:root:[  120] 94.752% (loss: 0.238) [mem: 2.30e+04]
INFO:root:[  140] 94.965% (loss: 0.226) [mem: 2.30e+04]
INFO:root:[  160] 95.217% (loss: 0.218) [mem: 2.30e+04]
INFO:root:[  180] 95.000% (loss: 0.230) [mem: 2.30e+04]
INFO:root:[  200] 95.174% (loss: 0.220) [mem: 2.30e+04]
INFO:root:[  220] 95.136% (loss: 0.218) [mem: 2.30e+04]
INFO:root:[  240] 95.166% (loss: 0.215) [mem: 2.30e+04]
INFO:root:[  260] 95.230% (loss: 0.206) [mem: 2.30e+04]
INFO:root:[  280] 94.982% (loss: 0.214) [mem: 2.30e+04]
INFO:root:[  300] 95.133% (loss: 0.207) [mem: 2.30e+04]
INFO:root:[  320] 95.093% (loss: 0.205) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.002) [mem: 2.30e+04]
INFO:root:[   20] 92.857% (loss: 0.292) [mem: 2.30e+04]
INFO:root:[   40] 93.171% (loss: 0.296) [mem: 2.30e+04]
INFO:root:[   60] 92.131% (loss: 0.359) [mem: 2.30e+04]
INFO:root:[   80] 91.728% (loss: 0.370) [mem: 2.30e+04]
INFO:root:[   18] train: 95.092% test: 91.845% (loss: 0.204, grad_norm: 23439.178)
Epoch 18: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 19
INFO:root:[    0] 85.000% (loss: 1.411) [mem: 2.30e+04]
INFO:root:[   20] 93.571% (loss: 0.273) [mem: 2.30e+04]
INFO:root:[   40] 94.268% (loss: 0.262) [mem: 2.30e+04]
INFO:root:[   60] 94.098% (loss: 0.257) [mem: 2.30e+04]
INFO:root:[   80] 93.457% (loss: 0.279) [mem: 2.30e+04]
INFO:root:[  100] 92.970% (loss: 0.281) [mem: 2.30e+04]
INFO:root:[  120] 93.347% (loss: 0.259) [mem: 2.30e+04]
INFO:root:[  140] 93.723% (loss: 0.241) [mem: 2.30e+04]
INFO:root:[  160] 93.696% (loss: 0.234) [mem: 2.30e+04]
INFO:root:[  180] 93.729% (loss: 0.227) [mem: 2.30e+04]
INFO:root:[  200] 93.831% (loss: 0.221) [mem: 2.30e+04]
INFO:root:[  220] 93.846% (loss: 0.222) [mem: 2.30e+04]
INFO:root:[  240] 94.108% (loss: 0.210) [mem: 2.30e+04]
INFO:root:[  260] 94.138% (loss: 0.208) [mem: 2.30e+04]
INFO:root:[  280] 94.253% (loss: 0.202) [mem: 2.30e+04]
INFO:root:[  300] 94.369% (loss: 0.199) [mem: 2.30e+04]
INFO:root:[  320] 94.486% (loss: 0.195) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.524% (loss: 0.021) [mem: 2.30e+04]
INFO:root:[   40] 99.512% (loss: 0.018) [mem: 2.30e+04]
INFO:root:[   60] 99.672% (loss: 0.012) [mem: 2.30e+04]
INFO:root:[   80] 99.753% (loss: 0.010) [mem: 2.30e+04]
INFO:root:[   19] train: 94.460% test: 99.762% (loss: 0.195, grad_norm: 20601.803)
Epoch 19: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 20
INFO:root:[    0] 90.000% (loss: 0.679) [mem: 2.30e+04]
INFO:root:[   20] 95.238% (loss: 0.201) [mem: 2.30e+04]
INFO:root:[   40] 95.732% (loss: 0.175) [mem: 2.30e+04]
INFO:root:[   60] 95.082% (loss: 0.183) [mem: 2.30e+04]
INFO:root:[   80] 95.309% (loss: 0.178) [mem: 2.30e+04]
INFO:root:[  100] 95.297% (loss: 0.178) [mem: 2.30e+04]
INFO:root:[  120] 95.331% (loss: 0.173) [mem: 2.30e+04]
INFO:root:[  140] 95.355% (loss: 0.179) [mem: 2.30e+04]
INFO:root:[  160] 95.404% (loss: 0.183) [mem: 2.30e+04]
INFO:root:[  180] 95.525% (loss: 0.186) [mem: 2.30e+04]
INFO:root:[  200] 95.398% (loss: 0.201) [mem: 2.30e+04]
INFO:root:[  220] 95.226% (loss: 0.199) [mem: 2.30e+04]
INFO:root:[  240] 95.166% (loss: 0.195) [mem: 2.30e+04]
INFO:root:[  260] 94.904% (loss: 0.200) [mem: 2.30e+04]
INFO:root:[  280] 94.947% (loss: 0.193) [mem: 2.30e+04]
INFO:root:[  300] 94.917% (loss: 0.197) [mem: 2.30e+04]
INFO:root:[  320] 94.891% (loss: 0.197) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.286% (loss: 0.028) [mem: 2.30e+04]
INFO:root:[   40] 99.390% (loss: 0.019) [mem: 2.30e+04]
INFO:root:[   60] 99.098% (loss: 0.029) [mem: 2.30e+04]
INFO:root:[   80] 99.259% (loss: 0.024) [mem: 2.30e+04]
INFO:root:[   20] train: 94.922% test: 99.286% (loss: 0.196, grad_norm: 32132.809)
Epoch 20: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 21
INFO:root:[    0] 90.000% (loss: 0.350) [mem: 2.30e+04]
INFO:root:[   20] 95.000% (loss: 0.115) [mem: 2.30e+04]
INFO:root:[   40] 94.268% (loss: 0.198) [mem: 2.30e+04]
INFO:root:[   60] 93.033% (loss: 0.233) [mem: 2.30e+04]
INFO:root:[   80] 93.333% (loss: 0.218) [mem: 2.30e+04]
INFO:root:[  100] 93.465% (loss: 0.200) [mem: 2.30e+04]
INFO:root:[  120] 93.223% (loss: 0.221) [mem: 2.30e+04]
INFO:root:[  140] 93.262% (loss: 0.218) [mem: 2.30e+04]
INFO:root:[  160] 93.137% (loss: 0.221) [mem: 2.30e+04]
INFO:root:[  180] 93.398% (loss: 0.213) [mem: 2.30e+04]
INFO:root:[  200] 93.582% (loss: 0.204) [mem: 2.30e+04]
INFO:root:[  220] 93.620% (loss: 0.202) [mem: 2.30e+04]
INFO:root:[  240] 93.568% (loss: 0.200) [mem: 2.30e+04]
INFO:root:[  260] 93.410% (loss: 0.204) [mem: 2.30e+04]
INFO:root:[  280] 93.523% (loss: 0.200) [mem: 2.30e+04]
INFO:root:[  300] 93.621% (loss: 0.198) [mem: 2.30e+04]
INFO:root:[  320] 93.754% (loss: 0.194) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.017) [mem: 2.30e+04]
INFO:root:[   20] 96.667% (loss: 0.103) [mem: 2.30e+04]
INFO:root:[   40] 97.683% (loss: 0.080) [mem: 2.30e+04]
INFO:root:[   60] 97.295% (loss: 0.087) [mem: 2.30e+04]
INFO:root:[   80] 97.407% (loss: 0.079) [mem: 2.30e+04]
INFO:root:[   21] train: 93.799% test: 97.440% (loss: 0.192, grad_norm: 56697.891)
Epoch 21: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 22
INFO:root:[    0] 95.000% (loss: 0.133) [mem: 2.30e+04]
INFO:root:[   20] 94.524% (loss: 0.187) [mem: 2.30e+04]
INFO:root:[   40] 93.537% (loss: 0.241) [mem: 2.30e+04]
INFO:root:[   60] 93.770% (loss: 0.211) [mem: 2.30e+04]
INFO:root:[   80] 94.136% (loss: 0.194) [mem: 2.30e+04]
INFO:root:[  100] 92.129% (loss: 0.312) [mem: 2.30e+04]
INFO:root:[  120] 92.645% (loss: 0.290) [mem: 2.30e+04]
INFO:root:[  140] 92.943% (loss: 0.272) [mem: 2.30e+04]
INFO:root:[  160] 93.137% (loss: 0.255) [mem: 2.30e+04]
INFO:root:[  180] 93.453% (loss: 0.242) [mem: 2.30e+04]
INFO:root:[  200] 93.308% (loss: 0.253) [mem: 2.30e+04]
INFO:root:[  220] 93.552% (loss: 0.243) [mem: 2.30e+04]
INFO:root:[  240] 93.651% (loss: 0.237) [mem: 2.30e+04]
INFO:root:[  260] 93.065% (loss: 0.254) [mem: 2.30e+04]
INFO:root:[  280] 93.132% (loss: 0.250) [mem: 2.30e+04]
INFO:root:[  300] 93.189% (loss: 0.243) [mem: 2.30e+04]
INFO:root:[  320] 93.224% (loss: 0.240) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.002) [mem: 2.30e+04]
INFO:root:[   20] 99.762% (loss: 0.014) [mem: 2.30e+04]
INFO:root:[   40] 99.756% (loss: 0.009) [mem: 2.30e+04]
INFO:root:[   60] 99.344% (loss: 0.021) [mem: 2.30e+04]
INFO:root:[   80] 99.198% (loss: 0.020) [mem: 2.30e+04]
INFO:root:[   22] train: 93.214% test: 99.226% (loss: 0.240, grad_norm: 60319.180)
Epoch 22: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 23
INFO:root:[    0] 100.000% (loss: 0.005) [mem: 2.30e+04]
INFO:root:[   20] 96.905% (loss: 0.078) [mem: 2.30e+04]
INFO:root:[   40] 95.610% (loss: 0.156) [mem: 2.30e+04]
INFO:root:[   60] 95.246% (loss: 0.161) [mem: 2.30e+04]
INFO:root:[   80] 95.556% (loss: 0.158) [mem: 2.30e+04]
INFO:root:[  100] 95.545% (loss: 0.184) [mem: 2.30e+04]
INFO:root:[  120] 94.587% (loss: 0.224) [mem: 2.30e+04]
INFO:root:[  140] 94.504% (loss: 0.230) [mem: 2.30e+04]
INFO:root:[  160] 94.410% (loss: 0.232) [mem: 2.30e+04]
INFO:root:[  180] 94.503% (loss: 0.220) [mem: 2.30e+04]
INFO:root:[  200] 94.502% (loss: 0.212) [mem: 2.30e+04]
INFO:root:[  220] 94.683% (loss: 0.203) [mem: 2.30e+04]
INFO:root:[  240] 94.689% (loss: 0.198) [mem: 2.30e+04]
INFO:root:[  260] 94.693% (loss: 0.194) [mem: 2.30e+04]
INFO:root:[  280] 94.769% (loss: 0.191) [mem: 2.30e+04]
INFO:root:[  300] 94.767% (loss: 0.188) [mem: 2.30e+04]
INFO:root:[  320] 94.642% (loss: 0.195) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.001) [mem: 2.30e+04]
INFO:root:[   20] 99.286% (loss: 0.025) [mem: 2.30e+04]
INFO:root:[   40] 99.146% (loss: 0.020) [mem: 2.30e+04]
INFO:root:[   60] 98.934% (loss: 0.031) [mem: 2.30e+04]
INFO:root:[   80] 98.889% (loss: 0.031) [mem: 2.30e+04]
INFO:root:[   23] train: 94.614% test: 98.929% (loss: 0.194, grad_norm: 106110.969)
Epoch 23: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 24
INFO:root:[    0] 100.000% (loss: 0.034) [mem: 2.30e+04]
INFO:root:[   20] 94.286% (loss: 0.155) [mem: 2.30e+04]
INFO:root:[   40] 94.268% (loss: 0.174) [mem: 2.30e+04]
INFO:root:[   60] 94.180% (loss: 0.158) [mem: 2.30e+04]
INFO:root:[   80] 93.951% (loss: 0.160) [mem: 2.30e+04]
NaN or Inf in gradient of model.patch_embed.proj.weight
NaN or Inf in gradient of model.patch_embed.proj.bias
NaN or Inf in gradient of model.blocks.0.norm1.weight
NaN or Inf in gradient of model.blocks.0.norm1.bias
NaN or Inf in gradient of model.blocks.0.attn.qkv.weight
NaN or Inf in gradient of model.blocks.0.attn.qkv.bias
NaN or Inf in gradient of model.blocks.0.attn.proj.weight
NaN or Inf in gradient of model.blocks.0.attn.proj.bias
NaN or Inf in gradient of model.blocks.0.norm2.weight
NaN or Inf in gradient of model.blocks.0.norm2.bias
NaN or Inf in gradient of model.blocks.0.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.0.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.0.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.0.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.1.norm1.weight
NaN or Inf in gradient of model.blocks.1.norm1.bias
NaN or Inf in gradient of model.blocks.1.attn.qkv.weight
NaN or Inf in gradient of model.blocks.1.attn.qkv.bias
NaN or Inf in gradient of model.blocks.1.attn.proj.weight
NaN or Inf in gradient of model.blocks.1.attn.proj.bias
NaN or Inf in gradient of model.blocks.1.norm2.weight
NaN or Inf in gradient of model.blocks.1.norm2.bias
NaN or Inf in gradient of model.blocks.1.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.1.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.1.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.1.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.2.norm1.weight
NaN or Inf in gradient of model.blocks.2.norm1.bias
NaN or Inf in gradient of model.blocks.2.attn.qkv.weight
NaN or Inf in gradient of model.blocks.2.attn.qkv.bias
NaN or Inf in gradient of model.blocks.2.attn.proj.weight
NaN or Inf in gradient of model.blocks.2.attn.proj.bias
NaN or Inf in gradient of model.blocks.2.norm2.weight
NaN or Inf in gradient of model.blocks.2.norm2.bias
NaN or Inf in gradient of model.blocks.2.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.2.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.2.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.2.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.3.norm1.weight
NaN or Inf in gradient of model.blocks.3.norm1.bias
NaN or Inf in gradient of model.blocks.3.attn.qkv.weight
NaN or Inf in gradient of model.blocks.3.attn.qkv.bias
NaN or Inf in gradient of model.blocks.3.attn.proj.weight
NaN or Inf in gradient of model.blocks.3.attn.proj.bias
NaN or Inf in gradient of model.blocks.3.norm2.weight
NaN or Inf in gradient of model.blocks.3.norm2.bias
NaN or Inf in gradient of model.blocks.3.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.3.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.3.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.3.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.4.norm1.weight
NaN or Inf in gradient of model.blocks.4.norm1.bias
NaN or Inf in gradient of model.blocks.4.attn.qkv.weight
NaN or Inf in gradient of model.blocks.4.attn.qkv.bias
NaN or Inf in gradient of model.blocks.4.attn.proj.weight
NaN or Inf in gradient of model.blocks.4.attn.proj.bias
NaN or Inf in gradient of model.blocks.4.norm2.weight
NaN or Inf in gradient of model.blocks.4.norm2.bias
NaN or Inf in gradient of model.blocks.4.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.4.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.4.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.4.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.5.norm1.weight
NaN or Inf in gradient of model.blocks.5.norm1.bias
NaN or Inf in gradient of model.blocks.5.attn.qkv.weight
NaN or Inf in gradient of model.blocks.5.attn.qkv.bias
NaN or Inf in gradient of model.blocks.5.attn.proj.weight
NaN or Inf in gradient of model.blocks.5.attn.proj.bias
NaN or Inf in gradient of model.blocks.5.norm2.weight
NaN or Inf in gradient of model.blocks.5.norm2.bias
NaN or Inf in gradient of model.blocks.5.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.5.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.5.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.5.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.6.norm1.weight
NaN or Inf in gradient of model.blocks.6.norm1.bias
NaN or Inf in gradient of model.blocks.6.attn.qkv.weight
NaN or Inf in gradient of model.blocks.6.attn.qkv.bias
NaN or Inf in gradient of model.blocks.6.attn.proj.weight
NaN or Inf in gradient of model.blocks.6.attn.proj.bias
NaN or Inf in gradient of model.blocks.6.norm2.weight
NaN or Inf in gradient of model.blocks.6.norm2.bias
NaN or Inf in gradient of model.blocks.6.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.6.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.6.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.6.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.7.norm1.weight
NaN or Inf in gradient of model.blocks.7.norm1.bias
NaN or Inf in gradient of model.blocks.7.attn.qkv.weight
NaN or Inf in gradient of model.blocks.7.attn.qkv.bias
NaN or Inf in gradient of model.blocks.7.attn.proj.weight
NaN or Inf in gradient of model.blocks.7.attn.proj.bias
NaN or Inf in gradient of model.blocks.7.norm2.weight
NaN or Inf in gradient of model.blocks.7.norm2.bias
NaN or Inf in gradient of model.blocks.7.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.7.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.7.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.7.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.8.norm1.weight
NaN or Inf in gradient of model.blocks.8.norm1.bias
NaN or Inf in gradient of model.blocks.8.attn.qkv.weight
NaN or Inf in gradient of model.blocks.8.attn.qkv.bias
NaN or Inf in gradient of model.blocks.8.attn.proj.weight
NaN or Inf in gradient of model.blocks.8.attn.proj.bias
NaN or Inf in gradient of model.blocks.8.norm2.weight
NaN or Inf in gradient of model.blocks.8.norm2.bias
NaN or Inf in gradient of model.blocks.8.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.8.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.8.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.8.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.9.norm1.weight
NaN or Inf in gradient of model.blocks.9.norm1.bias
NaN or Inf in gradient of model.blocks.9.attn.qkv.weight
NaN or Inf in gradient of model.blocks.9.attn.qkv.bias
NaN or Inf in gradient of model.blocks.9.attn.proj.weight
NaN or Inf in gradient of model.blocks.9.attn.proj.bias
NaN or Inf in gradient of model.blocks.9.norm2.weight
NaN or Inf in gradient of model.blocks.9.norm2.bias
NaN or Inf in gradient of model.blocks.9.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.9.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.9.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.9.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.10.norm1.weight
NaN or Inf in gradient of model.blocks.10.norm1.bias
NaN or Inf in gradient of model.blocks.10.attn.qkv.weight
NaN or Inf in gradient of model.blocks.10.attn.qkv.bias
NaN or Inf in gradient of model.blocks.10.attn.proj.weight
NaN or Inf in gradient of model.blocks.10.attn.proj.bias
NaN or Inf in gradient of model.blocks.10.norm2.weight
NaN or Inf in gradient of model.blocks.10.norm2.bias
NaN or Inf in gradient of model.blocks.10.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.10.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.10.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.10.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.11.norm1.weight
NaN or Inf in gradient of model.blocks.11.norm1.bias
NaN or Inf in gradient of model.blocks.11.attn.qkv.weight
NaN or Inf in gradient of model.blocks.11.attn.qkv.bias
NaN or Inf in gradient of model.blocks.11.attn.proj.weight
NaN or Inf in gradient of model.blocks.11.attn.proj.bias
NaN or Inf in gradient of model.blocks.11.norm2.weight
NaN or Inf in gradient of model.blocks.11.norm2.bias
NaN or Inf in gradient of model.blocks.11.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.11.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.11.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.11.mlp.fc2.bias
NaN or Inf in gradient of model.norm.weight
NaN or Inf in gradient of model.norm.bias
NaN or Inf in gradient of module.pooler.query_tokens
NaN or Inf in gradient of module.pooler.cross_attention_block.norm1.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.norm1.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.q.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.q.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.kv.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.kv.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.norm2.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.norm2.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc1.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc1.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc2.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc2.bias
NaN or Inf in gradient of module.linear.weight
NaN or Inf in gradient of module.linear.bias
INFO:root:[  100] 94.010% (loss: 0.162) [mem: 2.30e+04]
INFO:root:[  120] 94.174% (loss: 0.170) [mem: 2.30e+04]
INFO:root:[  140] 94.149% (loss: 0.173) [mem: 2.30e+04]
INFO:root:[  160] 93.789% (loss: 0.187) [mem: 2.30e+04]
INFO:root:[  180] 93.425% (loss: 0.198) [mem: 2.30e+04]
INFO:root:[  200] 92.836% (loss: 0.221) [mem: 2.30e+04]
INFO:root:[  220] 92.738% (loss: 0.222) [mem: 2.30e+04]
INFO:root:[  240] 92.884% (loss: 0.216) [mem: 2.30e+04]
NaN or Inf in gradient of model.patch_embed.proj.weight
INFO:root:[  260] 93.084% (loss: 0.212) [mem: 2.30e+04]
INFO:root:[  280] 92.972% (loss: 0.217) [mem: 2.30e+04]
INFO:root:[  300] 92.973% (loss: 0.220) [mem: 2.30e+04]
INFO:root:[  320] 92.960% (loss: 0.224) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.524% (loss: 0.008) [mem: 2.30e+04]
INFO:root:[   40] 99.756% (loss: 0.005) [mem: 2.30e+04]
INFO:root:[   60] 99.508% (loss: 0.009) [mem: 2.30e+04]
INFO:root:[   80] 99.630% (loss: 0.007) [mem: 2.30e+04]
INFO:root:[   24] train: 92.999% test: 99.643% (loss: 0.225, grad_norm: nan)
Epoch 24: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 25
INFO:root:[    0] 95.000% (loss: 0.248) [mem: 2.30e+04]
INFO:root:[   20] 90.000% (loss: 0.382) [mem: 2.30e+04]
INFO:root:[   40] 92.195% (loss: 0.289) [mem: 2.30e+04]
INFO:root:[   60] 92.049% (loss: 0.282) [mem: 2.30e+04]
INFO:root:[   80] 92.284% (loss: 0.260) [mem: 2.30e+04]
INFO:root:[  100] 91.980% (loss: 0.263) [mem: 2.30e+04]
INFO:root:[  120] 92.066% (loss: 0.254) [mem: 2.30e+04]
INFO:root:[  140] 92.234% (loss: 0.254) [mem: 2.30e+04]
INFO:root:[  160] 92.516% (loss: 0.251) [mem: 2.30e+04]
INFO:root:[  180] 92.431% (loss: 0.249) [mem: 2.30e+04]
INFO:root:[  200] 92.239% (loss: 0.252) [mem: 2.30e+04]
INFO:root:[  220] 92.421% (loss: 0.245) [mem: 2.30e+04]
INFO:root:[  240] 92.407% (loss: 0.240) [mem: 2.30e+04]
INFO:root:[  260] 92.586% (loss: 0.233) [mem: 2.30e+04]
INFO:root:[  280] 92.456% (loss: 0.231) [mem: 2.30e+04]
INFO:root:[  300] 92.226% (loss: 0.241) [mem: 2.30e+04]
INFO:root:[  320] 92.103% (loss: 0.244) [mem: 2.30e+04]
INFO:root:[    0] 95.000% (loss: 0.062) [mem: 2.30e+04]
INFO:root:[   20] 96.190% (loss: 0.123) [mem: 2.30e+04]
INFO:root:[   40] 95.000% (loss: 0.148) [mem: 2.30e+04]
INFO:root:[   60] 93.852% (loss: 0.172) [mem: 2.30e+04]
INFO:root:[   80] 94.321% (loss: 0.163) [mem: 2.30e+04]
INFO:root:[   25] train: 92.168% test: 94.405% (loss: 0.242, grad_norm: 22598.957)
Epoch 25: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 26
INFO:root:[    0] 85.000% (loss: 0.474) [mem: 2.30e+04]
INFO:root:[   20] 90.238% (loss: 0.248) [mem: 2.30e+04]
INFO:root:[   40] 92.195% (loss: 0.203) [mem: 2.30e+04]
INFO:root:[   60] 91.639% (loss: 0.251) [mem: 2.30e+04]
INFO:root:[   80] 89.877% (loss: 0.302) [mem: 2.30e+04]
INFO:root:[  100] 89.901% (loss: 0.291) [mem: 2.30e+04]
INFO:root:[  120] 90.537% (loss: 0.271) [mem: 2.30e+04]
INFO:root:[  140] 90.816% (loss: 0.261) [mem: 2.30e+04]
INFO:root:[  160] 91.087% (loss: 0.253) [mem: 2.30e+04]
INFO:root:[  180] 91.464% (loss: 0.244) [mem: 2.30e+04]
INFO:root:[  200] 91.592% (loss: 0.244) [mem: 2.30e+04]
INFO:root:[  220] 91.810% (loss: 0.236) [mem: 2.30e+04]
INFO:root:[  240] 91.598% (loss: 0.242) [mem: 2.30e+04]
INFO:root:[  260] 91.648% (loss: 0.241) [mem: 2.30e+04]
INFO:root:[  280] 91.726% (loss: 0.242) [mem: 2.30e+04]
INFO:root:[  300] 91.877% (loss: 0.239) [mem: 2.30e+04]
INFO:root:[  320] 92.025% (loss: 0.235) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.003) [mem: 2.30e+04]
INFO:root:[   20] 98.333% (loss: 0.037) [mem: 2.30e+04]
INFO:root:[   40] 98.780% (loss: 0.031) [mem: 2.30e+04]
INFO:root:[   60] 98.525% (loss: 0.041) [mem: 2.30e+04]
INFO:root:[   80] 98.765% (loss: 0.035) [mem: 2.30e+04]
INFO:root:[   26] train: 92.045% test: 98.810% (loss: 0.233, grad_norm: 19951.340)
Epoch 26: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 27
INFO:root:[    0] 95.000% (loss: 0.111) [mem: 2.30e+04]
INFO:root:[   20] 95.238% (loss: 0.114) [mem: 2.30e+04]
INFO:root:[   40] 92.561% (loss: 0.218) [mem: 2.30e+04]
INFO:root:[   60] 93.115% (loss: 0.198) [mem: 2.30e+04]
INFO:root:[   80] 93.457% (loss: 0.196) [mem: 2.30e+04]
INFO:root:[  100] 93.713% (loss: 0.189) [mem: 2.30e+04]
INFO:root:[  120] 93.347% (loss: 0.195) [mem: 2.30e+04]
INFO:root:[  140] 93.582% (loss: 0.187) [mem: 2.30e+04]
INFO:root:[  160] 93.478% (loss: 0.199) [mem: 2.30e+04]
INFO:root:[  180] 93.343% (loss: 0.199) [mem: 2.30e+04]
INFO:root:[  200] 93.333% (loss: 0.197) [mem: 2.30e+04]
INFO:root:[  220] 93.371% (loss: 0.199) [mem: 2.30e+04]
INFO:root:[  240] 93.382% (loss: 0.200) [mem: 2.30e+04]
INFO:root:[  260] 93.295% (loss: 0.201) [mem: 2.30e+04]
INFO:root:[  280] 93.577% (loss: 0.193) [mem: 2.30e+04]
INFO:root:[  300] 93.754% (loss: 0.190) [mem: 2.30e+04]
INFO:root:[  320] 93.692% (loss: 0.190) [mem: 2.30e+04]
INFO:root:[    0] 90.000% (loss: 0.385) [mem: 2.30e+04]
INFO:root:[   20] 98.095% (loss: 0.075) [mem: 2.30e+04]
INFO:root:[   40] 97.317% (loss: 0.074) [mem: 2.30e+04]
INFO:root:[   60] 96.721% (loss: 0.088) [mem: 2.30e+04]
INFO:root:[   80] 96.790% (loss: 0.082) [mem: 2.30e+04]
INFO:root:[   27] train: 93.692% test: 96.905% (loss: 0.190, grad_norm: 15383.971)
Epoch 27: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 28
INFO:root:[    0] 95.000% (loss: 0.088) [mem: 2.30e+04]
INFO:root:[   20] 95.238% (loss: 0.130) [mem: 2.30e+04]
INFO:root:[   40] 94.756% (loss: 0.185) [mem: 2.30e+04]
INFO:root:[   60] 94.344% (loss: 0.186) [mem: 2.30e+04]
INFO:root:[   80] 93.889% (loss: 0.206) [mem: 2.30e+04]
INFO:root:[  100] 93.465% (loss: 0.216) [mem: 2.30e+04]
INFO:root:[  120] 93.760% (loss: 0.202) [mem: 2.30e+04]
INFO:root:[  140] 93.901% (loss: 0.193) [mem: 2.30e+04]
NaN or Inf in gradient of model.patch_embed.proj.weight
NaN or Inf in gradient of model.patch_embed.proj.bias
NaN or Inf in gradient of model.blocks.0.norm1.weight
NaN or Inf in gradient of model.blocks.0.norm1.bias
NaN or Inf in gradient of model.blocks.0.attn.qkv.weight
NaN or Inf in gradient of model.blocks.0.attn.qkv.bias
NaN or Inf in gradient of model.blocks.0.attn.proj.weight
NaN or Inf in gradient of model.blocks.0.attn.proj.bias
NaN or Inf in gradient of model.blocks.0.norm2.weight
NaN or Inf in gradient of model.blocks.0.norm2.bias
NaN or Inf in gradient of model.blocks.0.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.0.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.0.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.0.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.1.norm1.weight
NaN or Inf in gradient of model.blocks.1.norm1.bias
NaN or Inf in gradient of model.blocks.1.attn.qkv.weight
NaN or Inf in gradient of model.blocks.1.attn.qkv.bias
NaN or Inf in gradient of model.blocks.1.attn.proj.weight
NaN or Inf in gradient of model.blocks.1.attn.proj.bias
NaN or Inf in gradient of model.blocks.1.norm2.weight
NaN or Inf in gradient of model.blocks.1.norm2.bias
NaN or Inf in gradient of model.blocks.1.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.1.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.1.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.1.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.2.norm1.weight
NaN or Inf in gradient of model.blocks.2.norm1.bias
NaN or Inf in gradient of model.blocks.2.attn.qkv.weight
NaN or Inf in gradient of model.blocks.2.attn.qkv.bias
NaN or Inf in gradient of model.blocks.2.attn.proj.weight
NaN or Inf in gradient of model.blocks.2.attn.proj.bias
NaN or Inf in gradient of model.blocks.2.norm2.weight
NaN or Inf in gradient of model.blocks.2.norm2.bias
NaN or Inf in gradient of model.blocks.2.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.2.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.2.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.2.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.3.norm1.weight
NaN or Inf in gradient of model.blocks.3.norm1.bias
NaN or Inf in gradient of model.blocks.3.attn.qkv.weight
NaN or Inf in gradient of model.blocks.3.attn.qkv.bias
NaN or Inf in gradient of model.blocks.3.attn.proj.weight
NaN or Inf in gradient of model.blocks.3.attn.proj.bias
NaN or Inf in gradient of model.blocks.3.norm2.weight
NaN or Inf in gradient of model.blocks.3.norm2.bias
NaN or Inf in gradient of model.blocks.3.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.3.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.3.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.3.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.4.norm1.weight
NaN or Inf in gradient of model.blocks.4.norm1.bias
NaN or Inf in gradient of model.blocks.4.attn.qkv.weight
NaN or Inf in gradient of model.blocks.4.attn.qkv.bias
NaN or Inf in gradient of model.blocks.4.attn.proj.weight
NaN or Inf in gradient of model.blocks.4.attn.proj.bias
NaN or Inf in gradient of model.blocks.4.norm2.weight
NaN or Inf in gradient of model.blocks.4.norm2.bias
NaN or Inf in gradient of model.blocks.4.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.4.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.4.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.4.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.5.norm1.weight
NaN or Inf in gradient of model.blocks.5.norm1.bias
NaN or Inf in gradient of model.blocks.5.attn.qkv.weight
NaN or Inf in gradient of model.blocks.5.attn.qkv.bias
NaN or Inf in gradient of model.blocks.5.attn.proj.weight
NaN or Inf in gradient of model.blocks.5.attn.proj.bias
NaN or Inf in gradient of model.blocks.5.norm2.weight
NaN or Inf in gradient of model.blocks.5.norm2.bias
NaN or Inf in gradient of model.blocks.5.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.5.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.5.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.5.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.6.norm1.weight
NaN or Inf in gradient of model.blocks.6.norm1.bias
NaN or Inf in gradient of model.blocks.6.attn.qkv.weight
NaN or Inf in gradient of model.blocks.6.attn.qkv.bias
NaN or Inf in gradient of model.blocks.6.attn.proj.weight
NaN or Inf in gradient of model.blocks.6.attn.proj.bias
NaN or Inf in gradient of model.blocks.6.norm2.weight
NaN or Inf in gradient of model.blocks.6.norm2.bias
NaN or Inf in gradient of model.blocks.6.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.6.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.6.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.6.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.7.norm1.weight
NaN or Inf in gradient of model.blocks.7.norm1.bias
NaN or Inf in gradient of model.blocks.7.attn.qkv.weight
NaN or Inf in gradient of model.blocks.7.attn.qkv.bias
NaN or Inf in gradient of model.blocks.7.attn.proj.weight
NaN or Inf in gradient of model.blocks.7.attn.proj.bias
NaN or Inf in gradient of model.blocks.7.norm2.weight
NaN or Inf in gradient of model.blocks.7.norm2.bias
NaN or Inf in gradient of model.blocks.7.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.7.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.7.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.7.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.8.norm1.weight
NaN or Inf in gradient of model.blocks.8.norm1.bias
NaN or Inf in gradient of model.blocks.8.attn.qkv.weight
NaN or Inf in gradient of model.blocks.8.attn.qkv.bias
NaN or Inf in gradient of model.blocks.8.attn.proj.weight
NaN or Inf in gradient of model.blocks.8.attn.proj.bias
NaN or Inf in gradient of model.blocks.8.norm2.weight
NaN or Inf in gradient of model.blocks.8.norm2.bias
NaN or Inf in gradient of model.blocks.8.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.8.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.8.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.8.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.9.norm1.weight
NaN or Inf in gradient of model.blocks.9.norm1.bias
NaN or Inf in gradient of model.blocks.9.attn.qkv.weight
NaN or Inf in gradient of model.blocks.9.attn.qkv.bias
NaN or Inf in gradient of model.blocks.9.attn.proj.weight
NaN or Inf in gradient of model.blocks.9.attn.proj.bias
NaN or Inf in gradient of model.blocks.9.norm2.weight
NaN or Inf in gradient of model.blocks.9.norm2.bias
NaN or Inf in gradient of model.blocks.9.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.9.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.9.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.9.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.10.norm1.weight
NaN or Inf in gradient of model.blocks.10.norm1.bias
NaN or Inf in gradient of model.blocks.10.attn.qkv.weight
NaN or Inf in gradient of model.blocks.10.attn.qkv.bias
NaN or Inf in gradient of model.blocks.10.attn.proj.weight
NaN or Inf in gradient of model.blocks.10.attn.proj.bias
NaN or Inf in gradient of model.blocks.10.norm2.weight
NaN or Inf in gradient of model.blocks.10.norm2.bias
NaN or Inf in gradient of model.blocks.10.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.10.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.10.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.10.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.11.norm1.weight
NaN or Inf in gradient of model.blocks.11.norm1.bias
NaN or Inf in gradient of model.blocks.11.attn.qkv.weight
NaN or Inf in gradient of model.blocks.11.attn.qkv.bias
NaN or Inf in gradient of model.blocks.11.attn.proj.weight
NaN or Inf in gradient of model.blocks.11.attn.proj.bias
NaN or Inf in gradient of model.blocks.11.norm2.weight
NaN or Inf in gradient of model.blocks.11.norm2.bias
NaN or Inf in gradient of model.blocks.11.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.11.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.11.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.11.mlp.fc2.bias
NaN or Inf in gradient of model.norm.weight
NaN or Inf in gradient of model.norm.bias
NaN or Inf in gradient of module.pooler.query_tokens
NaN or Inf in gradient of module.pooler.cross_attention_block.norm1.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.norm1.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.q.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.q.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.kv.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.kv.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.norm2.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.norm2.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc1.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc1.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc2.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc2.bias
NaN or Inf in gradient of module.linear.weight
NaN or Inf in gradient of module.linear.bias
INFO:root:[  160] 93.851% (loss: 0.194) [mem: 2.30e+04]
INFO:root:[  180] 94.088% (loss: 0.189) [mem: 2.30e+04]
INFO:root:[  200] 94.080% (loss: 0.185) [mem: 2.30e+04]
INFO:root:[  220] 94.186% (loss: 0.183) [mem: 2.30e+04]
INFO:root:[  240] 94.087% (loss: 0.186) [mem: 2.30e+04]
INFO:root:[  260] 94.061% (loss: 0.191) [mem: 2.30e+04]
INFO:root:[  280] 94.075% (loss: 0.193) [mem: 2.30e+04]
INFO:root:[  300] 94.236% (loss: 0.187) [mem: 2.30e+04]
INFO:root:[  320] 94.268% (loss: 0.182) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.001) [mem: 2.30e+04]
INFO:root:[   20] 99.524% (loss: 0.016) [mem: 2.30e+04]
INFO:root:[   40] 99.512% (loss: 0.011) [mem: 2.30e+04]
INFO:root:[   60] 99.016% (loss: 0.022) [mem: 2.30e+04]
INFO:root:[   80] 99.198% (loss: 0.018) [mem: 2.30e+04]
INFO:root:[   28] train: 94.245% test: 99.226% (loss: 0.183, grad_norm: nan)
Epoch 28: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 29
INFO:root:[    0] 90.000% (loss: 0.337) [mem: 2.30e+04]
INFO:root:[   20] 90.238% (loss: 0.235) [mem: 2.30e+04]
INFO:root:[   40] 92.439% (loss: 0.227) [mem: 2.30e+04]
INFO:root:[   60] 92.377% (loss: 0.214) [mem: 2.30e+04]
INFO:root:[   80] 92.531% (loss: 0.214) [mem: 2.30e+04]
INFO:root:[  100] 91.881% (loss: 0.222) [mem: 2.30e+04]
INFO:root:[  120] 92.231% (loss: 0.217) [mem: 2.30e+04]
INFO:root:[  140] 92.305% (loss: 0.215) [mem: 2.30e+04]
INFO:root:[  160] 92.267% (loss: 0.215) [mem: 2.30e+04]
INFO:root:[  180] 92.320% (loss: 0.216) [mem: 2.30e+04]
INFO:root:[  200] 92.587% (loss: 0.215) [mem: 2.30e+04]
INFO:root:[  220] 92.738% (loss: 0.216) [mem: 2.30e+04]
INFO:root:[  240] 92.905% (loss: 0.212) [mem: 2.30e+04]
INFO:root:[  260] 92.720% (loss: 0.215) [mem: 2.30e+04]
INFO:root:[  280] 92.811% (loss: 0.212) [mem: 2.30e+04]
INFO:root:[  300] 92.575% (loss: 0.222) [mem: 2.30e+04]
INFO:root:[  320] 92.648% (loss: 0.219) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.002) [mem: 2.30e+04]
INFO:root:[   20] 99.286% (loss: 0.031) [mem: 2.30e+04]
INFO:root:[   40] 99.634% (loss: 0.017) [mem: 2.30e+04]
INFO:root:[   60] 99.426% (loss: 0.024) [mem: 2.30e+04]
INFO:root:[   80] 99.506% (loss: 0.021) [mem: 2.30e+04]
INFO:root:[   29] train: 92.677% test: 99.524% (loss: 0.219, grad_norm: 9093.885)
Epoch 29: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 30
INFO:root:[    0] 90.000% (loss: 0.536) [mem: 2.30e+04]
INFO:root:[   20] 86.905% (loss: 0.356) [mem: 2.30e+04]
INFO:root:[   40] 90.732% (loss: 0.263) [mem: 2.30e+04]
INFO:root:[   60] 91.639% (loss: 0.246) [mem: 2.30e+04]
INFO:root:[   80] 92.716% (loss: 0.214) [mem: 2.30e+04]
INFO:root:[  100] 93.168% (loss: 0.207) [mem: 2.30e+04]
INFO:root:[  120] 93.388% (loss: 0.202) [mem: 2.30e+04]
INFO:root:[  140] 93.191% (loss: 0.216) [mem: 2.30e+04]
INFO:root:[  160] 92.671% (loss: 0.242) [mem: 2.30e+04]
INFO:root:[  180] 92.652% (loss: 0.240) [mem: 2.30e+04]
INFO:root:[  200] 92.861% (loss: 0.232) [mem: 2.30e+04]
INFO:root:[  220] 93.032% (loss: 0.228) [mem: 2.30e+04]
INFO:root:[  240] 93.278% (loss: 0.217) [mem: 2.30e+04]
INFO:root:[  260] 93.180% (loss: 0.221) [mem: 2.30e+04]
INFO:root:[  280] 93.167% (loss: 0.220) [mem: 2.30e+04]
INFO:root:[  300] 93.156% (loss: 0.222) [mem: 2.30e+04]
INFO:root:[  320] 93.115% (loss: 0.226) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.001) [mem: 2.30e+04]
INFO:root:[   20] 100.000% (loss: 0.002) [mem: 2.30e+04]
INFO:root:[   40] 100.000% (loss: 0.002) [mem: 2.30e+04]
INFO:root:[   60] 99.836% (loss: 0.008) [mem: 2.30e+04]
INFO:root:[   80] 99.815% (loss: 0.008) [mem: 2.30e+04]
INFO:root:[   30] train: 93.138% test: 99.821% (loss: 0.226, grad_norm: 4899.833)
Epoch 30: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 31
INFO:root:[    0] 100.000% (loss: 0.027) [mem: 2.30e+04]
INFO:root:[   20] 92.619% (loss: 0.229) [mem: 2.30e+04]
INFO:root:[   40] 94.024% (loss: 0.208) [mem: 2.30e+04]
INFO:root:[   60] 94.098% (loss: 0.211) [mem: 2.30e+04]
INFO:root:[   80] 92.840% (loss: 0.249) [mem: 2.30e+04]
INFO:root:[  100] 93.564% (loss: 0.220) [mem: 2.30e+04]
INFO:root:[  120] 93.512% (loss: 0.217) [mem: 2.30e+04]
INFO:root:[  140] 93.652% (loss: 0.216) [mem: 2.30e+04]
INFO:root:[  160] 93.571% (loss: 0.216) [mem: 2.30e+04]
INFO:root:[  180] 93.481% (loss: 0.215) [mem: 2.30e+04]
INFO:root:[  200] 93.731% (loss: 0.209) [mem: 2.30e+04]
INFO:root:[  220] 94.005% (loss: 0.203) [mem: 2.30e+04]
INFO:root:[  240] 94.004% (loss: 0.203) [mem: 2.30e+04]
INFO:root:[  260] 94.100% (loss: 0.197) [mem: 2.30e+04]
INFO:root:[  280] 94.128% (loss: 0.193) [mem: 2.30e+04]
INFO:root:[  300] 94.286% (loss: 0.188) [mem: 2.30e+04]
INFO:root:[  320] 94.330% (loss: 0.185) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.762% (loss: 0.016) [mem: 2.30e+04]
INFO:root:[   40] 99.756% (loss: 0.018) [mem: 2.30e+04]
INFO:root:[   60] 99.344% (loss: 0.032) [mem: 2.30e+04]
INFO:root:[   80] 99.506% (loss: 0.024) [mem: 2.30e+04]
INFO:root:[   31] train: 94.307% test: 99.524% (loss: 0.186, grad_norm: 4634.484)
Epoch 31: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 32
INFO:root:[    0] 100.000% (loss: 0.008) [mem: 2.30e+04]
INFO:root:[   20] 95.952% (loss: 0.159) [mem: 2.30e+04]
INFO:root:[   40] 94.756% (loss: 0.198) [mem: 2.30e+04]
INFO:root:[   60] 94.590% (loss: 0.193) [mem: 2.30e+04]
INFO:root:[   80] 94.630% (loss: 0.194) [mem: 2.30e+04]
INFO:root:[  100] 94.604% (loss: 0.188) [mem: 2.30e+04]
INFO:root:[  120] 94.959% (loss: 0.178) [mem: 2.30e+04]
INFO:root:[  140] 95.142% (loss: 0.173) [mem: 2.30e+04]
INFO:root:[  160] 95.124% (loss: 0.177) [mem: 2.30e+04]
INFO:root:[  180] 94.834% (loss: 0.188) [mem: 2.30e+04]
INFO:root:[  200] 94.851% (loss: 0.185) [mem: 2.30e+04]
INFO:root:[  220] 94.683% (loss: 0.186) [mem: 2.30e+04]
INFO:root:[  240] 94.689% (loss: 0.184) [mem: 2.30e+04]
INFO:root:[  260] 94.713% (loss: 0.184) [mem: 2.30e+04]
INFO:root:[  280] 94.840% (loss: 0.179) [mem: 2.30e+04]
INFO:root:[  300] 94.950% (loss: 0.175) [mem: 2.30e+04]
INFO:root:[  320] 94.875% (loss: 0.181) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 98.333% (loss: 0.052) [mem: 2.30e+04]
INFO:root:[   40] 98.537% (loss: 0.051) [mem: 2.30e+04]
INFO:root:[   60] 98.197% (loss: 0.076) [mem: 2.30e+04]
INFO:root:[   80] 98.210% (loss: 0.068) [mem: 2.30e+04]
INFO:root:[   32] train: 94.876% test: 98.274% (loss: 0.180, grad_norm: 8258.984)
Epoch 32: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 33
INFO:root:[    0] 100.000% (loss: 0.002) [mem: 2.30e+04]
INFO:root:[   20] 96.190% (loss: 0.127) [mem: 2.30e+04]
INFO:root:[   40] 95.122% (loss: 0.161) [mem: 2.30e+04]
INFO:root:[   60] 95.000% (loss: 0.165) [mem: 2.30e+04]
INFO:root:[   80] 95.000% (loss: 0.168) [mem: 2.30e+04]
INFO:root:[  100] 94.851% (loss: 0.188) [mem: 2.30e+04]
INFO:root:[  120] 95.000% (loss: 0.182) [mem: 2.30e+04]
INFO:root:[  140] 95.106% (loss: 0.182) [mem: 2.30e+04]
INFO:root:[  160] 94.907% (loss: 0.195) [mem: 2.30e+04]
INFO:root:[  180] 94.530% (loss: 0.206) [mem: 2.30e+04]
INFO:root:[  200] 94.527% (loss: 0.205) [mem: 2.30e+04]
INFO:root:[  220] 94.548% (loss: 0.201) [mem: 2.30e+04]
INFO:root:[  240] 94.544% (loss: 0.199) [mem: 2.30e+04]
INFO:root:[  260] 94.559% (loss: 0.206) [mem: 2.30e+04]
INFO:root:[  280] 94.644% (loss: 0.204) [mem: 2.30e+04]
INFO:root:[  300] 94.751% (loss: 0.199) [mem: 2.30e+04]
INFO:root:[  320] 94.844% (loss: 0.193) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 97.381% (loss: 0.082) [mem: 2.30e+04]
INFO:root:[   40] 98.171% (loss: 0.059) [mem: 2.30e+04]
INFO:root:[   60] 97.869% (loss: 0.073) [mem: 2.30e+04]
INFO:root:[   80] 97.901% (loss: 0.069) [mem: 2.30e+04]
INFO:root:[   33] train: 94.861% test: 97.976% (loss: 0.192, grad_norm: 8184.865)
Epoch 33: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 34
INFO:root:[    0] 90.000% (loss: 0.616) [mem: 2.30e+04]
INFO:root:[   20] 93.571% (loss: 0.260) [mem: 2.30e+04]
INFO:root:[   40] 94.024% (loss: 0.237) [mem: 2.30e+04]
INFO:root:[   60] 94.508% (loss: 0.200) [mem: 2.30e+04]
INFO:root:[   80] 94.630% (loss: 0.197) [mem: 2.30e+04]
INFO:root:[  100] 94.703% (loss: 0.196) [mem: 2.30e+04]
INFO:root:[  120] 94.545% (loss: 0.189) [mem: 2.30e+04]
INFO:root:[  140] 94.468% (loss: 0.181) [mem: 2.30e+04]
INFO:root:[  160] 94.441% (loss: 0.183) [mem: 2.30e+04]
INFO:root:[  180] 94.530% (loss: 0.177) [mem: 2.30e+04]
INFO:root:[  200] 94.254% (loss: 0.188) [mem: 2.30e+04]
INFO:root:[  220] 94.321% (loss: 0.181) [mem: 2.30e+04]
INFO:root:[  240] 94.419% (loss: 0.176) [mem: 2.30e+04]
INFO:root:[  260] 94.521% (loss: 0.173) [mem: 2.30e+04]
INFO:root:[  280] 94.733% (loss: 0.169) [mem: 2.30e+04]
INFO:root:[  300] 94.834% (loss: 0.164) [mem: 2.30e+04]
INFO:root:[  320] 94.829% (loss: 0.164) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 100.000% (loss: 0.001) [mem: 2.30e+04]
INFO:root:[   40] 100.000% (loss: 0.001) [mem: 2.30e+04]
INFO:root:[   60] 100.000% (loss: 0.001) [mem: 2.30e+04]
INFO:root:[   80] 99.877% (loss: 0.003) [mem: 2.30e+04]
INFO:root:[   34] train: 94.751% test: 99.881% (loss: 0.172, grad_norm: 5741.971)
Epoch 34: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 35
INFO:root:[    0] 85.000% (loss: 1.002) [mem: 2.30e+04]
INFO:root:[   20] 93.333% (loss: 0.309) [mem: 2.30e+04]
INFO:root:[   40] 95.366% (loss: 0.204) [mem: 2.30e+04]
INFO:root:[   60] 95.820% (loss: 0.181) [mem: 2.30e+04]
INFO:root:[   80] 96.235% (loss: 0.157) [mem: 2.30e+04]
INFO:root:[  100] 95.990% (loss: 0.176) [mem: 2.30e+04]
INFO:root:[  120] 95.785% (loss: 0.174) [mem: 2.30e+04]
INFO:root:[  140] 95.922% (loss: 0.164) [mem: 2.30e+04]
INFO:root:[  160] 95.901% (loss: 0.163) [mem: 2.30e+04]
INFO:root:[  180] 95.829% (loss: 0.161) [mem: 2.30e+04]
INFO:root:[  200] 95.995% (loss: 0.152) [mem: 2.30e+04]
INFO:root:[  220] 96.109% (loss: 0.146) [mem: 2.30e+04]
INFO:root:[  240] 96.162% (loss: 0.151) [mem: 2.30e+04]
INFO:root:[  260] 96.054% (loss: 0.156) [mem: 2.30e+04]
INFO:root:[  280] 96.157% (loss: 0.150) [mem: 2.30e+04]
INFO:root:[  300] 96.163% (loss: 0.150) [mem: 2.30e+04]
INFO:root:[  320] 96.044% (loss: 0.154) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.286% (loss: 0.037) [mem: 2.30e+04]
INFO:root:[   40] 99.634% (loss: 0.020) [mem: 2.30e+04]
INFO:root:[   60] 99.262% (loss: 0.031) [mem: 2.30e+04]
INFO:root:[   80] 99.321% (loss: 0.036) [mem: 2.30e+04]
INFO:root:[   35] train: 96.061% test: 99.345% (loss: 0.153, grad_norm: 8710.162)
Epoch 35: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 36
INFO:root:[    0] 100.000% (loss: 0.005) [mem: 2.30e+04]
INFO:root:[   20] 96.190% (loss: 0.160) [mem: 2.30e+04]
INFO:root:[   40] 95.732% (loss: 0.183) [mem: 2.30e+04]
INFO:root:[   60] 95.984% (loss: 0.184) [mem: 2.30e+04]
INFO:root:[   80] 96.235% (loss: 0.168) [mem: 2.30e+04]
INFO:root:[  100] 96.089% (loss: 0.172) [mem: 2.30e+04]
INFO:root:[  120] 95.909% (loss: 0.186) [mem: 2.30e+04]
INFO:root:[  140] 95.816% (loss: 0.183) [mem: 2.30e+04]
INFO:root:[  160] 95.839% (loss: 0.178) [mem: 2.30e+04]
INFO:root:[  180] 95.884% (loss: 0.174) [mem: 2.30e+04]
INFO:root:[  200] 95.995% (loss: 0.166) [mem: 2.30e+04]
INFO:root:[  220] 96.063% (loss: 0.163) [mem: 2.30e+04]
INFO:root:[  240] 95.975% (loss: 0.171) [mem: 2.30e+04]
INFO:root:[  260] 96.015% (loss: 0.166) [mem: 2.30e+04]
INFO:root:[  280] 95.996% (loss: 0.165) [mem: 2.30e+04]
INFO:root:[  300] 95.980% (loss: 0.164) [mem: 2.30e+04]
INFO:root:[  320] 95.981% (loss: 0.162) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 98.810% (loss: 0.022) [mem: 2.30e+04]
INFO:root:[   40] 99.024% (loss: 0.021) [mem: 2.30e+04]
INFO:root:[   60] 98.607% (loss: 0.039) [mem: 2.30e+04]
INFO:root:[   80] 98.889% (loss: 0.033) [mem: 2.30e+04]
INFO:root:[   36] train: 96.015% test: 98.929% (loss: 0.162, grad_norm: 8868.554)
Epoch 36: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 37
INFO:root:[    0] 100.000% (loss: 0.007) [mem: 2.30e+04]
INFO:root:[   20] 95.476% (loss: 0.209) [mem: 2.30e+04]
INFO:root:[   40] 95.000% (loss: 0.231) [mem: 2.30e+04]
INFO:root:[   60] 94.836% (loss: 0.215) [mem: 2.30e+04]
INFO:root:[   80] 95.370% (loss: 0.183) [mem: 2.30e+04]
INFO:root:[  100] 95.644% (loss: 0.180) [mem: 2.30e+04]
INFO:root:[  120] 95.702% (loss: 0.178) [mem: 2.30e+04]
INFO:root:[  140] 95.709% (loss: 0.169) [mem: 2.30e+04]
INFO:root:[  160] 95.807% (loss: 0.172) [mem: 2.30e+04]
INFO:root:[  180] 95.580% (loss: 0.178) [mem: 2.30e+04]
INFO:root:[  200] 95.697% (loss: 0.173) [mem: 2.30e+04]
INFO:root:[  220] 95.679% (loss: 0.172) [mem: 2.30e+04]
INFO:root:[  240] 95.705% (loss: 0.170) [mem: 2.30e+04]
INFO:root:[  260] 95.881% (loss: 0.165) [mem: 2.30e+04]
INFO:root:[  280] 95.979% (loss: 0.163) [mem: 2.30e+04]
INFO:root:[  300] 96.030% (loss: 0.163) [mem: 2.30e+04]
INFO:root:[  320] 96.090% (loss: 0.163) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 98.810% (loss: 0.061) [mem: 2.30e+04]
INFO:root:[   40] 99.146% (loss: 0.035) [mem: 2.30e+04]
INFO:root:[   60] 98.852% (loss: 0.051) [mem: 2.30e+04]
INFO:root:[   80] 99.074% (loss: 0.042) [mem: 2.30e+04]
INFO:root:[   37] train: 96.107% test: 99.107% (loss: 0.162, grad_norm: 9518.779)
Epoch 37: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 38
INFO:root:[    0] 90.000% (loss: 0.334) [mem: 2.30e+04]
INFO:root:[   20] 96.667% (loss: 0.161) [mem: 2.30e+04]
INFO:root:[   40] 96.463% (loss: 0.162) [mem: 2.30e+04]
INFO:root:[   60] 95.738% (loss: 0.191) [mem: 2.30e+04]
INFO:root:[   80] 96.049% (loss: 0.187) [mem: 2.30e+04]
INFO:root:[  100] 96.139% (loss: 0.183) [mem: 2.30e+04]
INFO:root:[  120] 96.157% (loss: 0.178) [mem: 2.30e+04]
INFO:root:[  140] 96.206% (loss: 0.182) [mem: 2.30e+04]
INFO:root:[  160] 95.994% (loss: 0.192) [mem: 2.30e+04]
INFO:root:[  180] 95.884% (loss: 0.191) [mem: 2.30e+04]
INFO:root:[  200] 95.945% (loss: 0.186) [mem: 2.30e+04]
INFO:root:[  220] 95.905% (loss: 0.193) [mem: 2.30e+04]
INFO:root:[  240] 95.996% (loss: 0.186) [mem: 2.30e+04]
INFO:root:[  260] 96.015% (loss: 0.184) [mem: 2.30e+04]
INFO:root:[  280] 96.014% (loss: 0.183) [mem: 2.30e+04]
INFO:root:[  300] 96.013% (loss: 0.184) [mem: 2.30e+04]
INFO:root:[  320] 96.059% (loss: 0.184) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.286% (loss: 0.021) [mem: 2.30e+04]
INFO:root:[   40] 99.390% (loss: 0.024) [mem: 2.30e+04]
INFO:root:[   60] 99.098% (loss: 0.033) [mem: 2.30e+04]
INFO:root:[   80] 99.136% (loss: 0.034) [mem: 2.30e+04]
INFO:root:[   38] train: 96.029% test: 99.167% (loss: 0.185, grad_norm: 15902.092)
Epoch 38: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 39
INFO:root:[    0] 85.000% (loss: 0.308) [mem: 2.30e+04]
INFO:root:[   20] 95.476% (loss: 0.242) [mem: 2.30e+04]
INFO:root:[   40] 96.585% (loss: 0.176) [mem: 2.30e+04]
INFO:root:[   60] 95.902% (loss: 0.210) [mem: 2.30e+04]
INFO:root:[   80] 95.247% (loss: 0.244) [mem: 2.30e+04]
INFO:root:[  100] 95.545% (loss: 0.233) [mem: 2.30e+04]
INFO:root:[  120] 95.868% (loss: 0.213) [mem: 2.30e+04]
INFO:root:[  140] 96.064% (loss: 0.202) [mem: 2.30e+04]
INFO:root:[  160] 95.839% (loss: 0.226) [mem: 2.30e+04]
INFO:root:[  180] 95.939% (loss: 0.221) [mem: 2.30e+04]
INFO:root:[  200] 95.920% (loss: 0.215) [mem: 2.30e+04]
INFO:root:[  220] 95.611% (loss: 0.225) [mem: 2.30e+04]
INFO:root:[  240] 95.560% (loss: 0.223) [mem: 2.30e+04]
INFO:root:[  260] 95.613% (loss: 0.219) [mem: 2.30e+04]
INFO:root:[  280] 95.605% (loss: 0.216) [mem: 2.30e+04]
INFO:root:[  300] 95.698% (loss: 0.206) [mem: 2.30e+04]
INFO:root:[  320] 95.748% (loss: 0.201) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 98.810% (loss: 0.046) [mem: 2.30e+04]
INFO:root:[   40] 99.268% (loss: 0.025) [mem: 2.30e+04]
INFO:root:[   60] 99.262% (loss: 0.026) [mem: 2.30e+04]
INFO:root:[   80] 99.259% (loss: 0.037) [mem: 2.30e+04]
INFO:root:[   39] train: 95.768% test: 99.286% (loss: 0.200, grad_norm: 26769.027)
Epoch 39: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 40
INFO:root:[    0] 100.000% (loss: 0.001) [mem: 2.30e+04]
INFO:root:[   20] 95.714% (loss: 0.192) [mem: 2.30e+04]
INFO:root:[   40] 96.829% (loss: 0.147) [mem: 2.30e+04]
INFO:root:[   60] 96.803% (loss: 0.145) [mem: 2.30e+04]
INFO:root:[   80] 97.099% (loss: 0.128) [mem: 2.30e+04]
INFO:root:[  100] 96.832% (loss: 0.147) [mem: 2.30e+04]
INFO:root:[  120] 96.529% (loss: 0.184) [mem: 2.30e+04]
INFO:root:[  140] 96.489% (loss: 0.184) [mem: 2.30e+04]
INFO:root:[  160] 96.429% (loss: 0.182) [mem: 2.30e+04]
INFO:root:[  180] 96.381% (loss: 0.183) [mem: 2.30e+04]
INFO:root:[  200] 96.393% (loss: 0.181) [mem: 2.30e+04]
INFO:root:[  220] 96.176% (loss: 0.187) [mem: 2.30e+04]
INFO:root:[  240] 96.079% (loss: 0.194) [mem: 2.30e+04]
INFO:root:[  260] 95.977% (loss: 0.201) [mem: 2.30e+04]
INFO:root:[  280] 95.925% (loss: 0.197) [mem: 2.30e+04]
INFO:root:[  300] 95.814% (loss: 0.199) [mem: 2.30e+04]
INFO:root:[  320] 95.701% (loss: 0.203) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.762% (loss: 0.018) [mem: 2.30e+04]
INFO:root:[   40] 99.756% (loss: 0.015) [mem: 2.30e+04]
INFO:root:[   60] 99.754% (loss: 0.013) [mem: 2.30e+04]
INFO:root:[   80] 99.753% (loss: 0.018) [mem: 2.30e+04]
INFO:root:[   40] train: 95.738% test: 99.762% (loss: 0.201, grad_norm: 49457.348)
Epoch 40: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 41
INFO:root:[    0] 95.000% (loss: 0.507) [mem: 2.30e+04]
INFO:root:[   20] 95.952% (loss: 0.189) [mem: 2.30e+04]
INFO:root:[   40] 96.220% (loss: 0.192) [mem: 2.30e+04]
INFO:root:[   60] 95.738% (loss: 0.216) [mem: 2.30e+04]
INFO:root:[   80] 95.864% (loss: 0.207) [mem: 2.30e+04]
INFO:root:[  100] 95.941% (loss: 0.209) [mem: 2.30e+04]
INFO:root:[  120] 95.950% (loss: 0.201) [mem: 2.30e+04]
INFO:root:[  140] 95.816% (loss: 0.215) [mem: 2.30e+04]
INFO:root:[  160] 95.870% (loss: 0.219) [mem: 2.30e+04]
INFO:root:[  180] 95.718% (loss: 0.225) [mem: 2.30e+04]
INFO:root:[  200] 95.622% (loss: 0.226) [mem: 2.30e+04]
INFO:root:[  220] 95.769% (loss: 0.218) [mem: 2.30e+04]
INFO:root:[  240] 95.809% (loss: 0.209) [mem: 2.30e+04]
INFO:root:[  260] 95.843% (loss: 0.205) [mem: 2.30e+04]
INFO:root:[  280] 95.872% (loss: 0.203) [mem: 2.30e+04]
INFO:root:[  300] 95.980% (loss: 0.195) [mem: 2.30e+04]
INFO:root:[  320] 96.012% (loss: 0.195) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.524% (loss: 0.022) [mem: 2.30e+04]
INFO:root:[   40] 99.756% (loss: 0.011) [mem: 2.30e+04]
INFO:root:[   60] 99.590% (loss: 0.014) [mem: 2.30e+04]
INFO:root:[   80] 99.691% (loss: 0.011) [mem: 2.30e+04]
INFO:root:[   41] train: 96.045% test: 99.702% (loss: 0.194, grad_norm: 52969.582)
Epoch 41: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 42
INFO:root:[    0] 95.000% (loss: 0.106) [mem: 2.30e+04]
INFO:root:[   20] 96.667% (loss: 0.122) [mem: 2.30e+04]
INFO:root:[   40] 96.220% (loss: 0.140) [mem: 2.30e+04]
INFO:root:[   60] 96.148% (loss: 0.168) [mem: 2.30e+04]
INFO:root:[   80] 96.358% (loss: 0.159) [mem: 2.30e+04]
INFO:root:[  100] 96.535% (loss: 0.156) [mem: 2.30e+04]
INFO:root:[  120] 96.488% (loss: 0.163) [mem: 2.30e+04]
INFO:root:[  140] 96.525% (loss: 0.168) [mem: 2.30e+04]
INFO:root:[  160] 96.491% (loss: 0.174) [mem: 2.30e+04]
INFO:root:[  180] 96.326% (loss: 0.182) [mem: 2.30e+04]
INFO:root:[  200] 96.393% (loss: 0.182) [mem: 2.30e+04]
INFO:root:[  220] 96.312% (loss: 0.187) [mem: 2.30e+04]
INFO:root:[  240] 96.328% (loss: 0.183) [mem: 2.30e+04]
INFO:root:[  260] 96.322% (loss: 0.187) [mem: 2.30e+04]
INFO:root:[  280] 96.352% (loss: 0.181) [mem: 2.30e+04]
INFO:root:[  300] 96.296% (loss: 0.186) [mem: 2.30e+04]
INFO:root:[  320] 96.277% (loss: 0.186) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.048% (loss: 0.065) [mem: 2.30e+04]
INFO:root:[   40] 99.268% (loss: 0.046) [mem: 2.30e+04]
INFO:root:[   60] 99.180% (loss: 0.044) [mem: 2.30e+04]
INFO:root:[   80] 99.259% (loss: 0.037) [mem: 2.30e+04]
INFO:root:[   42] train: 96.260% test: 99.286% (loss: 0.186, grad_norm: 39846.375)
Epoch 42: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 43
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 96.905% (loss: 0.159) [mem: 2.30e+04]
INFO:root:[   40] 96.951% (loss: 0.157) [mem: 2.30e+04]
INFO:root:[   60] 96.967% (loss: 0.159) [mem: 2.30e+04]
INFO:root:[   80] 96.667% (loss: 0.160) [mem: 2.30e+04]
INFO:root:[  100] 96.337% (loss: 0.168) [mem: 2.30e+04]
INFO:root:[  120] 95.702% (loss: 0.206) [mem: 2.30e+04]
INFO:root:[  140] 95.851% (loss: 0.187) [mem: 2.30e+04]
INFO:root:[  160] 95.714% (loss: 0.190) [mem: 2.30e+04]
INFO:root:[  180] 95.746% (loss: 0.191) [mem: 2.30e+04]
INFO:root:[  200] 95.796% (loss: 0.194) [mem: 2.30e+04]
INFO:root:[  220] 95.882% (loss: 0.192) [mem: 2.30e+04]
INFO:root:[  240] 95.871% (loss: 0.193) [mem: 2.30e+04]
INFO:root:[  260] 95.900% (loss: 0.194) [mem: 2.30e+04]
INFO:root:[  280] 95.854% (loss: 0.196) [mem: 2.30e+04]
INFO:root:[  300] 95.897% (loss: 0.191) [mem: 2.30e+04]
INFO:root:[  320] 95.950% (loss: 0.188) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 98.810% (loss: 0.066) [mem: 2.30e+04]
INFO:root:[   40] 99.024% (loss: 0.040) [mem: 2.30e+04]
INFO:root:[   60] 99.016% (loss: 0.041) [mem: 2.30e+04]
INFO:root:[   80] 99.074% (loss: 0.037) [mem: 2.30e+04]
INFO:root:[   43] train: 96.000% test: 99.107% (loss: 0.185, grad_norm: 31782.420)
Epoch 43: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 44
INFO:root:[    0] 95.000% (loss: 0.186) [mem: 2.30e+04]
INFO:root:[   20] 95.476% (loss: 0.301) [mem: 2.30e+04]
INFO:root:[   40] 96.341% (loss: 0.239) [mem: 2.30e+04]
INFO:root:[   60] 96.721% (loss: 0.201) [mem: 2.30e+04]
INFO:root:[   80] 96.296% (loss: 0.210) [mem: 2.30e+04]
INFO:root:[  100] 96.386% (loss: 0.203) [mem: 2.30e+04]
INFO:root:[  120] 95.992% (loss: 0.225) [mem: 2.30e+04]
INFO:root:[  140] 96.312% (loss: 0.212) [mem: 2.30e+04]
INFO:root:[  160] 96.398% (loss: 0.206) [mem: 2.30e+04]
INFO:root:[  180] 96.326% (loss: 0.208) [mem: 2.30e+04]
INFO:root:[  200] 96.393% (loss: 0.201) [mem: 2.30e+04]
INFO:root:[  220] 96.357% (loss: 0.201) [mem: 2.30e+04]
INFO:root:[  240] 96.452% (loss: 0.192) [mem: 2.30e+04]
INFO:root:[  260] 96.437% (loss: 0.187) [mem: 2.30e+04]
INFO:root:[  280] 96.406% (loss: 0.189) [mem: 2.30e+04]
INFO:root:[  300] 96.346% (loss: 0.192) [mem: 2.30e+04]
INFO:root:[  320] 96.340% (loss: 0.190) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.286% (loss: 0.011) [mem: 2.30e+04]
INFO:root:[   40] 99.390% (loss: 0.016) [mem: 2.30e+04]
INFO:root:[   60] 99.262% (loss: 0.016) [mem: 2.30e+04]
INFO:root:[   80] 99.444% (loss: 0.012) [mem: 2.30e+04]
INFO:root:[   44] train: 96.354% test: 99.464% (loss: 0.189, grad_norm: 28341.512)
Epoch 44: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 45
INFO:root:[    0] 95.000% (loss: 0.268) [mem: 2.30e+04]
INFO:root:[   20] 95.952% (loss: 0.284) [mem: 2.30e+04]
INFO:root:[   40] 95.488% (loss: 0.307) [mem: 2.30e+04]
INFO:root:[   60] 95.984% (loss: 0.261) [mem: 2.30e+04]
INFO:root:[   80] 95.926% (loss: 0.255) [mem: 2.30e+04]
INFO:root:[  100] 95.891% (loss: 0.252) [mem: 2.30e+04]
INFO:root:[  120] 96.198% (loss: 0.234) [mem: 2.30e+04]
INFO:root:[  140] 96.064% (loss: 0.234) [mem: 2.30e+04]
INFO:root:[  160] 95.994% (loss: 0.230) [mem: 2.30e+04]
INFO:root:[  180] 95.912% (loss: 0.226) [mem: 2.30e+04]
INFO:root:[  200] 95.796% (loss: 0.235) [mem: 2.30e+04]
INFO:root:[  220] 95.814% (loss: 0.229) [mem: 2.30e+04]
INFO:root:[  240] 95.934% (loss: 0.220) [mem: 2.30e+04]
INFO:root:[  260] 95.900% (loss: 0.219) [mem: 2.30e+04]
INFO:root:[  280] 95.979% (loss: 0.214) [mem: 2.30e+04]
INFO:root:[  300] 95.963% (loss: 0.210) [mem: 2.30e+04]
INFO:root:[  320] 96.028% (loss: 0.203) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.286% (loss: 0.060) [mem: 2.30e+04]
INFO:root:[   40] 99.512% (loss: 0.037) [mem: 2.30e+04]
INFO:root:[   60] 99.262% (loss: 0.035) [mem: 2.30e+04]
INFO:root:[   80] 99.259% (loss: 0.032) [mem: 2.30e+04]
INFO:root:[   45] train: 96.062% test: 99.286% (loss: 0.201, grad_norm: 30468.375)
Epoch 45: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 46
INFO:root:[    0] 100.000% (loss: 0.001) [mem: 2.30e+04]
INFO:root:[   20] 95.476% (loss: 0.192) [mem: 2.30e+04]
INFO:root:[   40] 95.854% (loss: 0.153) [mem: 2.30e+04]
INFO:root:[   60] 95.820% (loss: 0.178) [mem: 2.30e+04]
INFO:root:[   80] 96.049% (loss: 0.174) [mem: 2.30e+04]
INFO:root:[  100] 96.139% (loss: 0.162) [mem: 2.30e+04]
INFO:root:[  120] 96.281% (loss: 0.150) [mem: 2.30e+04]
INFO:root:[  140] 96.099% (loss: 0.167) [mem: 2.30e+04]
INFO:root:[  160] 96.149% (loss: 0.170) [mem: 2.30e+04]
INFO:root:[  180] 96.077% (loss: 0.179) [mem: 2.30e+04]
INFO:root:[  200] 96.119% (loss: 0.177) [mem: 2.30e+04]
INFO:root:[  220] 96.063% (loss: 0.186) [mem: 2.30e+04]
INFO:root:[  240] 96.037% (loss: 0.189) [mem: 2.30e+04]
INFO:root:[  260] 96.073% (loss: 0.186) [mem: 2.30e+04]
INFO:root:[  280] 96.157% (loss: 0.185) [mem: 2.30e+04]
INFO:root:[  300] 96.047% (loss: 0.192) [mem: 2.30e+04]
INFO:root:[  320] 95.966% (loss: 0.202) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.762% (loss: 0.013) [mem: 2.30e+04]
INFO:root:[   40] 99.634% (loss: 0.011) [mem: 2.30e+04]
INFO:root:[   60] 99.344% (loss: 0.018) [mem: 2.30e+04]
INFO:root:[   80] 99.444% (loss: 0.022) [mem: 2.30e+04]
INFO:root:[   46] train: 95.999% test: 99.405% (loss: 0.201, grad_norm: 38508.961)
Epoch 46: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 47
INFO:root:[    0] 95.000% (loss: 0.251) [mem: 2.30e+04]
INFO:root:[   20] 96.190% (loss: 0.204) [mem: 2.30e+04]
INFO:root:[   40] 96.341% (loss: 0.167) [mem: 2.30e+04]
INFO:root:[   60] 96.148% (loss: 0.181) [mem: 2.30e+04]
INFO:root:[   80] 96.358% (loss: 0.178) [mem: 2.30e+04]
INFO:root:[  100] 96.040% (loss: 0.195) [mem: 2.30e+04]
INFO:root:[  120] 96.116% (loss: 0.186) [mem: 2.30e+04]
INFO:root:[  140] 96.028% (loss: 0.186) [mem: 2.30e+04]
INFO:root:[  160] 96.056% (loss: 0.187) [mem: 2.30e+04]
INFO:root:[  180] 96.077% (loss: 0.182) [mem: 2.30e+04]
INFO:root:[  200] 95.995% (loss: 0.184) [mem: 2.30e+04]
INFO:root:[  220] 96.041% (loss: 0.181) [mem: 2.30e+04]
INFO:root:[  240] 96.120% (loss: 0.180) [mem: 2.30e+04]
INFO:root:[  260] 96.111% (loss: 0.181) [mem: 2.30e+04]
INFO:root:[  280] 96.103% (loss: 0.184) [mem: 2.30e+04]
INFO:root:[  300] 96.213% (loss: 0.184) [mem: 2.30e+04]
INFO:root:[  320] 96.340% (loss: 0.178) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   40] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   60] 99.508% (loss: 0.020) [mem: 2.30e+04]
INFO:root:[   80] 99.506% (loss: 0.020) [mem: 2.30e+04]
INFO:root:[   47] train: 96.354% test: 99.524% (loss: 0.179, grad_norm: 57088.316)
Epoch 47: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 48
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 97.143% (loss: 0.134) [mem: 2.30e+04]
INFO:root:[   40] 97.195% (loss: 0.158) [mem: 2.30e+04]
INFO:root:[   60] 96.721% (loss: 0.207) [mem: 2.30e+04]
INFO:root:[   80] 96.420% (loss: 0.233) [mem: 2.30e+04]
INFO:root:[  100] 96.782% (loss: 0.210) [mem: 2.30e+04]
INFO:root:[  120] 96.612% (loss: 0.215) [mem: 2.30e+04]
INFO:root:[  140] 96.702% (loss: 0.209) [mem: 2.30e+04]
INFO:root:[  160] 96.925% (loss: 0.191) [mem: 2.30e+04]
INFO:root:[  180] 97.017% (loss: 0.195) [mem: 2.30e+04]
INFO:root:[  200] 96.915% (loss: 0.205) [mem: 2.30e+04]
INFO:root:[  220] 97.014% (loss: 0.198) [mem: 2.30e+04]
INFO:root:[  240] 97.178% (loss: 0.192) [mem: 2.30e+04]
INFO:root:[  260] 97.011% (loss: 0.212) [mem: 2.30e+04]
INFO:root:[  280] 97.028% (loss: 0.211) [mem: 2.30e+04]
INFO:root:[  300] 96.910% (loss: 0.223) [mem: 2.30e+04]
INFO:root:[  320] 96.916% (loss: 0.220) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.286% (loss: 0.050) [mem: 2.30e+04]
INFO:root:[   40] 99.634% (loss: 0.026) [mem: 2.30e+04]
INFO:root:[   60] 99.344% (loss: 0.042) [mem: 2.30e+04]
INFO:root:[   80] 99.383% (loss: 0.040) [mem: 2.30e+04]
INFO:root:[   48] train: 96.922% test: 99.405% (loss: 0.219, grad_norm: 86454.734)
Epoch 48: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 49
INFO:root:[    0] 95.000% (loss: 0.120) [mem: 2.30e+04]
INFO:root:[   20] 95.952% (loss: 0.255) [mem: 2.30e+04]
INFO:root:[   40] 96.829% (loss: 0.189) [mem: 2.30e+04]
INFO:root:[   60] 97.131% (loss: 0.196) [mem: 2.30e+04]
INFO:root:[   80] 97.284% (loss: 0.179) [mem: 2.30e+04]
INFO:root:[  100] 97.376% (loss: 0.180) [mem: 2.30e+04]
INFO:root:[  120] 97.231% (loss: 0.197) [mem: 2.30e+04]
INFO:root:[  140] 97.021% (loss: 0.212) [mem: 2.30e+04]
INFO:root:[  160] 96.988% (loss: 0.214) [mem: 2.30e+04]
INFO:root:[  180] 97.072% (loss: 0.211) [mem: 2.30e+04]
INFO:root:[  200] 97.114% (loss: 0.202) [mem: 2.30e+04]
INFO:root:[  220] 97.172% (loss: 0.196) [mem: 2.30e+04]
INFO:root:[  240] 96.971% (loss: 0.210) [mem: 2.30e+04]
INFO:root:[  260] 97.088% (loss: 0.202) [mem: 2.30e+04]
INFO:root:[  280] 96.922% (loss: 0.215) [mem: 2.30e+04]
INFO:root:[  300] 96.960% (loss: 0.209) [mem: 2.30e+04]
INFO:root:[  320] 97.072% (loss: 0.202) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.524% (loss: 0.013) [mem: 2.30e+04]
INFO:root:[   40] 99.756% (loss: 0.007) [mem: 2.30e+04]
INFO:root:[   60] 99.672% (loss: 0.013) [mem: 2.30e+04]
INFO:root:[   80] 99.753% (loss: 0.010) [mem: 2.30e+04]
INFO:root:[   49] train: 97.044% test: 99.762% (loss: 0.203, grad_norm: 60520.840)
Epoch 49: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 50
INFO:root:[    0] 100.000% (loss: 0.008) [mem: 2.30e+04]
INFO:root:[   20] 96.667% (loss: 0.315) [mem: 2.30e+04]
INFO:root:[   40] 96.829% (loss: 0.250) [mem: 2.30e+04]
INFO:root:[   60] 96.557% (loss: 0.233) [mem: 2.30e+04]
INFO:root:[   80] 96.790% (loss: 0.233) [mem: 2.30e+04]
INFO:root:[  100] 97.178% (loss: 0.200) [mem: 2.30e+04]
INFO:root:[  120] 96.860% (loss: 0.225) [mem: 2.30e+04]
INFO:root:[  140] 96.844% (loss: 0.213) [mem: 2.30e+04]
INFO:root:[  160] 97.019% (loss: 0.198) [mem: 2.30e+04]
INFO:root:[  180] 97.044% (loss: 0.192) [mem: 2.30e+04]
INFO:root:[  200] 96.841% (loss: 0.208) [mem: 2.30e+04]
INFO:root:[  220] 96.742% (loss: 0.213) [mem: 2.30e+04]
INFO:root:[  240] 96.701% (loss: 0.216) [mem: 2.30e+04]
NaN or Inf in gradient of model.patch_embed.proj.weight
NaN or Inf in gradient of model.patch_embed.proj.bias
NaN or Inf in gradient of model.blocks.0.norm1.weight
NaN or Inf in gradient of model.blocks.0.norm1.bias
NaN or Inf in gradient of model.blocks.0.attn.qkv.weight
NaN or Inf in gradient of model.blocks.0.attn.qkv.bias
NaN or Inf in gradient of model.blocks.0.attn.proj.weight
NaN or Inf in gradient of model.blocks.0.attn.proj.bias
NaN or Inf in gradient of model.blocks.0.norm2.weight
NaN or Inf in gradient of model.blocks.0.norm2.bias
NaN or Inf in gradient of model.blocks.0.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.0.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.0.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.0.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.1.norm1.weight
NaN or Inf in gradient of model.blocks.1.norm1.bias
NaN or Inf in gradient of model.blocks.1.attn.qkv.weight
NaN or Inf in gradient of model.blocks.1.attn.qkv.bias
NaN or Inf in gradient of model.blocks.1.attn.proj.weight
NaN or Inf in gradient of model.blocks.1.attn.proj.bias
NaN or Inf in gradient of model.blocks.1.norm2.weight
NaN or Inf in gradient of model.blocks.1.norm2.bias
NaN or Inf in gradient of model.blocks.1.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.1.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.1.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.1.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.2.norm1.weight
NaN or Inf in gradient of model.blocks.2.norm1.bias
NaN or Inf in gradient of model.blocks.2.attn.qkv.weight
NaN or Inf in gradient of model.blocks.2.attn.qkv.bias
NaN or Inf in gradient of model.blocks.2.attn.proj.weight
NaN or Inf in gradient of model.blocks.2.attn.proj.bias
NaN or Inf in gradient of model.blocks.2.norm2.weight
NaN or Inf in gradient of model.blocks.2.norm2.bias
NaN or Inf in gradient of model.blocks.2.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.2.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.2.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.2.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.3.norm1.weight
NaN or Inf in gradient of model.blocks.3.norm1.bias
NaN or Inf in gradient of model.blocks.3.attn.qkv.weight
NaN or Inf in gradient of model.blocks.3.attn.qkv.bias
NaN or Inf in gradient of model.blocks.3.attn.proj.weight
NaN or Inf in gradient of model.blocks.3.attn.proj.bias
NaN or Inf in gradient of model.blocks.3.norm2.weight
NaN or Inf in gradient of model.blocks.3.norm2.bias
NaN or Inf in gradient of model.blocks.3.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.3.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.3.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.3.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.4.norm1.weight
NaN or Inf in gradient of model.blocks.4.norm1.bias
NaN or Inf in gradient of model.blocks.4.attn.qkv.weight
NaN or Inf in gradient of model.blocks.4.attn.qkv.bias
NaN or Inf in gradient of model.blocks.4.attn.proj.weight
NaN or Inf in gradient of model.blocks.4.attn.proj.bias
NaN or Inf in gradient of model.blocks.4.norm2.weight
NaN or Inf in gradient of model.blocks.4.norm2.bias
NaN or Inf in gradient of model.blocks.4.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.4.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.4.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.4.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.5.norm1.weight
NaN or Inf in gradient of model.blocks.5.norm1.bias
NaN or Inf in gradient of model.blocks.5.attn.qkv.weight
NaN or Inf in gradient of model.blocks.5.attn.qkv.bias
NaN or Inf in gradient of model.blocks.5.attn.proj.weight
NaN or Inf in gradient of model.blocks.5.attn.proj.bias
NaN or Inf in gradient of model.blocks.5.norm2.weight
NaN or Inf in gradient of model.blocks.5.norm2.bias
NaN or Inf in gradient of model.blocks.5.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.5.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.5.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.5.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.6.norm1.weight
NaN or Inf in gradient of model.blocks.6.norm1.bias
NaN or Inf in gradient of model.blocks.6.attn.qkv.weight
NaN or Inf in gradient of model.blocks.6.attn.qkv.bias
NaN or Inf in gradient of model.blocks.6.attn.proj.weight
NaN or Inf in gradient of model.blocks.6.attn.proj.bias
NaN or Inf in gradient of model.blocks.6.norm2.weight
NaN or Inf in gradient of model.blocks.6.norm2.bias
NaN or Inf in gradient of model.blocks.6.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.6.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.6.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.6.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.7.norm1.weight
NaN or Inf in gradient of model.blocks.7.norm1.bias
NaN or Inf in gradient of model.blocks.7.attn.qkv.weight
NaN or Inf in gradient of model.blocks.7.attn.qkv.bias
NaN or Inf in gradient of model.blocks.7.attn.proj.weight
NaN or Inf in gradient of model.blocks.7.attn.proj.bias
NaN or Inf in gradient of model.blocks.7.norm2.weight
NaN or Inf in gradient of model.blocks.7.norm2.bias
NaN or Inf in gradient of model.blocks.7.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.7.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.7.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.7.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.8.norm1.weight
NaN or Inf in gradient of model.blocks.8.norm1.bias
NaN or Inf in gradient of model.blocks.8.attn.qkv.weight
NaN or Inf in gradient of model.blocks.8.attn.qkv.bias
NaN or Inf in gradient of model.blocks.8.attn.proj.weight
NaN or Inf in gradient of model.blocks.8.attn.proj.bias
NaN or Inf in gradient of model.blocks.8.norm2.weight
NaN or Inf in gradient of model.blocks.8.norm2.bias
NaN or Inf in gradient of model.blocks.8.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.8.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.8.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.8.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.9.norm1.weight
NaN or Inf in gradient of model.blocks.9.norm1.bias
NaN or Inf in gradient of model.blocks.9.attn.qkv.weight
NaN or Inf in gradient of model.blocks.9.attn.qkv.bias
NaN or Inf in gradient of model.blocks.9.attn.proj.weight
NaN or Inf in gradient of model.blocks.9.attn.proj.bias
NaN or Inf in gradient of model.blocks.9.norm2.weight
NaN or Inf in gradient of model.blocks.9.norm2.bias
NaN or Inf in gradient of model.blocks.9.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.9.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.9.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.9.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.10.norm1.weight
NaN or Inf in gradient of model.blocks.10.norm1.bias
NaN or Inf in gradient of model.blocks.10.attn.qkv.weight
NaN or Inf in gradient of model.blocks.10.attn.qkv.bias
NaN or Inf in gradient of model.blocks.10.attn.proj.weight
NaN or Inf in gradient of model.blocks.10.attn.proj.bias
NaN or Inf in gradient of model.blocks.10.norm2.weight
NaN or Inf in gradient of model.blocks.10.norm2.bias
NaN or Inf in gradient of model.blocks.10.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.10.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.10.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.10.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.11.norm1.weight
NaN or Inf in gradient of model.blocks.11.norm1.bias
NaN or Inf in gradient of model.blocks.11.attn.qkv.weight
NaN or Inf in gradient of model.blocks.11.attn.qkv.bias
NaN or Inf in gradient of model.blocks.11.attn.proj.weight
NaN or Inf in gradient of model.blocks.11.attn.proj.bias
NaN or Inf in gradient of model.blocks.11.norm2.weight
NaN or Inf in gradient of model.blocks.11.norm2.bias
NaN or Inf in gradient of model.blocks.11.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.11.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.11.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.11.mlp.fc2.bias
NaN or Inf in gradient of model.norm.weight
NaN or Inf in gradient of model.norm.bias
NaN or Inf in gradient of module.pooler.query_tokens
NaN or Inf in gradient of module.pooler.cross_attention_block.norm1.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.norm1.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.q.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.q.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.kv.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.kv.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.norm2.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.norm2.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc1.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc1.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc2.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc2.bias
NaN or Inf in gradient of module.linear.weight
NaN or Inf in gradient of module.linear.bias
INFO:root:[  260] 96.743% (loss: 0.211) [mem: 2.30e+04]
INFO:root:[  280] 96.744% (loss: 0.207) [mem: 2.30e+04]
INFO:root:[  300] 96.645% (loss: 0.206) [mem: 2.30e+04]
INFO:root:[  320] 96.651% (loss: 0.209) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.524% (loss: 0.043) [mem: 2.30e+04]
INFO:root:[   40] 99.756% (loss: 0.022) [mem: 2.30e+04]
INFO:root:[   60] 99.672% (loss: 0.026) [mem: 2.30e+04]
INFO:root:[   80] 99.753% (loss: 0.019) [mem: 2.30e+04]
INFO:root:[   50] train: 96.677% test: 99.762% (loss: 0.207, grad_norm: nan)
Epoch 50: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 51
INFO:root:[    0] 100.000% (loss: 0.003) [mem: 2.30e+04]
INFO:root:[   20] 97.857% (loss: 0.073) [mem: 2.30e+04]
INFO:root:[   40] 97.683% (loss: 0.103) [mem: 2.30e+04]
INFO:root:[   60] 97.049% (loss: 0.161) [mem: 2.30e+04]
INFO:root:[   80] 97.037% (loss: 0.156) [mem: 2.30e+04]
INFO:root:[  100] 96.683% (loss: 0.180) [mem: 2.30e+04]
INFO:root:[  120] 96.736% (loss: 0.186) [mem: 2.30e+04]
INFO:root:[  140] 96.915% (loss: 0.184) [mem: 2.30e+04]
INFO:root:[  160] 96.988% (loss: 0.177) [mem: 2.30e+04]
INFO:root:[  180] 97.072% (loss: 0.176) [mem: 2.30e+04]
INFO:root:[  200] 96.940% (loss: 0.196) [mem: 2.30e+04]
INFO:root:[  220] 96.787% (loss: 0.210) [mem: 2.30e+04]
INFO:root:[  240] 96.660% (loss: 0.215) [mem: 2.30e+04]
INFO:root:[  260] 96.494% (loss: 0.218) [mem: 2.30e+04]
INFO:root:[  280] 96.459% (loss: 0.216) [mem: 2.30e+04]
INFO:root:[  300] 96.329% (loss: 0.219) [mem: 2.30e+04]
INFO:root:[  320] 96.433% (loss: 0.210) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.524% (loss: 0.013) [mem: 2.30e+04]
INFO:root:[   40] 99.756% (loss: 0.006) [mem: 2.30e+04]
INFO:root:[   60] 99.754% (loss: 0.010) [mem: 2.30e+04]
INFO:root:[   80] 99.815% (loss: 0.008) [mem: 2.30e+04]
INFO:root:[   51] train: 96.446% test: 99.762% (loss: 0.208, grad_norm: 33122.059)
Epoch 51: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 52
INFO:root:[    0] 95.000% (loss: 0.213) [mem: 2.30e+04]
INFO:root:[   20] 95.714% (loss: 0.189) [mem: 2.30e+04]
INFO:root:[   40] 96.341% (loss: 0.171) [mem: 2.30e+04]
INFO:root:[   60] 95.738% (loss: 0.202) [mem: 2.30e+04]
INFO:root:[   80] 95.617% (loss: 0.221) [mem: 2.30e+04]
INFO:root:[  100] 95.693% (loss: 0.224) [mem: 2.30e+04]
INFO:root:[  120] 95.785% (loss: 0.214) [mem: 2.30e+04]
INFO:root:[  140] 95.638% (loss: 0.215) [mem: 2.30e+04]
INFO:root:[  160] 95.621% (loss: 0.214) [mem: 2.30e+04]
INFO:root:[  180] 95.635% (loss: 0.216) [mem: 2.30e+04]
INFO:root:[  200] 95.473% (loss: 0.235) [mem: 2.30e+04]
INFO:root:[  220] 95.385% (loss: 0.231) [mem: 2.30e+04]
INFO:root:[  240] 95.436% (loss: 0.229) [mem: 2.30e+04]
INFO:root:[  260] 95.421% (loss: 0.220) [mem: 2.30e+04]
INFO:root:[  280] 95.409% (loss: 0.218) [mem: 2.30e+04]
INFO:root:[  300] 95.482% (loss: 0.217) [mem: 2.30e+04]
INFO:root:[  320] 95.623% (loss: 0.211) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.524% (loss: 0.018) [mem: 2.30e+04]
INFO:root:[   40] 99.634% (loss: 0.014) [mem: 2.30e+04]
INFO:root:[   60] 99.508% (loss: 0.020) [mem: 2.30e+04]
INFO:root:[   80] 99.383% (loss: 0.035) [mem: 2.30e+04]
INFO:root:[   52] train: 95.628% test: 99.405% (loss: 0.212, grad_norm: 35509.746)
Epoch 52: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 53
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 96.190% (loss: 0.258) [mem: 2.30e+04]
INFO:root:[   40] 96.585% (loss: 0.227) [mem: 2.30e+04]
INFO:root:[   60] 96.639% (loss: 0.248) [mem: 2.30e+04]
INFO:root:[   80] 96.667% (loss: 0.232) [mem: 2.30e+04]
INFO:root:[  100] 96.584% (loss: 0.232) [mem: 2.30e+04]
INFO:root:[  120] 96.364% (loss: 0.239) [mem: 2.30e+04]
INFO:root:[  140] 96.383% (loss: 0.224) [mem: 2.30e+04]
INFO:root:[  160] 96.149% (loss: 0.233) [mem: 2.30e+04]
INFO:root:[  180] 96.215% (loss: 0.220) [mem: 2.30e+04]
INFO:root:[  200] 96.393% (loss: 0.211) [mem: 2.30e+04]
INFO:root:[  220] 96.357% (loss: 0.208) [mem: 2.30e+04]
INFO:root:[  240] 96.432% (loss: 0.204) [mem: 2.30e+04]
INFO:root:[  260] 96.379% (loss: 0.207) [mem: 2.30e+04]
INFO:root:[  280] 96.317% (loss: 0.220) [mem: 2.30e+04]
INFO:root:[  300] 96.246% (loss: 0.230) [mem: 2.30e+04]
INFO:root:[  320] 96.262% (loss: 0.227) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 98.571% (loss: 0.054) [mem: 2.30e+04]
INFO:root:[   40] 99.268% (loss: 0.028) [mem: 2.30e+04]
INFO:root:[   60] 99.262% (loss: 0.032) [mem: 2.30e+04]
INFO:root:[   80] 99.383% (loss: 0.027) [mem: 2.30e+04]
INFO:root:[   53] train: 96.276% test: 99.345% (loss: 0.226, grad_norm: 29804.807)
Epoch 53: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 54
INFO:root:[    0] 90.000% (loss: 0.606) [mem: 2.30e+04]
INFO:root:[   20] 96.429% (loss: 0.184) [mem: 2.30e+04]
INFO:root:[   40] 95.976% (loss: 0.241) [mem: 2.30e+04]
INFO:root:[   60] 95.902% (loss: 0.245) [mem: 2.30e+04]
INFO:root:[   80] 96.235% (loss: 0.232) [mem: 2.30e+04]
INFO:root:[  100] 95.941% (loss: 0.225) [mem: 2.30e+04]
INFO:root:[  120] 96.074% (loss: 0.223) [mem: 2.30e+04]
INFO:root:[  140] 96.099% (loss: 0.227) [mem: 2.30e+04]
INFO:root:[  160] 95.745% (loss: 0.228) [mem: 2.30e+04]
INFO:root:[  180] 95.746% (loss: 0.223) [mem: 2.30e+04]
INFO:root:[  200] 95.846% (loss: 0.210) [mem: 2.30e+04]
INFO:root:[  220] 95.950% (loss: 0.202) [mem: 2.30e+04]
INFO:root:[  240] 96.079% (loss: 0.195) [mem: 2.30e+04]
INFO:root:[  260] 96.188% (loss: 0.192) [mem: 2.30e+04]
INFO:root:[  280] 96.246% (loss: 0.191) [mem: 2.30e+04]
INFO:root:[  300] 96.312% (loss: 0.187) [mem: 2.30e+04]
INFO:root:[  320] 96.308% (loss: 0.189) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.762% (loss: 0.033) [mem: 2.30e+04]
INFO:root:[   40] 99.634% (loss: 0.020) [mem: 2.30e+04]
INFO:root:[   60] 99.344% (loss: 0.038) [mem: 2.30e+04]
INFO:root:[   80] 99.444% (loss: 0.037) [mem: 2.30e+04]
INFO:root:[   54] train: 96.307% test: 99.464% (loss: 0.188, grad_norm: 26313.695)
Epoch 54: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 55
INFO:root:[    0] 95.000% (loss: 0.132) [mem: 2.30e+04]
INFO:root:[   20] 96.190% (loss: 0.168) [mem: 2.30e+04]
INFO:root:[   40] 95.976% (loss: 0.178) [mem: 2.30e+04]
INFO:root:[   60] 95.984% (loss: 0.194) [mem: 2.30e+04]
INFO:root:[   80] 96.235% (loss: 0.192) [mem: 2.30e+04]
INFO:root:[  100] 96.386% (loss: 0.191) [mem: 2.30e+04]
INFO:root:[  120] 96.281% (loss: 0.197) [mem: 2.30e+04]
INFO:root:[  140] 96.206% (loss: 0.204) [mem: 2.30e+04]
INFO:root:[  160] 96.335% (loss: 0.195) [mem: 2.30e+04]
INFO:root:[  180] 96.298% (loss: 0.201) [mem: 2.30e+04]
INFO:root:[  200] 96.169% (loss: 0.213) [mem: 2.30e+04]
INFO:root:[  220] 96.109% (loss: 0.214) [mem: 2.30e+04]
INFO:root:[  240] 96.058% (loss: 0.215) [mem: 2.30e+04]
INFO:root:[  260] 96.092% (loss: 0.212) [mem: 2.30e+04]
INFO:root:[  280] 96.121% (loss: 0.214) [mem: 2.30e+04]
INFO:root:[  300] 96.213% (loss: 0.207) [mem: 2.30e+04]
INFO:root:[  320] 96.293% (loss: 0.201) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 98.810% (loss: 0.145) [mem: 2.30e+04]
INFO:root:[   40] 99.146% (loss: 0.077) [mem: 2.30e+04]
INFO:root:[   60] 98.934% (loss: 0.091) [mem: 2.30e+04]
INFO:root:[   80] 99.198% (loss: 0.068) [mem: 2.30e+04]
INFO:root:[   55] train: 96.323% test: 99.226% (loss: 0.200, grad_norm: 27917.893)
Epoch 55: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 56
INFO:root:[    0] 100.000% (loss: 0.003) [mem: 2.30e+04]
INFO:root:[   20] 98.095% (loss: 0.183) [mem: 2.30e+04]
INFO:root:[   40] 97.317% (loss: 0.165) [mem: 2.30e+04]
INFO:root:[   60] 97.049% (loss: 0.187) [mem: 2.30e+04]
INFO:root:[   80] 96.728% (loss: 0.201) [mem: 2.30e+04]
INFO:root:[  100] 96.683% (loss: 0.226) [mem: 2.30e+04]
INFO:root:[  120] 96.860% (loss: 0.211) [mem: 2.30e+04]
INFO:root:[  140] 96.950% (loss: 0.193) [mem: 2.30e+04]
INFO:root:[  160] 96.894% (loss: 0.183) [mem: 2.30e+04]
INFO:root:[  180] 96.906% (loss: 0.178) [mem: 2.30e+04]
INFO:root:[  200] 96.965% (loss: 0.175) [mem: 2.30e+04]
INFO:root:[  220] 96.923% (loss: 0.178) [mem: 2.30e+04]
INFO:root:[  240] 96.867% (loss: 0.188) [mem: 2.30e+04]
INFO:root:[  260] 96.628% (loss: 0.201) [mem: 2.30e+04]
INFO:root:[  280] 96.673% (loss: 0.196) [mem: 2.30e+04]
INFO:root:[  300] 96.694% (loss: 0.192) [mem: 2.30e+04]
INFO:root:[  320] 96.667% (loss: 0.193) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.524% (loss: 0.006) [mem: 2.30e+04]
INFO:root:[   40] 99.756% (loss: 0.003) [mem: 2.30e+04]
INFO:root:[   60] 99.672% (loss: 0.007) [mem: 2.30e+04]
INFO:root:[   80] 99.753% (loss: 0.006) [mem: 2.30e+04]
INFO:root:[   56] train: 96.677% test: 99.762% (loss: 0.192, grad_norm: 26643.248)
Epoch 56: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 57
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 96.667% (loss: 0.197) [mem: 2.30e+04]
INFO:root:[   40] 96.707% (loss: 0.218) [mem: 2.30e+04]
INFO:root:[   60] 96.885% (loss: 0.198) [mem: 2.30e+04]
INFO:root:[   80] 96.728% (loss: 0.202) [mem: 2.30e+04]
INFO:root:[  100] 96.485% (loss: 0.213) [mem: 2.30e+04]
INFO:root:[  120] 96.488% (loss: 0.218) [mem: 2.30e+04]
INFO:root:[  140] 96.702% (loss: 0.197) [mem: 2.30e+04]
INFO:root:[  160] 96.677% (loss: 0.197) [mem: 2.30e+04]
INFO:root:[  180] 96.575% (loss: 0.196) [mem: 2.30e+04]
INFO:root:[  200] 96.791% (loss: 0.183) [mem: 2.30e+04]
INFO:root:[  220] 96.878% (loss: 0.176) [mem: 2.30e+04]
INFO:root:[  240] 96.992% (loss: 0.167) [mem: 2.30e+04]
INFO:root:[  260] 96.992% (loss: 0.164) [mem: 2.30e+04]
INFO:root:[  280] 97.011% (loss: 0.162) [mem: 2.30e+04]
INFO:root:[  300] 97.027% (loss: 0.164) [mem: 2.30e+04]
INFO:root:[  320] 97.009% (loss: 0.168) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 98.810% (loss: 0.042) [mem: 2.30e+04]
INFO:root:[   40] 99.390% (loss: 0.022) [mem: 2.30e+04]
INFO:root:[   60] 99.098% (loss: 0.044) [mem: 2.30e+04]
INFO:root:[   80] 99.136% (loss: 0.046) [mem: 2.30e+04]
INFO:root:[   57] train: 97.015% test: 99.167% (loss: 0.168, grad_norm: 46122.996)
Epoch 57: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 58
INFO:root:[    0] 95.000% (loss: 0.086) [mem: 2.30e+04]
INFO:root:[   20] 97.381% (loss: 0.122) [mem: 2.30e+04]
INFO:root:[   40] 97.195% (loss: 0.172) [mem: 2.30e+04]
INFO:root:[   60] 97.213% (loss: 0.190) [mem: 2.30e+04]
INFO:root:[   80] 97.222% (loss: 0.176) [mem: 2.30e+04]
INFO:root:[  100] 97.178% (loss: 0.177) [mem: 2.30e+04]
INFO:root:[  120] 97.149% (loss: 0.181) [mem: 2.30e+04]
INFO:root:[  140] 97.163% (loss: 0.184) [mem: 2.30e+04]
INFO:root:[  160] 97.298% (loss: 0.175) [mem: 2.30e+04]
INFO:root:[  180] 97.099% (loss: 0.177) [mem: 2.30e+04]
INFO:root:[  200] 97.189% (loss: 0.173) [mem: 2.30e+04]
INFO:root:[  220] 97.217% (loss: 0.177) [mem: 2.30e+04]
INFO:root:[  240] 97.178% (loss: 0.178) [mem: 2.30e+04]
INFO:root:[  260] 97.222% (loss: 0.174) [mem: 2.30e+04]
INFO:root:[  280] 97.260% (loss: 0.170) [mem: 2.30e+04]
INFO:root:[  300] 97.243% (loss: 0.171) [mem: 2.30e+04]
INFO:root:[  320] 97.243% (loss: 0.172) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.762% (loss: 0.009) [mem: 2.30e+04]
INFO:root:[   40] 99.878% (loss: 0.005) [mem: 2.30e+04]
INFO:root:[   60] 99.508% (loss: 0.020) [mem: 2.30e+04]
INFO:root:[   80] 99.506% (loss: 0.024) [mem: 2.30e+04]
INFO:root:[   58] train: 97.262% test: 99.524% (loss: 0.170, grad_norm: 43944.281)
Epoch 58: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 59
INFO:root:[    0] 95.000% (loss: 0.323) [mem: 2.30e+04]
INFO:root:[   20] 97.381% (loss: 0.145) [mem: 2.30e+04]
INFO:root:[   40] 97.317% (loss: 0.139) [mem: 2.30e+04]
INFO:root:[   60] 96.967% (loss: 0.165) [mem: 2.30e+04]
INFO:root:[   80] 97.222% (loss: 0.162) [mem: 2.30e+04]
INFO:root:[  100] 97.178% (loss: 0.176) [mem: 2.30e+04]
INFO:root:[  120] 97.273% (loss: 0.174) [mem: 2.30e+04]
NaN or Inf in gradient of model.patch_embed.proj.weight
NaN or Inf in gradient of model.patch_embed.proj.bias
NaN or Inf in gradient of model.blocks.0.norm1.weight
NaN or Inf in gradient of model.blocks.0.norm1.bias
NaN or Inf in gradient of model.blocks.0.attn.qkv.weight
NaN or Inf in gradient of model.blocks.0.attn.qkv.bias
NaN or Inf in gradient of model.blocks.0.attn.proj.weight
NaN or Inf in gradient of model.blocks.0.attn.proj.bias
NaN or Inf in gradient of model.blocks.0.norm2.weight
NaN or Inf in gradient of model.blocks.0.norm2.bias
NaN or Inf in gradient of model.blocks.0.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.0.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.0.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.0.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.1.norm1.weight
NaN or Inf in gradient of model.blocks.1.norm1.bias
NaN or Inf in gradient of model.blocks.1.attn.qkv.weight
NaN or Inf in gradient of model.blocks.1.attn.qkv.bias
NaN or Inf in gradient of model.blocks.1.attn.proj.weight
NaN or Inf in gradient of model.blocks.1.attn.proj.bias
NaN or Inf in gradient of model.blocks.1.norm2.weight
NaN or Inf in gradient of model.blocks.1.norm2.bias
NaN or Inf in gradient of model.blocks.1.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.1.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.1.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.1.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.2.norm1.weight
NaN or Inf in gradient of model.blocks.2.norm1.bias
NaN or Inf in gradient of model.blocks.2.attn.qkv.weight
NaN or Inf in gradient of model.blocks.2.attn.qkv.bias
NaN or Inf in gradient of model.blocks.2.attn.proj.weight
NaN or Inf in gradient of model.blocks.2.attn.proj.bias
NaN or Inf in gradient of model.blocks.2.norm2.weight
NaN or Inf in gradient of model.blocks.2.norm2.bias
NaN or Inf in gradient of model.blocks.2.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.2.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.2.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.2.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.3.norm1.weight
NaN or Inf in gradient of model.blocks.3.norm1.bias
NaN or Inf in gradient of model.blocks.3.attn.qkv.weight
NaN or Inf in gradient of model.blocks.3.attn.qkv.bias
NaN or Inf in gradient of model.blocks.3.attn.proj.weight
NaN or Inf in gradient of model.blocks.3.attn.proj.bias
NaN or Inf in gradient of model.blocks.3.norm2.weight
NaN or Inf in gradient of model.blocks.3.norm2.bias
NaN or Inf in gradient of model.blocks.3.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.3.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.3.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.3.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.4.norm1.weight
NaN or Inf in gradient of model.blocks.4.norm1.bias
NaN or Inf in gradient of model.blocks.4.attn.qkv.weight
NaN or Inf in gradient of model.blocks.4.attn.qkv.bias
NaN or Inf in gradient of model.blocks.4.attn.proj.weight
NaN or Inf in gradient of model.blocks.4.attn.proj.bias
NaN or Inf in gradient of model.blocks.4.norm2.weight
NaN or Inf in gradient of model.blocks.4.norm2.bias
NaN or Inf in gradient of model.blocks.4.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.4.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.4.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.4.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.5.norm1.weight
NaN or Inf in gradient of model.blocks.5.norm1.bias
NaN or Inf in gradient of model.blocks.5.attn.qkv.weight
NaN or Inf in gradient of model.blocks.5.attn.qkv.bias
NaN or Inf in gradient of model.blocks.5.attn.proj.weight
NaN or Inf in gradient of model.blocks.5.attn.proj.bias
NaN or Inf in gradient of model.blocks.5.norm2.weight
NaN or Inf in gradient of model.blocks.5.norm2.bias
NaN or Inf in gradient of model.blocks.5.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.5.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.5.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.5.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.6.norm1.weight
NaN or Inf in gradient of model.blocks.6.norm1.bias
NaN or Inf in gradient of model.blocks.6.attn.qkv.weight
NaN or Inf in gradient of model.blocks.6.attn.qkv.bias
NaN or Inf in gradient of model.blocks.6.attn.proj.weight
NaN or Inf in gradient of model.blocks.6.attn.proj.bias
NaN or Inf in gradient of model.blocks.6.norm2.weight
NaN or Inf in gradient of model.blocks.6.norm2.bias
NaN or Inf in gradient of model.blocks.6.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.6.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.6.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.6.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.7.norm1.weight
NaN or Inf in gradient of model.blocks.7.norm1.bias
NaN or Inf in gradient of model.blocks.7.attn.qkv.weight
NaN or Inf in gradient of model.blocks.7.attn.qkv.bias
NaN or Inf in gradient of model.blocks.7.attn.proj.weight
NaN or Inf in gradient of model.blocks.7.attn.proj.bias
NaN or Inf in gradient of model.blocks.7.norm2.weight
NaN or Inf in gradient of model.blocks.7.norm2.bias
NaN or Inf in gradient of model.blocks.7.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.7.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.7.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.7.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.8.norm1.weight
NaN or Inf in gradient of model.blocks.8.norm1.bias
NaN or Inf in gradient of model.blocks.8.attn.qkv.weight
NaN or Inf in gradient of model.blocks.8.attn.qkv.bias
NaN or Inf in gradient of model.blocks.8.attn.proj.weight
NaN or Inf in gradient of model.blocks.8.attn.proj.bias
NaN or Inf in gradient of model.blocks.8.norm2.weight
NaN or Inf in gradient of model.blocks.8.norm2.bias
NaN or Inf in gradient of model.blocks.8.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.8.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.8.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.8.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.9.norm1.weight
NaN or Inf in gradient of model.blocks.9.norm1.bias
NaN or Inf in gradient of model.blocks.9.attn.qkv.weight
NaN or Inf in gradient of model.blocks.9.attn.qkv.bias
NaN or Inf in gradient of model.blocks.9.attn.proj.weight
NaN or Inf in gradient of model.blocks.9.attn.proj.bias
NaN or Inf in gradient of model.blocks.9.norm2.weight
NaN or Inf in gradient of model.blocks.9.norm2.bias
NaN or Inf in gradient of model.blocks.9.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.9.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.9.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.9.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.10.norm1.weight
NaN or Inf in gradient of model.blocks.10.norm1.bias
NaN or Inf in gradient of model.blocks.10.attn.qkv.weight
NaN or Inf in gradient of model.blocks.10.attn.qkv.bias
NaN or Inf in gradient of model.blocks.10.attn.proj.weight
NaN or Inf in gradient of model.blocks.10.attn.proj.bias
NaN or Inf in gradient of model.blocks.10.norm2.weight
NaN or Inf in gradient of model.blocks.10.norm2.bias
NaN or Inf in gradient of model.blocks.10.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.10.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.10.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.10.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.11.norm1.weight
NaN or Inf in gradient of model.blocks.11.norm1.bias
NaN or Inf in gradient of model.blocks.11.attn.qkv.weight
NaN or Inf in gradient of model.blocks.11.attn.qkv.bias
NaN or Inf in gradient of model.blocks.11.attn.proj.weight
NaN or Inf in gradient of model.blocks.11.attn.proj.bias
NaN or Inf in gradient of model.blocks.11.norm2.weight
NaN or Inf in gradient of model.blocks.11.norm2.bias
NaN or Inf in gradient of model.blocks.11.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.11.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.11.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.11.mlp.fc2.bias
NaN or Inf in gradient of model.norm.weight
NaN or Inf in gradient of model.norm.bias
NaN or Inf in gradient of module.pooler.query_tokens
NaN or Inf in gradient of module.pooler.cross_attention_block.norm1.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.norm1.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.q.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.q.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.kv.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.kv.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.norm2.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.norm2.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc1.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc1.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc2.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc2.bias
NaN or Inf in gradient of module.linear.weight
NaN or Inf in gradient of module.linear.bias
INFO:root:[  140] 97.092% (loss: 0.202) [mem: 2.30e+04]
INFO:root:[  160] 97.112% (loss: 0.195) [mem: 2.30e+04]
INFO:root:[  180] 96.961% (loss: 0.202) [mem: 2.30e+04]
INFO:root:[  200] 96.940% (loss: 0.199) [mem: 2.30e+04]
INFO:root:[  220] 96.833% (loss: 0.195) [mem: 2.30e+04]
INFO:root:[  240] 96.805% (loss: 0.195) [mem: 2.30e+04]
INFO:root:[  260] 96.839% (loss: 0.192) [mem: 2.30e+04]
INFO:root:[  280] 96.833% (loss: 0.191) [mem: 2.30e+04]
INFO:root:[  300] 96.811% (loss: 0.198) [mem: 2.30e+04]
INFO:root:[  320] 96.885% (loss: 0.194) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.762% (loss: 0.052) [mem: 2.30e+04]
INFO:root:[   40] 99.756% (loss: 0.028) [mem: 2.30e+04]
INFO:root:[   60] 99.508% (loss: 0.032) [mem: 2.30e+04]
INFO:root:[   80] 99.506% (loss: 0.036) [mem: 2.30e+04]
INFO:root:[   59] train: 96.892% test: 99.524% (loss: 0.193, grad_norm: nan)
Epoch 59: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 60
INFO:root:[    0] 95.000% (loss: 0.240) [mem: 2.30e+04]
INFO:root:[   20] 96.429% (loss: 0.253) [mem: 2.30e+04]
INFO:root:[   40] 96.707% (loss: 0.208) [mem: 2.30e+04]
INFO:root:[   60] 96.393% (loss: 0.240) [mem: 2.30e+04]
INFO:root:[   80] 96.543% (loss: 0.222) [mem: 2.30e+04]
INFO:root:[  100] 96.238% (loss: 0.221) [mem: 2.30e+04]
INFO:root:[  120] 96.322% (loss: 0.221) [mem: 2.30e+04]
INFO:root:[  140] 96.312% (loss: 0.229) [mem: 2.30e+04]
INFO:root:[  160] 96.025% (loss: 0.255) [mem: 2.30e+04]
INFO:root:[  180] 96.188% (loss: 0.247) [mem: 2.30e+04]
INFO:root:[  200] 96.144% (loss: 0.256) [mem: 2.30e+04]
INFO:root:[  220] 96.199% (loss: 0.254) [mem: 2.30e+04]
INFO:root:[  240] 96.328% (loss: 0.239) [mem: 2.30e+04]
INFO:root:[  260] 96.360% (loss: 0.237) [mem: 2.30e+04]
INFO:root:[  280] 96.477% (loss: 0.230) [mem: 2.30e+04]
INFO:root:[  300] 96.545% (loss: 0.225) [mem: 2.30e+04]
INFO:root:[  320] 96.433% (loss: 0.226) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.762% (loss: 0.018) [mem: 2.30e+04]
INFO:root:[   40] 99.878% (loss: 0.009) [mem: 2.30e+04]
INFO:root:[   60] 99.836% (loss: 0.007) [mem: 2.30e+04]
INFO:root:[   80] 99.877% (loss: 0.005) [mem: 2.30e+04]
INFO:root:[   60] train: 96.446% test: 99.881% (loss: 0.226, grad_norm: 34699.953)
Epoch 60: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 61
INFO:root:[    0] 95.000% (loss: 0.825) [mem: 2.30e+04]
INFO:root:[   20] 97.619% (loss: 0.264) [mem: 2.30e+04]
INFO:root:[   40] 97.073% (loss: 0.283) [mem: 2.30e+04]
INFO:root:[   60] 95.738% (loss: 0.333) [mem: 2.30e+04]
INFO:root:[   80] 96.173% (loss: 0.280) [mem: 2.30e+04]
INFO:root:[  100] 96.386% (loss: 0.265) [mem: 2.30e+04]
INFO:root:[  120] 96.405% (loss: 0.269) [mem: 2.30e+04]
INFO:root:[  140] 96.383% (loss: 0.264) [mem: 2.30e+04]
INFO:root:[  160] 96.460% (loss: 0.256) [mem: 2.30e+04]
INFO:root:[  180] 96.409% (loss: 0.253) [mem: 2.30e+04]
INFO:root:[  200] 96.443% (loss: 0.254) [mem: 2.30e+04]
INFO:root:[  220] 96.403% (loss: 0.255) [mem: 2.30e+04]
INFO:root:[  240] 96.494% (loss: 0.246) [mem: 2.30e+04]
INFO:root:[  260] 96.437% (loss: 0.245) [mem: 2.30e+04]
INFO:root:[  280] 96.441% (loss: 0.246) [mem: 2.30e+04]
INFO:root:[  300] 96.445% (loss: 0.241) [mem: 2.30e+04]
INFO:root:[  320] 96.542% (loss: 0.233) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.524% (loss: 0.006) [mem: 2.30e+04]
INFO:root:[   40] 99.634% (loss: 0.012) [mem: 2.30e+04]
INFO:root:[   60] 99.508% (loss: 0.021) [mem: 2.30e+04]
INFO:root:[   80] 99.630% (loss: 0.016) [mem: 2.30e+04]
INFO:root:[   61] train: 96.569% test: 99.643% (loss: 0.231, grad_norm: 33604.883)
Epoch 61: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 62
INFO:root:[    0] 100.000% (loss: 0.001) [mem: 2.30e+04]
INFO:root:[   20] 96.190% (loss: 0.224) [mem: 2.30e+04]
INFO:root:[   40] 96.829% (loss: 0.205) [mem: 2.30e+04]
INFO:root:[   60] 96.967% (loss: 0.185) [mem: 2.30e+04]
INFO:root:[   80] 97.222% (loss: 0.169) [mem: 2.30e+04]
INFO:root:[  100] 97.228% (loss: 0.176) [mem: 2.30e+04]
INFO:root:[  120] 97.231% (loss: 0.184) [mem: 2.30e+04]
INFO:root:[  140] 97.234% (loss: 0.178) [mem: 2.30e+04]
INFO:root:[  160] 97.174% (loss: 0.183) [mem: 2.30e+04]
INFO:root:[  180] 97.099% (loss: 0.191) [mem: 2.30e+04]
INFO:root:[  200] 97.139% (loss: 0.184) [mem: 2.30e+04]
INFO:root:[  220] 96.991% (loss: 0.188) [mem: 2.30e+04]
INFO:root:[  240] 96.929% (loss: 0.196) [mem: 2.30e+04]
INFO:root:[  260] 96.877% (loss: 0.201) [mem: 2.30e+04]
INFO:root:[  280] 96.868% (loss: 0.202) [mem: 2.30e+04]
INFO:root:[  300] 96.927% (loss: 0.195) [mem: 2.30e+04]
INFO:root:[  320] 96.838% (loss: 0.206) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.524% (loss: 0.067) [mem: 2.30e+04]
INFO:root:[   40] 99.756% (loss: 0.034) [mem: 2.30e+04]
INFO:root:[   60] 99.672% (loss: 0.034) [mem: 2.30e+04]
INFO:root:[   80] 99.630% (loss: 0.031) [mem: 2.30e+04]
INFO:root:[   62] train: 96.846% test: 99.643% (loss: 0.204, grad_norm: 30430.803)
Epoch 62: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 63
INFO:root:[    0] 100.000% (loss: 0.018) [mem: 2.30e+04]
INFO:root:[   20] 95.714% (loss: 0.308) [mem: 2.30e+04]
INFO:root:[   40] 95.854% (loss: 0.288) [mem: 2.30e+04]
INFO:root:[   60] 96.066% (loss: 0.252) [mem: 2.30e+04]
INFO:root:[   80] 96.358% (loss: 0.231) [mem: 2.30e+04]
INFO:root:[  100] 96.089% (loss: 0.232) [mem: 2.30e+04]
INFO:root:[  120] 96.116% (loss: 0.232) [mem: 2.30e+04]
INFO:root:[  140] 96.241% (loss: 0.231) [mem: 2.30e+04]
INFO:root:[  160] 96.398% (loss: 0.225) [mem: 2.30e+04]
INFO:root:[  180] 96.519% (loss: 0.222) [mem: 2.30e+04]
INFO:root:[  200] 96.617% (loss: 0.217) [mem: 2.30e+04]
INFO:root:[  220] 96.584% (loss: 0.219) [mem: 2.30e+04]
INFO:root:[  240] 96.598% (loss: 0.220) [mem: 2.30e+04]
INFO:root:[  260] 96.648% (loss: 0.211) [mem: 2.30e+04]
INFO:root:[  280] 96.637% (loss: 0.207) [mem: 2.30e+04]
INFO:root:[  300] 96.694% (loss: 0.203) [mem: 2.30e+04]
INFO:root:[  320] 96.620% (loss: 0.202) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.762% (loss: 0.029) [mem: 2.30e+04]
INFO:root:[   40] 99.756% (loss: 0.020) [mem: 2.30e+04]
INFO:root:[   60] 99.590% (loss: 0.033) [mem: 2.30e+04]
INFO:root:[   80] 99.691% (loss: 0.025) [mem: 2.30e+04]
INFO:root:[   63] train: 96.630% test: 99.702% (loss: 0.201, grad_norm: 31902.600)
Epoch 63: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 64
INFO:root:[    0] 100.000% (loss: 0.020) [mem: 2.30e+04]
INFO:root:[   20] 97.619% (loss: 0.123) [mem: 2.30e+04]
INFO:root:[   40] 97.927% (loss: 0.125) [mem: 2.30e+04]
INFO:root:[   60] 97.869% (loss: 0.143) [mem: 2.30e+04]
INFO:root:[   80] 97.840% (loss: 0.148) [mem: 2.30e+04]
INFO:root:[  100] 97.426% (loss: 0.191) [mem: 2.30e+04]
INFO:root:[  120] 97.603% (loss: 0.173) [mem: 2.30e+04]
INFO:root:[  140] 97.589% (loss: 0.173) [mem: 2.30e+04]
INFO:root:[  160] 97.640% (loss: 0.165) [mem: 2.30e+04]
INFO:root:[  180] 97.569% (loss: 0.172) [mem: 2.30e+04]
INFO:root:[  200] 97.537% (loss: 0.172) [mem: 2.30e+04]
INFO:root:[  220] 97.534% (loss: 0.167) [mem: 2.30e+04]
INFO:root:[  240] 97.386% (loss: 0.180) [mem: 2.30e+04]
INFO:root:[  260] 97.203% (loss: 0.187) [mem: 2.30e+04]
INFO:root:[  280] 97.171% (loss: 0.187) [mem: 2.30e+04]
NaN or Inf in gradient of model.patch_embed.proj.weight
NaN or Inf in gradient of model.patch_embed.proj.bias
NaN or Inf in gradient of model.blocks.0.norm1.weight
NaN or Inf in gradient of model.blocks.0.norm1.bias
NaN or Inf in gradient of model.blocks.0.attn.qkv.weight
NaN or Inf in gradient of model.blocks.0.attn.qkv.bias
NaN or Inf in gradient of model.blocks.0.attn.proj.weight
NaN or Inf in gradient of model.blocks.0.attn.proj.bias
NaN or Inf in gradient of model.blocks.0.norm2.weight
NaN or Inf in gradient of model.blocks.0.norm2.bias
NaN or Inf in gradient of model.blocks.0.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.0.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.0.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.0.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.1.norm1.weight
NaN or Inf in gradient of model.blocks.1.norm1.bias
NaN or Inf in gradient of model.blocks.1.attn.qkv.weight
NaN or Inf in gradient of model.blocks.1.attn.qkv.bias
NaN or Inf in gradient of model.blocks.1.attn.proj.weight
NaN or Inf in gradient of model.blocks.1.attn.proj.bias
NaN or Inf in gradient of model.blocks.1.norm2.weight
NaN or Inf in gradient of model.blocks.1.norm2.bias
NaN or Inf in gradient of model.blocks.1.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.1.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.1.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.1.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.2.norm1.weight
NaN or Inf in gradient of model.blocks.2.norm1.bias
NaN or Inf in gradient of model.blocks.2.attn.qkv.weight
NaN or Inf in gradient of model.blocks.2.attn.qkv.bias
NaN or Inf in gradient of model.blocks.2.attn.proj.weight
NaN or Inf in gradient of model.blocks.2.attn.proj.bias
NaN or Inf in gradient of model.blocks.2.norm2.weight
NaN or Inf in gradient of model.blocks.2.norm2.bias
NaN or Inf in gradient of model.blocks.2.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.2.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.2.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.2.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.3.norm1.weight
NaN or Inf in gradient of model.blocks.3.norm1.bias
NaN or Inf in gradient of model.blocks.3.attn.qkv.weight
NaN or Inf in gradient of model.blocks.3.attn.qkv.bias
NaN or Inf in gradient of model.blocks.3.attn.proj.weight
NaN or Inf in gradient of model.blocks.3.attn.proj.bias
NaN or Inf in gradient of model.blocks.3.norm2.weight
NaN or Inf in gradient of model.blocks.3.norm2.bias
NaN or Inf in gradient of model.blocks.3.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.3.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.3.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.3.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.4.norm1.weight
NaN or Inf in gradient of model.blocks.4.norm1.bias
NaN or Inf in gradient of model.blocks.4.attn.qkv.weight
NaN or Inf in gradient of model.blocks.4.attn.qkv.bias
NaN or Inf in gradient of model.blocks.4.attn.proj.weight
NaN or Inf in gradient of model.blocks.4.attn.proj.bias
NaN or Inf in gradient of model.blocks.4.norm2.weight
NaN or Inf in gradient of model.blocks.4.norm2.bias
NaN or Inf in gradient of model.blocks.4.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.4.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.4.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.4.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.5.norm1.weight
NaN or Inf in gradient of model.blocks.5.norm1.bias
NaN or Inf in gradient of model.blocks.5.attn.qkv.weight
NaN or Inf in gradient of model.blocks.5.attn.qkv.bias
NaN or Inf in gradient of model.blocks.5.attn.proj.weight
NaN or Inf in gradient of model.blocks.5.attn.proj.bias
NaN or Inf in gradient of model.blocks.5.norm2.weight
NaN or Inf in gradient of model.blocks.5.norm2.bias
NaN or Inf in gradient of model.blocks.5.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.5.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.5.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.5.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.6.norm1.weight
NaN or Inf in gradient of model.blocks.6.norm1.bias
NaN or Inf in gradient of model.blocks.6.attn.qkv.weight
NaN or Inf in gradient of model.blocks.6.attn.qkv.bias
NaN or Inf in gradient of model.blocks.6.attn.proj.weight
NaN or Inf in gradient of model.blocks.6.attn.proj.bias
NaN or Inf in gradient of model.blocks.6.norm2.weight
NaN or Inf in gradient of model.blocks.6.norm2.bias
NaN or Inf in gradient of model.blocks.6.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.6.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.6.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.6.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.7.norm1.weight
NaN or Inf in gradient of model.blocks.7.norm1.bias
NaN or Inf in gradient of model.blocks.7.attn.qkv.weight
NaN or Inf in gradient of model.blocks.7.attn.qkv.bias
NaN or Inf in gradient of model.blocks.7.attn.proj.weight
NaN or Inf in gradient of model.blocks.7.attn.proj.bias
NaN or Inf in gradient of model.blocks.7.norm2.weight
NaN or Inf in gradient of model.blocks.7.norm2.bias
NaN or Inf in gradient of model.blocks.7.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.7.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.7.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.7.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.8.norm1.weight
NaN or Inf in gradient of model.blocks.8.norm1.bias
NaN or Inf in gradient of model.blocks.8.attn.qkv.weight
NaN or Inf in gradient of model.blocks.8.attn.qkv.bias
NaN or Inf in gradient of model.blocks.8.attn.proj.weight
NaN or Inf in gradient of model.blocks.8.attn.proj.bias
NaN or Inf in gradient of model.blocks.8.norm2.weight
NaN or Inf in gradient of model.blocks.8.norm2.bias
NaN or Inf in gradient of model.blocks.8.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.8.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.8.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.8.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.9.norm1.weight
NaN or Inf in gradient of model.blocks.9.norm1.bias
NaN or Inf in gradient of model.blocks.9.attn.qkv.weight
NaN or Inf in gradient of model.blocks.9.attn.qkv.bias
NaN or Inf in gradient of model.blocks.9.attn.proj.weight
NaN or Inf in gradient of model.blocks.9.attn.proj.bias
NaN or Inf in gradient of model.blocks.9.norm2.weight
NaN or Inf in gradient of model.blocks.9.norm2.bias
NaN or Inf in gradient of model.blocks.9.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.9.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.9.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.9.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.10.norm1.weight
NaN or Inf in gradient of model.blocks.10.norm1.bias
NaN or Inf in gradient of model.blocks.10.attn.qkv.weight
NaN or Inf in gradient of model.blocks.10.attn.qkv.bias
NaN or Inf in gradient of model.blocks.10.attn.proj.weight
NaN or Inf in gradient of model.blocks.10.attn.proj.bias
NaN or Inf in gradient of model.blocks.10.norm2.weight
NaN or Inf in gradient of model.blocks.10.norm2.bias
NaN or Inf in gradient of model.blocks.10.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.10.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.10.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.10.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.11.norm1.weight
NaN or Inf in gradient of model.blocks.11.norm1.bias
NaN or Inf in gradient of model.blocks.11.attn.qkv.weight
NaN or Inf in gradient of model.blocks.11.attn.qkv.bias
NaN or Inf in gradient of model.blocks.11.attn.proj.weight
NaN or Inf in gradient of model.blocks.11.attn.proj.bias
NaN or Inf in gradient of model.blocks.11.norm2.weight
NaN or Inf in gradient of model.blocks.11.norm2.bias
NaN or Inf in gradient of model.blocks.11.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.11.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.11.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.11.mlp.fc2.bias
NaN or Inf in gradient of model.norm.weight
NaN or Inf in gradient of model.norm.bias
NaN or Inf in gradient of module.pooler.query_tokens
NaN or Inf in gradient of module.pooler.cross_attention_block.norm1.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.norm1.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.q.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.q.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.kv.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.kv.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.norm2.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.norm2.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc1.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc1.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc2.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc2.bias
NaN or Inf in gradient of module.linear.weight
NaN or Inf in gradient of module.linear.bias
INFO:root:[  300] 97.043% (loss: 0.195) [mem: 2.30e+04]
INFO:root:[  320] 97.150% (loss: 0.187) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.048% (loss: 0.067) [mem: 2.30e+04]
INFO:root:[   40] 99.512% (loss: 0.034) [mem: 2.30e+04]
INFO:root:[   60] 99.262% (loss: 0.062) [mem: 2.30e+04]
INFO:root:[   80] 99.321% (loss: 0.058) [mem: 2.30e+04]
INFO:root:[   64] train: 97.168% test: 99.345% (loss: 0.186, grad_norm: nan)
Epoch 64: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 65
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 96.190% (loss: 0.210) [mem: 2.30e+04]
INFO:root:[   40] 96.585% (loss: 0.222) [mem: 2.30e+04]
INFO:root:[   60] 96.639% (loss: 0.217) [mem: 2.30e+04]
INFO:root:[   80] 96.914% (loss: 0.192) [mem: 2.30e+04]
INFO:root:[  100] 96.733% (loss: 0.199) [mem: 2.30e+04]
INFO:root:[  120] 96.653% (loss: 0.205) [mem: 2.30e+04]
INFO:root:[  140] 96.773% (loss: 0.209) [mem: 2.30e+04]
INFO:root:[  160] 96.832% (loss: 0.208) [mem: 2.30e+04]
INFO:root:[  180] 96.878% (loss: 0.205) [mem: 2.30e+04]
INFO:root:[  200] 96.866% (loss: 0.210) [mem: 2.30e+04]
INFO:root:[  220] 96.810% (loss: 0.212) [mem: 2.30e+04]
INFO:root:[  240] 96.846% (loss: 0.211) [mem: 2.30e+04]
INFO:root:[  260] 96.782% (loss: 0.214) [mem: 2.30e+04]
INFO:root:[  280] 96.868% (loss: 0.206) [mem: 2.30e+04]
INFO:root:[  300] 96.993% (loss: 0.199) [mem: 2.30e+04]
INFO:root:[  320] 97.025% (loss: 0.198) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.286% (loss: 0.065) [mem: 2.30e+04]
INFO:root:[   40] 99.512% (loss: 0.048) [mem: 2.30e+04]
INFO:root:[   60] 99.344% (loss: 0.059) [mem: 2.30e+04]
INFO:root:[   80] 99.383% (loss: 0.048) [mem: 2.30e+04]
INFO:root:[   65] train: 97.062% test: 99.405% (loss: 0.196, grad_norm: 10912.340)
Epoch 65: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 66
INFO:root:[    0] 95.000% (loss: 0.054) [mem: 2.30e+04]
INFO:root:[   20] 97.381% (loss: 0.114) [mem: 2.30e+04]
INFO:root:[   40] 96.951% (loss: 0.181) [mem: 2.30e+04]
INFO:root:[   60] 96.967% (loss: 0.185) [mem: 2.30e+04]
INFO:root:[   80] 97.099% (loss: 0.164) [mem: 2.30e+04]
INFO:root:[  100] 97.030% (loss: 0.175) [mem: 2.30e+04]
INFO:root:[  120] 96.901% (loss: 0.176) [mem: 2.30e+04]
INFO:root:[  140] 96.844% (loss: 0.182) [mem: 2.30e+04]
INFO:root:[  160] 96.708% (loss: 0.189) [mem: 2.30e+04]
INFO:root:[  180] 96.575% (loss: 0.197) [mem: 2.30e+04]
INFO:root:[  200] 96.692% (loss: 0.190) [mem: 2.30e+04]
INFO:root:[  220] 96.674% (loss: 0.189) [mem: 2.30e+04]
INFO:root:[  240] 96.722% (loss: 0.183) [mem: 2.30e+04]
INFO:root:[  260] 96.705% (loss: 0.182) [mem: 2.30e+04]
INFO:root:[  280] 96.779% (loss: 0.179) [mem: 2.30e+04]
INFO:root:[  300] 96.877% (loss: 0.176) [mem: 2.30e+04]
INFO:root:[  320] 96.947% (loss: 0.174) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.524% (loss: 0.043) [mem: 2.30e+04]
INFO:root:[   40] 99.756% (loss: 0.022) [mem: 2.30e+04]
INFO:root:[   60] 99.672% (loss: 0.035) [mem: 2.30e+04]
INFO:root:[   80] 99.691% (loss: 0.030) [mem: 2.30e+04]
INFO:root:[   66] train: 96.954% test: 99.702% (loss: 0.174, grad_norm: 12205.376)
Epoch 66: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 67
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.048% (loss: 0.084) [mem: 2.30e+04]
INFO:root:[   40] 98.415% (loss: 0.118) [mem: 2.30e+04]
INFO:root:[   60] 97.951% (loss: 0.147) [mem: 2.30e+04]
INFO:root:[   80] 98.148% (loss: 0.128) [mem: 2.30e+04]
INFO:root:[  100] 97.673% (loss: 0.156) [mem: 2.30e+04]
INFO:root:[  120] 97.727% (loss: 0.147) [mem: 2.30e+04]
INFO:root:[  140] 97.660% (loss: 0.151) [mem: 2.30e+04]
INFO:root:[  160] 97.609% (loss: 0.152) [mem: 2.30e+04]
INFO:root:[  180] 97.459% (loss: 0.164) [mem: 2.30e+04]
INFO:root:[  200] 97.413% (loss: 0.165) [mem: 2.30e+04]
INFO:root:[  220] 97.308% (loss: 0.172) [mem: 2.30e+04]
INFO:root:[  240] 97.365% (loss: 0.171) [mem: 2.30e+04]
INFO:root:[  260] 97.414% (loss: 0.167) [mem: 2.30e+04]
INFO:root:[  280] 97.384% (loss: 0.176) [mem: 2.30e+04]
INFO:root:[  300] 97.492% (loss: 0.169) [mem: 2.30e+04]
INFO:root:[  320] 97.477% (loss: 0.166) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 100.000% (loss: 0.001) [mem: 2.30e+04]
INFO:root:[   40] 99.878% (loss: 0.002) [mem: 2.30e+04]
INFO:root:[   60] 99.754% (loss: 0.007) [mem: 2.30e+04]
INFO:root:[   80] 99.753% (loss: 0.009) [mem: 2.30e+04]
INFO:root:[   67] train: 97.461% test: 99.762% (loss: 0.166, grad_norm: 6758.618)
Epoch 67: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 68
INFO:root:[    0] 100.000% (loss: 0.021) [mem: 2.30e+04]
INFO:root:[   20] 97.857% (loss: 0.189) [mem: 2.30e+04]
INFO:root:[   40] 97.317% (loss: 0.198) [mem: 2.30e+04]
INFO:root:[   60] 97.295% (loss: 0.176) [mem: 2.30e+04]
INFO:root:[   80] 97.469% (loss: 0.168) [mem: 2.30e+04]
INFO:root:[  100] 97.129% (loss: 0.193) [mem: 2.30e+04]
INFO:root:[  120] 97.025% (loss: 0.205) [mem: 2.30e+04]
INFO:root:[  140] 96.986% (loss: 0.199) [mem: 2.30e+04]
INFO:root:[  160] 96.801% (loss: 0.206) [mem: 2.30e+04]
INFO:root:[  180] 96.796% (loss: 0.204) [mem: 2.30e+04]
INFO:root:[  200] 96.816% (loss: 0.193) [mem: 2.30e+04]
INFO:root:[  220] 96.697% (loss: 0.202) [mem: 2.30e+04]
INFO:root:[  240] 96.639% (loss: 0.200) [mem: 2.30e+04]
INFO:root:[  260] 96.743% (loss: 0.194) [mem: 2.30e+04]
INFO:root:[  280] 96.797% (loss: 0.192) [mem: 2.30e+04]
INFO:root:[  300] 96.860% (loss: 0.191) [mem: 2.30e+04]
INFO:root:[  320] 96.900% (loss: 0.191) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.048% (loss: 0.067) [mem: 2.30e+04]
INFO:root:[   40] 99.390% (loss: 0.036) [mem: 2.30e+04]
INFO:root:[   60] 99.262% (loss: 0.035) [mem: 2.30e+04]
INFO:root:[   80] 99.444% (loss: 0.026) [mem: 2.30e+04]
INFO:root:[   68] train: 96.892% test: 99.464% (loss: 0.190, grad_norm: 15785.761)
Epoch 68: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 69
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 98.333% (loss: 0.130) [mem: 2.30e+04]
INFO:root:[   40] 97.439% (loss: 0.186) [mem: 2.30e+04]
INFO:root:[   60] 97.049% (loss: 0.213) [mem: 2.30e+04]
INFO:root:[   80] 97.099% (loss: 0.208) [mem: 2.30e+04]
INFO:root:[  100] 97.079% (loss: 0.200) [mem: 2.30e+04]
INFO:root:[  120] 96.901% (loss: 0.193) [mem: 2.30e+04]
INFO:root:[  140] 97.021% (loss: 0.187) [mem: 2.30e+04]
INFO:root:[  160] 97.112% (loss: 0.185) [mem: 2.30e+04]
INFO:root:[  180] 97.127% (loss: 0.189) [mem: 2.30e+04]
INFO:root:[  200] 97.065% (loss: 0.185) [mem: 2.30e+04]
INFO:root:[  220] 97.172% (loss: 0.176) [mem: 2.30e+04]
INFO:root:[  240] 97.116% (loss: 0.180) [mem: 2.30e+04]
INFO:root:[  260] 97.031% (loss: 0.187) [mem: 2.30e+04]
INFO:root:[  280] 97.028% (loss: 0.184) [mem: 2.30e+04]
INFO:root:[  300] 97.126% (loss: 0.181) [mem: 2.30e+04]
INFO:root:[  320] 97.056% (loss: 0.182) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.286% (loss: 0.052) [mem: 2.30e+04]
INFO:root:[   40] 99.390% (loss: 0.035) [mem: 2.30e+04]
INFO:root:[   60] 99.262% (loss: 0.037) [mem: 2.30e+04]
INFO:root:[   80] 99.321% (loss: 0.041) [mem: 2.30e+04]
INFO:root:[   69] train: 97.062% test: 99.345% (loss: 0.182, grad_norm: 33092.434)
Epoch 69: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 70
INFO:root:[    0] 95.000% (loss: 0.281) [mem: 2.30e+04]
INFO:root:[   20] 96.667% (loss: 0.237) [mem: 2.30e+04]
INFO:root:[   40] 97.317% (loss: 0.143) [mem: 2.30e+04]
INFO:root:[   60] 97.213% (loss: 0.144) [mem: 2.30e+04]
INFO:root:[   80] 97.160% (loss: 0.158) [mem: 2.30e+04]
INFO:root:[  100] 97.327% (loss: 0.150) [mem: 2.30e+04]
INFO:root:[  120] 97.479% (loss: 0.142) [mem: 2.30e+04]
INFO:root:[  140] 97.411% (loss: 0.153) [mem: 2.30e+04]
INFO:root:[  160] 97.578% (loss: 0.142) [mem: 2.30e+04]
INFO:root:[  180] 97.541% (loss: 0.143) [mem: 2.30e+04]
INFO:root:[  200] 97.463% (loss: 0.147) [mem: 2.30e+04]
INFO:root:[  220] 97.489% (loss: 0.143) [mem: 2.30e+04]
INFO:root:[  240] 97.427% (loss: 0.147) [mem: 2.30e+04]
INFO:root:[  260] 97.395% (loss: 0.156) [mem: 2.30e+04]
INFO:root:[  280] 97.402% (loss: 0.158) [mem: 2.30e+04]
INFO:root:[  300] 97.359% (loss: 0.159) [mem: 2.30e+04]
INFO:root:[  320] 97.430% (loss: 0.158) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.762% (loss: 0.010) [mem: 2.30e+04]
INFO:root:[   40] 99.878% (loss: 0.005) [mem: 2.30e+04]
INFO:root:[   60] 99.918% (loss: 0.003) [mem: 2.30e+04]
INFO:root:[   80] 99.815% (loss: 0.008) [mem: 2.30e+04]
INFO:root:[   70] train: 97.445% test: 99.821% (loss: 0.157, grad_norm: 7079.812)
Epoch 70: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 71
INFO:root:[    0] 95.000% (loss: 0.045) [mem: 2.30e+04]
INFO:root:[   20] 97.143% (loss: 0.210) [mem: 2.30e+04]
INFO:root:[   40] 97.195% (loss: 0.196) [mem: 2.30e+04]
INFO:root:[   60] 97.377% (loss: 0.190) [mem: 2.30e+04]
INFO:root:[   80] 97.716% (loss: 0.163) [mem: 2.30e+04]
INFO:root:[  100] 97.871% (loss: 0.149) [mem: 2.30e+04]
INFO:root:[  120] 97.934% (loss: 0.144) [mem: 2.30e+04]
INFO:root:[  140] 97.801% (loss: 0.151) [mem: 2.30e+04]
INFO:root:[  160] 97.795% (loss: 0.150) [mem: 2.30e+04]
INFO:root:[  180] 97.680% (loss: 0.156) [mem: 2.30e+04]
INFO:root:[  200] 97.736% (loss: 0.156) [mem: 2.30e+04]
INFO:root:[  220] 97.851% (loss: 0.147) [mem: 2.30e+04]
INFO:root:[  240] 97.842% (loss: 0.147) [mem: 2.30e+04]
INFO:root:[  260] 97.912% (loss: 0.141) [mem: 2.30e+04]
INFO:root:[  280] 97.883% (loss: 0.143) [mem: 2.30e+04]
INFO:root:[  300] 97.874% (loss: 0.146) [mem: 2.30e+04]
INFO:root:[  320] 97.897% (loss: 0.146) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   40] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   60] 99.836% (loss: 0.010) [mem: 2.30e+04]
INFO:root:[   80] 99.877% (loss: 0.008) [mem: 2.30e+04]
INFO:root:[   71] train: 97.907% test: 99.881% (loss: 0.145, grad_norm: 17414.064)
Epoch 71: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 72
INFO:root:[    0] 100.000% (loss: 0.001) [mem: 2.30e+04]
INFO:root:[   20] 97.857% (loss: 0.166) [mem: 2.30e+04]
INFO:root:[   40] 97.439% (loss: 0.175) [mem: 2.30e+04]
INFO:root:[   60] 97.459% (loss: 0.178) [mem: 2.30e+04]
INFO:root:[   80] 97.778% (loss: 0.164) [mem: 2.30e+04]
INFO:root:[  100] 97.772% (loss: 0.172) [mem: 2.30e+04]
INFO:root:[  120] 97.810% (loss: 0.164) [mem: 2.30e+04]
INFO:root:[  140] 97.908% (loss: 0.149) [mem: 2.30e+04]
INFO:root:[  160] 97.981% (loss: 0.144) [mem: 2.30e+04]
INFO:root:[  180] 97.983% (loss: 0.140) [mem: 2.30e+04]
INFO:root:[  200] 98.035% (loss: 0.139) [mem: 2.30e+04]
INFO:root:[  220] 97.964% (loss: 0.146) [mem: 2.30e+04]
INFO:root:[  240] 97.863% (loss: 0.164) [mem: 2.30e+04]
INFO:root:[  260] 97.912% (loss: 0.159) [mem: 2.30e+04]
INFO:root:[  280] 97.918% (loss: 0.158) [mem: 2.30e+04]
INFO:root:[  300] 97.973% (loss: 0.156) [mem: 2.30e+04]
INFO:root:[  320] 97.944% (loss: 0.159) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.524% (loss: 0.032) [mem: 2.30e+04]
INFO:root:[   40] 99.512% (loss: 0.026) [mem: 2.30e+04]
INFO:root:[   60] 99.262% (loss: 0.035) [mem: 2.30e+04]
INFO:root:[   80] 99.383% (loss: 0.032) [mem: 2.30e+04]
INFO:root:[   72] train: 97.953% test: 99.405% (loss: 0.158, grad_norm: 25232.395)
Epoch 72: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 73
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 98.810% (loss: 0.132) [mem: 2.30e+04]
INFO:root:[   40] 98.415% (loss: 0.136) [mem: 2.30e+04]
INFO:root:[   60] 98.279% (loss: 0.120) [mem: 2.30e+04]
INFO:root:[   80] 98.148% (loss: 0.124) [mem: 2.30e+04]
INFO:root:[  100] 98.069% (loss: 0.130) [mem: 2.30e+04]
INFO:root:[  120] 98.099% (loss: 0.137) [mem: 2.30e+04]
INFO:root:[  140] 98.050% (loss: 0.147) [mem: 2.30e+04]
INFO:root:[  160] 98.106% (loss: 0.142) [mem: 2.30e+04]
INFO:root:[  180] 98.122% (loss: 0.140) [mem: 2.30e+04]
INFO:root:[  200] 98.060% (loss: 0.152) [mem: 2.30e+04]
INFO:root:[  220] 98.032% (loss: 0.153) [mem: 2.30e+04]
INFO:root:[  240] 97.925% (loss: 0.155) [mem: 2.30e+04]
INFO:root:[  260] 97.950% (loss: 0.156) [mem: 2.30e+04]
INFO:root:[  280] 97.989% (loss: 0.150) [mem: 2.30e+04]
INFO:root:[  300] 97.990% (loss: 0.146) [mem: 2.30e+04]
INFO:root:[  320] 98.006% (loss: 0.148) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.762% (loss: 0.034) [mem: 2.30e+04]
INFO:root:[   40] 99.878% (loss: 0.017) [mem: 2.30e+04]
INFO:root:[   60] 99.836% (loss: 0.024) [mem: 2.30e+04]
INFO:root:[   80] 99.815% (loss: 0.019) [mem: 2.30e+04]
INFO:root:[   73] train: 98.031% test: 99.821% (loss: 0.146, grad_norm: 21231.791)
Epoch 73: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 74
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 97.143% (loss: 0.230) [mem: 2.30e+04]
INFO:root:[   40] 97.561% (loss: 0.187) [mem: 2.30e+04]
INFO:root:[   60] 97.787% (loss: 0.162) [mem: 2.30e+04]
INFO:root:[   80] 98.086% (loss: 0.136) [mem: 2.30e+04]
INFO:root:[  100] 98.069% (loss: 0.139) [mem: 2.30e+04]
INFO:root:[  120] 98.017% (loss: 0.156) [mem: 2.30e+04]
INFO:root:[  140] 97.908% (loss: 0.158) [mem: 2.30e+04]
INFO:root:[  160] 97.826% (loss: 0.158) [mem: 2.30e+04]
INFO:root:[  180] 97.790% (loss: 0.160) [mem: 2.30e+04]
INFO:root:[  200] 97.811% (loss: 0.160) [mem: 2.30e+04]
INFO:root:[  220] 97.941% (loss: 0.153) [mem: 2.30e+04]
INFO:root:[  240] 97.822% (loss: 0.163) [mem: 2.30e+04]
INFO:root:[  260] 97.854% (loss: 0.160) [mem: 2.30e+04]
INFO:root:[  280] 97.811% (loss: 0.167) [mem: 2.30e+04]
INFO:root:[  300] 97.757% (loss: 0.167) [mem: 2.30e+04]
INFO:root:[  320] 97.804% (loss: 0.166) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.286% (loss: 0.064) [mem: 2.30e+04]
INFO:root:[   40] 99.512% (loss: 0.045) [mem: 2.30e+04]
INFO:root:[   60] 99.672% (loss: 0.030) [mem: 2.30e+04]
INFO:root:[   80] 99.753% (loss: 0.023) [mem: 2.30e+04]
INFO:root:[   74] train: 97.799% test: 99.762% (loss: 0.168, grad_norm: 29085.494)
Epoch 74: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 75
INFO:root:[    0] 85.000% (loss: 0.728) [mem: 2.30e+04]
INFO:root:[   20] 96.429% (loss: 0.253) [mem: 2.30e+04]
INFO:root:[   40] 97.195% (loss: 0.188) [mem: 2.30e+04]
INFO:root:[   60] 97.459% (loss: 0.176) [mem: 2.30e+04]
INFO:root:[   80] 97.654% (loss: 0.154) [mem: 2.30e+04]
INFO:root:[  100] 97.574% (loss: 0.169) [mem: 2.30e+04]
INFO:root:[  120] 97.645% (loss: 0.166) [mem: 2.30e+04]
INFO:root:[  140] 97.624% (loss: 0.166) [mem: 2.30e+04]
INFO:root:[  160] 97.671% (loss: 0.164) [mem: 2.30e+04]
INFO:root:[  180] 97.459% (loss: 0.167) [mem: 2.30e+04]
INFO:root:[  200] 97.537% (loss: 0.161) [mem: 2.30e+04]
INFO:root:[  220] 97.602% (loss: 0.161) [mem: 2.30e+04]
INFO:root:[  240] 97.635% (loss: 0.157) [mem: 2.30e+04]
INFO:root:[  260] 97.605% (loss: 0.154) [mem: 2.30e+04]
INFO:root:[  280] 97.687% (loss: 0.148) [mem: 2.30e+04]
INFO:root:[  300] 97.741% (loss: 0.148) [mem: 2.30e+04]
INFO:root:[  320] 97.741% (loss: 0.147) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.762% (loss: 0.012) [mem: 2.30e+04]
INFO:root:[   40] 99.878% (loss: 0.006) [mem: 2.30e+04]
INFO:root:[   60] 99.918% (loss: 0.004) [mem: 2.30e+04]
INFO:root:[   80] 99.938% (loss: 0.003) [mem: 2.30e+04]
INFO:root:[   75] train: 97.738% test: 99.940% (loss: 0.146, grad_norm: 25287.094)
Epoch 75: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 76
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 98.571% (loss: 0.094) [mem: 2.30e+04]
INFO:root:[   40] 97.805% (loss: 0.166) [mem: 2.30e+04]
INFO:root:[   60] 97.951% (loss: 0.153) [mem: 2.30e+04]
INFO:root:[   80] 98.025% (loss: 0.149) [mem: 2.30e+04]
INFO:root:[  100] 98.020% (loss: 0.159) [mem: 2.30e+04]
INFO:root:[  120] 98.099% (loss: 0.153) [mem: 2.30e+04]
INFO:root:[  140] 98.121% (loss: 0.145) [mem: 2.30e+04]
INFO:root:[  160] 98.106% (loss: 0.147) [mem: 2.30e+04]
INFO:root:[  180] 97.956% (loss: 0.160) [mem: 2.30e+04]
INFO:root:[  200] 97.910% (loss: 0.161) [mem: 2.30e+04]
INFO:root:[  220] 97.851% (loss: 0.174) [mem: 2.30e+04]
INFO:root:[  240] 97.780% (loss: 0.176) [mem: 2.30e+04]
INFO:root:[  260] 97.797% (loss: 0.178) [mem: 2.30e+04]
INFO:root:[  280] 97.740% (loss: 0.181) [mem: 2.30e+04]
INFO:root:[  300] 97.791% (loss: 0.175) [mem: 2.30e+04]
INFO:root:[  320] 97.819% (loss: 0.171) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.524% (loss: 0.034) [mem: 2.30e+04]
INFO:root:[   40] 99.756% (loss: 0.017) [mem: 2.30e+04]
INFO:root:[   60] 99.754% (loss: 0.022) [mem: 2.30e+04]
INFO:root:[   80] 99.815% (loss: 0.016) [mem: 2.30e+04]
INFO:root:[   76] train: 97.831% test: 99.821% (loss: 0.170, grad_norm: 31176.238)
Epoch 76: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 77
INFO:root:[    0] 100.000% (loss: 0.034) [mem: 2.30e+04]
INFO:root:[   20] 97.619% (loss: 0.200) [mem: 2.30e+04]
INFO:root:[   40] 97.805% (loss: 0.158) [mem: 2.30e+04]
INFO:root:[   60] 97.623% (loss: 0.171) [mem: 2.30e+04]
INFO:root:[   80] 97.593% (loss: 0.164) [mem: 2.30e+04]
INFO:root:[  100] 97.376% (loss: 0.177) [mem: 2.30e+04]
INFO:root:[  120] 97.479% (loss: 0.167) [mem: 2.30e+04]
INFO:root:[  140] 97.340% (loss: 0.171) [mem: 2.30e+04]
INFO:root:[  160] 97.329% (loss: 0.171) [mem: 2.30e+04]
INFO:root:[  180] 97.238% (loss: 0.185) [mem: 2.30e+04]
INFO:root:[  200] 97.239% (loss: 0.180) [mem: 2.30e+04]
INFO:root:[  220] 97.398% (loss: 0.166) [mem: 2.30e+04]
INFO:root:[  240] 97.448% (loss: 0.163) [mem: 2.30e+04]
INFO:root:[  260] 97.433% (loss: 0.167) [mem: 2.30e+04]
INFO:root:[  280] 97.473% (loss: 0.166) [mem: 2.30e+04]
INFO:root:[  300] 97.442% (loss: 0.173) [mem: 2.30e+04]
INFO:root:[  320] 97.414% (loss: 0.177) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.286% (loss: 0.038) [mem: 2.30e+04]
INFO:root:[   40] 99.634% (loss: 0.019) [mem: 2.30e+04]
INFO:root:[   60] 99.590% (loss: 0.016) [mem: 2.30e+04]
INFO:root:[   80] 99.691% (loss: 0.012) [mem: 2.30e+04]
INFO:root:[   77] train: 97.446% test: 99.702% (loss: 0.175, grad_norm: 38145.004)
Epoch 77: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 78
INFO:root:[    0] 95.000% (loss: 0.167) [mem: 2.30e+04]
INFO:root:[   20] 97.857% (loss: 0.168) [mem: 2.30e+04]
INFO:root:[   40] 97.927% (loss: 0.178) [mem: 2.30e+04]
INFO:root:[   60] 97.951% (loss: 0.172) [mem: 2.30e+04]
INFO:root:[   80] 98.148% (loss: 0.149) [mem: 2.30e+04]
INFO:root:[  100] 97.970% (loss: 0.143) [mem: 2.30e+04]
INFO:root:[  120] 97.934% (loss: 0.137) [mem: 2.30e+04]
INFO:root:[  140] 97.837% (loss: 0.142) [mem: 2.30e+04]
INFO:root:[  160] 97.826% (loss: 0.139) [mem: 2.30e+04]
INFO:root:[  180] 97.873% (loss: 0.133) [mem: 2.30e+04]
INFO:root:[  200] 97.960% (loss: 0.123) [mem: 2.30e+04]
INFO:root:[  220] 97.941% (loss: 0.128) [mem: 2.30e+04]
INFO:root:[  240] 97.863% (loss: 0.140) [mem: 2.30e+04]
INFO:root:[  260] 97.969% (loss: 0.133) [mem: 2.30e+04]
INFO:root:[  280] 97.954% (loss: 0.136) [mem: 2.30e+04]
INFO:root:[  300] 97.924% (loss: 0.138) [mem: 2.30e+04]
INFO:root:[  320] 97.960% (loss: 0.135) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.762% (loss: 0.010) [mem: 2.30e+04]
INFO:root:[   40] 99.756% (loss: 0.007) [mem: 2.30e+04]
INFO:root:[   60] 99.508% (loss: 0.023) [mem: 2.30e+04]
INFO:root:[   80] 99.630% (loss: 0.017) [mem: 2.30e+04]
INFO:root:[   78] train: 97.954% test: 99.643% (loss: 0.137, grad_norm: 38417.738)
Epoch 78: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 79
INFO:root:[    0] 90.000% (loss: 0.656) [mem: 2.30e+04]
INFO:root:[   20] 96.667% (loss: 0.229) [mem: 2.30e+04]
INFO:root:[   40] 97.683% (loss: 0.159) [mem: 2.30e+04]
INFO:root:[   60] 97.787% (loss: 0.141) [mem: 2.30e+04]
INFO:root:[   80] 97.963% (loss: 0.124) [mem: 2.30e+04]
INFO:root:[  100] 97.921% (loss: 0.129) [mem: 2.30e+04]
INFO:root:[  120] 97.934% (loss: 0.124) [mem: 2.30e+04]
INFO:root:[  140] 97.908% (loss: 0.130) [mem: 2.30e+04]
INFO:root:[  160] 97.950% (loss: 0.130) [mem: 2.30e+04]
INFO:root:[  180] 97.873% (loss: 0.134) [mem: 2.30e+04]
INFO:root:[  200] 97.960% (loss: 0.129) [mem: 2.30e+04]
INFO:root:[  220] 97.851% (loss: 0.134) [mem: 2.30e+04]
INFO:root:[  240] 97.884% (loss: 0.134) [mem: 2.30e+04]
INFO:root:[  260] 97.816% (loss: 0.141) [mem: 2.30e+04]
INFO:root:[  280] 97.811% (loss: 0.149) [mem: 2.30e+04]
INFO:root:[  300] 97.757% (loss: 0.153) [mem: 2.30e+04]
INFO:root:[  320] 97.773% (loss: 0.152) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.286% (loss: 0.071) [mem: 2.30e+04]
INFO:root:[   40] 99.390% (loss: 0.045) [mem: 2.30e+04]
INFO:root:[   60] 99.590% (loss: 0.030) [mem: 2.30e+04]
INFO:root:[   80] 99.691% (loss: 0.023) [mem: 2.30e+04]
INFO:root:[   79] train: 97.784% test: 99.702% (loss: 0.151, grad_norm: 47590.809)
Epoch 79: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 80
INFO:root:[    0] 100.000% (loss: 0.014) [mem: 2.30e+04]
INFO:root:[   20] 98.333% (loss: 0.088) [mem: 2.30e+04]
INFO:root:[   40] 98.780% (loss: 0.076) [mem: 2.30e+04]
INFO:root:[   60] 98.443% (loss: 0.106) [mem: 2.30e+04]
INFO:root:[   80] 98.272% (loss: 0.123) [mem: 2.30e+04]
INFO:root:[  100] 98.069% (loss: 0.119) [mem: 2.30e+04]
INFO:root:[  120] 98.140% (loss: 0.114) [mem: 2.30e+04]
INFO:root:[  140] 98.404% (loss: 0.098) [mem: 2.30e+04]
INFO:root:[  160] 98.292% (loss: 0.106) [mem: 2.30e+04]
INFO:root:[  180] 98.204% (loss: 0.124) [mem: 2.30e+04]
INFO:root:[  200] 98.259% (loss: 0.116) [mem: 2.30e+04]
INFO:root:[  220] 98.235% (loss: 0.115) [mem: 2.30e+04]
INFO:root:[  240] 98.216% (loss: 0.119) [mem: 2.30e+04]
INFO:root:[  260] 98.257% (loss: 0.114) [mem: 2.30e+04]
INFO:root:[  280] 98.221% (loss: 0.122) [mem: 2.30e+04]
INFO:root:[  300] 98.189% (loss: 0.125) [mem: 2.30e+04]
INFO:root:[  320] 98.240% (loss: 0.123) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.524% (loss: 0.020) [mem: 2.30e+04]
INFO:root:[   40] 99.512% (loss: 0.013) [mem: 2.30e+04]
INFO:root:[   60] 99.508% (loss: 0.013) [mem: 2.30e+04]
INFO:root:[   80] 99.630% (loss: 0.010) [mem: 2.30e+04]
INFO:root:[   80] train: 98.215% test: 99.643% (loss: 0.126, grad_norm: 42843.062)
Epoch 80: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 81
INFO:root:[    0] 100.000% (loss: 0.031) [mem: 2.30e+04]
INFO:root:[   20] 98.333% (loss: 0.132) [mem: 2.30e+04]
INFO:root:[   40] 98.049% (loss: 0.129) [mem: 2.30e+04]
INFO:root:[   60] 98.361% (loss: 0.105) [mem: 2.30e+04]
INFO:root:[   80] 98.580% (loss: 0.099) [mem: 2.30e+04]
INFO:root:[  100] 98.416% (loss: 0.105) [mem: 2.30e+04]
INFO:root:[  120] 98.430% (loss: 0.107) [mem: 2.30e+04]
INFO:root:[  140] 98.298% (loss: 0.118) [mem: 2.30e+04]
INFO:root:[  160] 98.261% (loss: 0.122) [mem: 2.30e+04]
INFO:root:[  180] 98.260% (loss: 0.134) [mem: 2.30e+04]
INFO:root:[  200] 98.259% (loss: 0.130) [mem: 2.30e+04]
INFO:root:[  220] 98.167% (loss: 0.142) [mem: 2.30e+04]
INFO:root:[  240] 98.278% (loss: 0.135) [mem: 2.30e+04]
INFO:root:[  260] 98.238% (loss: 0.134) [mem: 2.30e+04]
INFO:root:[  280] 98.274% (loss: 0.131) [mem: 2.30e+04]
NaN or Inf in gradient of model.patch_embed.proj.weight
INFO:root:[  300] 98.256% (loss: 0.134) [mem: 2.30e+04]
INFO:root:[  320] 98.193% (loss: 0.138) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.762% (loss: 0.037) [mem: 2.30e+04]
INFO:root:[   40] 99.878% (loss: 0.019) [mem: 2.30e+04]
INFO:root:[   60] 99.836% (loss: 0.023) [mem: 2.30e+04]
INFO:root:[   80] 99.877% (loss: 0.017) [mem: 2.30e+04]
INFO:root:[   81] train: 98.184% test: 99.881% (loss: 0.139, grad_norm: inf)
Epoch 81: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 82
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 98.333% (loss: 0.121) [mem: 2.30e+04]
INFO:root:[   40] 97.561% (loss: 0.178) [mem: 2.30e+04]
INFO:root:[   60] 97.377% (loss: 0.203) [mem: 2.30e+04]
INFO:root:[   80] 97.531% (loss: 0.187) [mem: 2.30e+04]
INFO:root:[  100] 97.624% (loss: 0.173) [mem: 2.30e+04]
INFO:root:[  120] 97.810% (loss: 0.157) [mem: 2.30e+04]
INFO:root:[  140] 97.801% (loss: 0.155) [mem: 2.30e+04]
INFO:root:[  160] 97.857% (loss: 0.156) [mem: 2.30e+04]
INFO:root:[  180] 97.680% (loss: 0.163) [mem: 2.30e+04]
INFO:root:[  200] 97.637% (loss: 0.168) [mem: 2.30e+04]
INFO:root:[  220] 97.624% (loss: 0.161) [mem: 2.30e+04]
INFO:root:[  240] 97.759% (loss: 0.154) [mem: 2.30e+04]
INFO:root:[  260] 97.759% (loss: 0.152) [mem: 2.30e+04]
INFO:root:[  280] 97.811% (loss: 0.150) [mem: 2.30e+04]
INFO:root:[  300] 97.824% (loss: 0.148) [mem: 2.30e+04]
INFO:root:[  320] 97.850% (loss: 0.145) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.762% (loss: 0.006) [mem: 2.30e+04]
INFO:root:[   40] 99.878% (loss: 0.003) [mem: 2.30e+04]
INFO:root:[   60] 99.836% (loss: 0.003) [mem: 2.30e+04]
INFO:root:[   80] 99.877% (loss: 0.002) [mem: 2.30e+04]
INFO:root:[   82] train: 97.877% test: 99.881% (loss: 0.143, grad_norm: 16325.910)
Epoch 82: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 83
INFO:root:[    0] 95.000% (loss: 0.150) [mem: 2.30e+04]
INFO:root:[   20] 99.286% (loss: 0.027) [mem: 2.30e+04]
INFO:root:[   40] 98.537% (loss: 0.092) [mem: 2.30e+04]
INFO:root:[   60] 98.525% (loss: 0.089) [mem: 2.30e+04]
INFO:root:[   80] 98.272% (loss: 0.117) [mem: 2.30e+04]
INFO:root:[  100] 98.168% (loss: 0.120) [mem: 2.30e+04]
INFO:root:[  120] 98.264% (loss: 0.111) [mem: 2.30e+04]
INFO:root:[  140] 98.227% (loss: 0.106) [mem: 2.30e+04]
INFO:root:[  160] 98.292% (loss: 0.113) [mem: 2.30e+04]
INFO:root:[  180] 98.343% (loss: 0.114) [mem: 2.30e+04]
INFO:root:[  200] 98.358% (loss: 0.116) [mem: 2.30e+04]
INFO:root:[  220] 98.258% (loss: 0.124) [mem: 2.30e+04]
INFO:root:[  240] 98.112% (loss: 0.138) [mem: 2.30e+04]
INFO:root:[  260] 98.180% (loss: 0.132) [mem: 2.30e+04]
INFO:root:[  280] 98.060% (loss: 0.134) [mem: 2.30e+04]
INFO:root:[  300] 98.056% (loss: 0.133) [mem: 2.30e+04]
INFO:root:[  320] 97.991% (loss: 0.139) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.286% (loss: 0.029) [mem: 2.30e+04]
INFO:root:[   40] 99.634% (loss: 0.015) [mem: 2.30e+04]
INFO:root:[   60] 99.590% (loss: 0.020) [mem: 2.30e+04]
INFO:root:[   80] 99.630% (loss: 0.020) [mem: 2.30e+04]
INFO:root:[   83] train: 97.999% test: 99.643% (loss: 0.139, grad_norm: 20361.678)
Epoch 83: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 84
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.048% (loss: 0.051) [mem: 2.30e+04]
INFO:root:[   40] 98.780% (loss: 0.091) [mem: 2.30e+04]
INFO:root:[   60] 98.525% (loss: 0.110) [mem: 2.30e+04]
INFO:root:[   80] 98.580% (loss: 0.092) [mem: 2.30e+04]
INFO:root:[  100] 98.366% (loss: 0.117) [mem: 2.30e+04]
INFO:root:[  120] 98.388% (loss: 0.119) [mem: 2.30e+04]
INFO:root:[  140] 98.369% (loss: 0.115) [mem: 2.30e+04]
INFO:root:[  160] 98.323% (loss: 0.115) [mem: 2.30e+04]
INFO:root:[  180] 98.287% (loss: 0.117) [mem: 2.30e+04]
INFO:root:[  200] 98.333% (loss: 0.116) [mem: 2.30e+04]
INFO:root:[  220] 98.303% (loss: 0.123) [mem: 2.30e+04]
INFO:root:[  240] 98.195% (loss: 0.128) [mem: 2.30e+04]
INFO:root:[  260] 98.199% (loss: 0.134) [mem: 2.30e+04]
INFO:root:[  280] 98.167% (loss: 0.137) [mem: 2.30e+04]
INFO:root:[  300] 98.189% (loss: 0.136) [mem: 2.30e+04]
INFO:root:[  320] 98.193% (loss: 0.134) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.524% (loss: 0.020) [mem: 2.30e+04]
INFO:root:[   40] 99.634% (loss: 0.013) [mem: 2.30e+04]
INFO:root:[   60] 99.590% (loss: 0.018) [mem: 2.30e+04]
INFO:root:[   80] 99.630% (loss: 0.015) [mem: 2.30e+04]
INFO:root:[   84] train: 98.200% test: 99.643% (loss: 0.134, grad_norm: 17193.373)
Epoch 84: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 85
INFO:root:[    0] 90.000% (loss: 0.660) [mem: 2.30e+04]
INFO:root:[   20] 97.619% (loss: 0.138) [mem: 2.30e+04]
INFO:root:[   40] 98.049% (loss: 0.125) [mem: 2.30e+04]
INFO:root:[   60] 98.361% (loss: 0.116) [mem: 2.30e+04]
INFO:root:[   80] 98.148% (loss: 0.135) [mem: 2.30e+04]
INFO:root:[  100] 98.267% (loss: 0.130) [mem: 2.30e+04]
INFO:root:[  120] 98.264% (loss: 0.128) [mem: 2.30e+04]
INFO:root:[  140] 98.298% (loss: 0.122) [mem: 2.30e+04]
INFO:root:[  160] 98.168% (loss: 0.136) [mem: 2.30e+04]
INFO:root:[  180] 98.094% (loss: 0.141) [mem: 2.30e+04]
INFO:root:[  200] 97.985% (loss: 0.147) [mem: 2.30e+04]
INFO:root:[  220] 97.919% (loss: 0.151) [mem: 2.30e+04]
INFO:root:[  240] 97.925% (loss: 0.148) [mem: 2.30e+04]
INFO:root:[  260] 97.931% (loss: 0.144) [mem: 2.30e+04]
INFO:root:[  280] 97.900% (loss: 0.150) [mem: 2.30e+04]
INFO:root:[  300] 97.924% (loss: 0.149) [mem: 2.30e+04]
INFO:root:[  320] 97.882% (loss: 0.152) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.762% (loss: 0.006) [mem: 2.30e+04]
INFO:root:[   40] 99.878% (loss: 0.003) [mem: 2.30e+04]
INFO:root:[   60] 99.918% (loss: 0.002) [mem: 2.30e+04]
INFO:root:[   80] 99.877% (loss: 0.002) [mem: 2.30e+04]
INFO:root:[   85] train: 97.908% test: 99.881% (loss: 0.150, grad_norm: 29002.822)
Epoch 85: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 86
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.048% (loss: 0.106) [mem: 2.30e+04]
INFO:root:[   40] 98.659% (loss: 0.100) [mem: 2.30e+04]
INFO:root:[   60] 98.033% (loss: 0.121) [mem: 2.30e+04]
INFO:root:[   80] 97.901% (loss: 0.126) [mem: 2.30e+04]
INFO:root:[  100] 98.069% (loss: 0.116) [mem: 2.30e+04]
INFO:root:[  120] 98.099% (loss: 0.123) [mem: 2.30e+04]
INFO:root:[  140] 98.050% (loss: 0.124) [mem: 2.30e+04]
INFO:root:[  160] 98.012% (loss: 0.129) [mem: 2.30e+04]
INFO:root:[  180] 97.956% (loss: 0.134) [mem: 2.30e+04]
INFO:root:[  200] 97.910% (loss: 0.139) [mem: 2.30e+04]
INFO:root:[  220] 97.873% (loss: 0.141) [mem: 2.30e+04]
INFO:root:[  240] 97.925% (loss: 0.137) [mem: 2.30e+04]
INFO:root:[  260] 97.950% (loss: 0.135) [mem: 2.30e+04]
INFO:root:[  280] 97.954% (loss: 0.138) [mem: 2.30e+04]
INFO:root:[  300] 97.940% (loss: 0.139) [mem: 2.30e+04]
INFO:root:[  320] 97.975% (loss: 0.137) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.762% (loss: 0.003) [mem: 2.30e+04]
INFO:root:[   40] 99.756% (loss: 0.010) [mem: 2.30e+04]
INFO:root:[   60] 99.754% (loss: 0.015) [mem: 2.30e+04]
INFO:root:[   80] 99.815% (loss: 0.011) [mem: 2.30e+04]
INFO:root:[   86] train: 97.954% test: 99.821% (loss: 0.137, grad_norm: 15970.045)
Epoch 86: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 87
INFO:root:[    0] 95.000% (loss: 0.180) [mem: 2.30e+04]
INFO:root:[   20] 98.333% (loss: 0.091) [mem: 2.30e+04]
INFO:root:[   40] 98.293% (loss: 0.118) [mem: 2.30e+04]
INFO:root:[   60] 98.115% (loss: 0.125) [mem: 2.30e+04]
INFO:root:[   80] 98.272% (loss: 0.107) [mem: 2.30e+04]
INFO:root:[  100] 98.168% (loss: 0.121) [mem: 2.30e+04]
INFO:root:[  120] 98.264% (loss: 0.119) [mem: 2.30e+04]
INFO:root:[  140] 98.227% (loss: 0.116) [mem: 2.30e+04]
INFO:root:[  160] 98.292% (loss: 0.115) [mem: 2.30e+04]
INFO:root:[  180] 98.149% (loss: 0.127) [mem: 2.30e+04]
INFO:root:[  200] 98.109% (loss: 0.128) [mem: 2.30e+04]
INFO:root:[  220] 98.054% (loss: 0.135) [mem: 2.30e+04]
INFO:root:[  240] 98.050% (loss: 0.137) [mem: 2.30e+04]
INFO:root:[  260] 98.046% (loss: 0.137) [mem: 2.30e+04]
INFO:root:[  280] 98.078% (loss: 0.132) [mem: 2.30e+04]
INFO:root:[  300] 98.023% (loss: 0.135) [mem: 2.30e+04]
INFO:root:[  320] 98.022% (loss: 0.137) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.524% (loss: 0.060) [mem: 2.30e+04]
INFO:root:[   40] 99.756% (loss: 0.031) [mem: 2.30e+04]
INFO:root:[   60] 99.672% (loss: 0.027) [mem: 2.30e+04]
INFO:root:[   80] 99.753% (loss: 0.020) [mem: 2.30e+04]
INFO:root:[   87] train: 98.046% test: 99.762% (loss: 0.135, grad_norm: 22241.160)
Epoch 87: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 88
INFO:root:[    0] 100.000% (loss: 0.003) [mem: 2.30e+04]
INFO:root:[   20] 98.571% (loss: 0.104) [mem: 2.30e+04]
INFO:root:[   40] 98.780% (loss: 0.086) [mem: 2.30e+04]
INFO:root:[   60] 98.689% (loss: 0.098) [mem: 2.30e+04]
INFO:root:[   80] 98.704% (loss: 0.093) [mem: 2.30e+04]
INFO:root:[  100] 98.465% (loss: 0.109) [mem: 2.30e+04]
INFO:root:[  120] 98.430% (loss: 0.117) [mem: 2.30e+04]
INFO:root:[  140] 98.227% (loss: 0.131) [mem: 2.30e+04]
INFO:root:[  160] 98.106% (loss: 0.139) [mem: 2.30e+04]
INFO:root:[  180] 98.149% (loss: 0.136) [mem: 2.30e+04]
INFO:root:[  200] 98.134% (loss: 0.145) [mem: 2.30e+04]
INFO:root:[  220] 98.167% (loss: 0.146) [mem: 2.30e+04]
INFO:root:[  240] 98.008% (loss: 0.160) [mem: 2.30e+04]
INFO:root:[  260] 98.008% (loss: 0.160) [mem: 2.30e+04]
INFO:root:[  280] 98.007% (loss: 0.164) [mem: 2.30e+04]
INFO:root:[  300] 97.973% (loss: 0.165) [mem: 2.30e+04]
INFO:root:[  320] 98.006% (loss: 0.160) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   40] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   60] 99.836% (loss: 0.010) [mem: 2.30e+04]
INFO:root:[   80] 99.815% (loss: 0.013) [mem: 2.30e+04]
INFO:root:[   88] train: 98.031% test: 99.762% (loss: 0.158, grad_norm: 35473.535)
Epoch 88: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 89
INFO:root:[    0] 95.000% (loss: 0.425) [mem: 2.30e+04]
INFO:root:[   20] 98.095% (loss: 0.111) [mem: 2.30e+04]
INFO:root:[   40] 98.293% (loss: 0.106) [mem: 2.30e+04]
INFO:root:[   60] 98.197% (loss: 0.111) [mem: 2.30e+04]
INFO:root:[   80] 98.519% (loss: 0.097) [mem: 2.30e+04]
INFO:root:[  100] 98.663% (loss: 0.090) [mem: 2.30e+04]
INFO:root:[  120] 98.595% (loss: 0.098) [mem: 2.30e+04]
INFO:root:[  140] 98.511% (loss: 0.102) [mem: 2.30e+04]
INFO:root:[  160] 98.447% (loss: 0.105) [mem: 2.30e+04]
INFO:root:[  180] 98.453% (loss: 0.106) [mem: 2.30e+04]
INFO:root:[  200] 98.458% (loss: 0.110) [mem: 2.30e+04]
INFO:root:[  220] 98.416% (loss: 0.110) [mem: 2.30e+04]
INFO:root:[  240] 98.423% (loss: 0.111) [mem: 2.30e+04]
INFO:root:[  260] 98.410% (loss: 0.114) [mem: 2.30e+04]
INFO:root:[  280] 98.399% (loss: 0.118) [mem: 2.30e+04]
INFO:root:[  300] 98.422% (loss: 0.120) [mem: 2.30e+04]
INFO:root:[  320] 98.396% (loss: 0.119) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.524% (loss: 0.037) [mem: 2.30e+04]
INFO:root:[   40] 99.756% (loss: 0.019) [mem: 2.30e+04]
INFO:root:[   60] 99.754% (loss: 0.019) [mem: 2.30e+04]
INFO:root:[   80] 99.753% (loss: 0.017) [mem: 2.30e+04]
INFO:root:[   89] train: 98.415% test: 99.762% (loss: 0.118, grad_norm: 48210.242)
Epoch 89: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 90
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 98.571% (loss: 0.151) [mem: 2.30e+04]
INFO:root:[   40] 98.293% (loss: 0.135) [mem: 2.30e+04]
INFO:root:[   60] 98.361% (loss: 0.131) [mem: 2.30e+04]
INFO:root:[   80] 98.395% (loss: 0.122) [mem: 2.30e+04]
INFO:root:[  100] 98.218% (loss: 0.140) [mem: 2.30e+04]
INFO:root:[  120] 98.182% (loss: 0.146) [mem: 2.30e+04]
INFO:root:[  140] 98.298% (loss: 0.133) [mem: 2.30e+04]
INFO:root:[  160] 98.292% (loss: 0.132) [mem: 2.30e+04]
INFO:root:[  180] 98.315% (loss: 0.129) [mem: 2.30e+04]
INFO:root:[  200] 98.333% (loss: 0.131) [mem: 2.30e+04]
INFO:root:[  220] 98.077% (loss: 0.146) [mem: 2.30e+04]
INFO:root:[  240] 98.174% (loss: 0.139) [mem: 2.30e+04]
INFO:root:[  260] 98.218% (loss: 0.136) [mem: 2.30e+04]
INFO:root:[  280] 98.292% (loss: 0.129) [mem: 2.30e+04]
INFO:root:[  300] 98.289% (loss: 0.129) [mem: 2.30e+04]
INFO:root:[  320] 98.224% (loss: 0.131) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.524% (loss: 0.032) [mem: 2.30e+04]
INFO:root:[   40] 99.756% (loss: 0.016) [mem: 2.30e+04]
INFO:root:[   60] 99.508% (loss: 0.025) [mem: 2.30e+04]
INFO:root:[   80] 99.568% (loss: 0.024) [mem: 2.30e+04]
INFO:root:[   90] train: 98.231% test: 99.583% (loss: 0.131, grad_norm: 33579.715)
Epoch 90: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 91
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 97.143% (loss: 0.251) [mem: 2.30e+04]
INFO:root:[   40] 97.561% (loss: 0.203) [mem: 2.30e+04]
INFO:root:[   60] 98.115% (loss: 0.166) [mem: 2.30e+04]
INFO:root:[   80] 98.025% (loss: 0.174) [mem: 2.30e+04]
INFO:root:[  100] 97.871% (loss: 0.178) [mem: 2.30e+04]
INFO:root:[  120] 97.934% (loss: 0.176) [mem: 2.30e+04]
INFO:root:[  140] 97.872% (loss: 0.181) [mem: 2.30e+04]
INFO:root:[  160] 97.919% (loss: 0.173) [mem: 2.30e+04]
INFO:root:[  180] 97.901% (loss: 0.169) [mem: 2.30e+04]
INFO:root:[  200] 97.910% (loss: 0.169) [mem: 2.30e+04]
INFO:root:[  220] 97.851% (loss: 0.177) [mem: 2.30e+04]
INFO:root:[  240] 97.905% (loss: 0.170) [mem: 2.30e+04]
INFO:root:[  260] 97.874% (loss: 0.176) [mem: 2.30e+04]
INFO:root:[  280] 97.847% (loss: 0.176) [mem: 2.30e+04]
INFO:root:[  300] 97.874% (loss: 0.173) [mem: 2.30e+04]
INFO:root:[  320] 97.944% (loss: 0.168) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.762% (loss: 0.006) [mem: 2.30e+04]
INFO:root:[   40] 99.878% (loss: 0.003) [mem: 2.30e+04]
INFO:root:[   60] 99.836% (loss: 0.005) [mem: 2.30e+04]
INFO:root:[   80] 99.877% (loss: 0.004) [mem: 2.30e+04]
INFO:root:[   91] train: 97.954% test: 99.881% (loss: 0.167, grad_norm: 46695.637)
Epoch 91: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 92
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 98.333% (loss: 0.119) [mem: 2.30e+04]
INFO:root:[   40] 98.293% (loss: 0.134) [mem: 2.30e+04]
INFO:root:[   60] 98.197% (loss: 0.131) [mem: 2.30e+04]
INFO:root:[   80] 98.333% (loss: 0.115) [mem: 2.30e+04]
INFO:root:[  100] 98.168% (loss: 0.130) [mem: 2.30e+04]
INFO:root:[  120] 98.058% (loss: 0.138) [mem: 2.30e+04]
INFO:root:[  140] 97.979% (loss: 0.148) [mem: 2.30e+04]
INFO:root:[  160] 98.106% (loss: 0.138) [mem: 2.30e+04]
INFO:root:[  180] 98.122% (loss: 0.140) [mem: 2.30e+04]
INFO:root:[  200] 98.109% (loss: 0.141) [mem: 2.30e+04]
INFO:root:[  220] 98.167% (loss: 0.138) [mem: 2.30e+04]
INFO:root:[  240] 98.216% (loss: 0.131) [mem: 2.30e+04]
INFO:root:[  260] 98.218% (loss: 0.130) [mem: 2.30e+04]
INFO:root:[  280] 98.132% (loss: 0.132) [mem: 2.30e+04]
INFO:root:[  300] 98.140% (loss: 0.133) [mem: 2.30e+04]
INFO:root:[  320] 98.209% (loss: 0.129) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.524% (loss: 0.033) [mem: 2.30e+04]
INFO:root:[   40] 99.756% (loss: 0.017) [mem: 2.30e+04]
INFO:root:[   60] 99.836% (loss: 0.011) [mem: 2.30e+04]
INFO:root:[   80] 99.877% (loss: 0.009) [mem: 2.30e+04]
INFO:root:[   92] train: 98.152% test: 99.881% (loss: 0.134, grad_norm: 51545.574)
Epoch 92: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 93
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.048% (loss: 0.059) [mem: 2.30e+04]
INFO:root:[   40] 98.537% (loss: 0.084) [mem: 2.30e+04]
INFO:root:[   60] 98.443% (loss: 0.098) [mem: 2.30e+04]
INFO:root:[   80] 98.457% (loss: 0.119) [mem: 2.30e+04]
INFO:root:[  100] 98.366% (loss: 0.128) [mem: 2.30e+04]
INFO:root:[  120] 98.471% (loss: 0.119) [mem: 2.30e+04]
INFO:root:[  140] 98.369% (loss: 0.125) [mem: 2.30e+04]
INFO:root:[  160] 98.292% (loss: 0.124) [mem: 2.30e+04]
INFO:root:[  180] 98.204% (loss: 0.121) [mem: 2.30e+04]
INFO:root:[  200] 98.234% (loss: 0.119) [mem: 2.30e+04]
INFO:root:[  220] 98.303% (loss: 0.117) [mem: 2.30e+04]
INFO:root:[  240] 98.299% (loss: 0.117) [mem: 2.30e+04]
INFO:root:[  260] 98.257% (loss: 0.123) [mem: 2.30e+04]
INFO:root:[  280] 98.292% (loss: 0.119) [mem: 2.30e+04]
INFO:root:[  300] 98.306% (loss: 0.118) [mem: 2.30e+04]
INFO:root:[  320] 98.318% (loss: 0.117) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   40] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   60] 99.836% (loss: 0.007) [mem: 2.30e+04]
INFO:root:[   80] 99.877% (loss: 0.005) [mem: 2.30e+04]
INFO:root:[   93] train: 98.306% test: 99.881% (loss: 0.117, grad_norm: 37579.141)
Epoch 93: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 94
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 98.333% (loss: 0.147) [mem: 2.30e+04]
INFO:root:[   40] 98.780% (loss: 0.107) [mem: 2.30e+04]
INFO:root:[   60] 98.443% (loss: 0.137) [mem: 2.30e+04]
INFO:root:[   80] 98.704% (loss: 0.112) [mem: 2.30e+04]
INFO:root:[  100] 98.713% (loss: 0.112) [mem: 2.30e+04]
INFO:root:[  120] 98.719% (loss: 0.107) [mem: 2.30e+04]
INFO:root:[  140] 98.723% (loss: 0.109) [mem: 2.30e+04]
INFO:root:[  160] 98.758% (loss: 0.098) [mem: 2.30e+04]
INFO:root:[  180] 98.674% (loss: 0.106) [mem: 2.30e+04]
INFO:root:[  200] 98.557% (loss: 0.111) [mem: 2.30e+04]
INFO:root:[  220] 98.416% (loss: 0.119) [mem: 2.30e+04]
INFO:root:[  240] 98.423% (loss: 0.121) [mem: 2.30e+04]
INFO:root:[  260] 98.391% (loss: 0.121) [mem: 2.30e+04]
INFO:root:[  280] 98.381% (loss: 0.122) [mem: 2.30e+04]
INFO:root:[  300] 98.422% (loss: 0.120) [mem: 2.30e+04]
INFO:root:[  320] 98.396% (loss: 0.121) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.286% (loss: 0.032) [mem: 2.30e+04]
INFO:root:[   40] 99.634% (loss: 0.016) [mem: 2.30e+04]
INFO:root:[   60] 99.590% (loss: 0.023) [mem: 2.30e+04]
INFO:root:[   80] 99.568% (loss: 0.021) [mem: 2.30e+04]
INFO:root:[   94] train: 98.385% test: 99.583% (loss: 0.122, grad_norm: 78692.234)
Epoch 94: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 95
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
NaN or Inf in gradient of model.patch_embed.proj.weight
INFO:root:[   20] 98.810% (loss: 0.087) [mem: 2.30e+04]
INFO:root:[   40] 99.024% (loss: 0.070) [mem: 2.30e+04]
INFO:root:[   60] 99.098% (loss: 0.062) [mem: 2.30e+04]
INFO:root:[   80] 99.074% (loss: 0.063) [mem: 2.30e+04]
INFO:root:[  100] 98.713% (loss: 0.082) [mem: 2.30e+04]
INFO:root:[  120] 98.678% (loss: 0.083) [mem: 2.30e+04]
INFO:root:[  140] 98.617% (loss: 0.090) [mem: 2.30e+04]
INFO:root:[  160] 98.665% (loss: 0.091) [mem: 2.30e+04]
INFO:root:[  180] 98.646% (loss: 0.099) [mem: 2.30e+04]
INFO:root:[  200] 98.557% (loss: 0.104) [mem: 2.30e+04]
INFO:root:[  220] 98.552% (loss: 0.112) [mem: 2.30e+04]
INFO:root:[  240] 98.506% (loss: 0.117) [mem: 2.30e+04]
INFO:root:[  260] 98.467% (loss: 0.117) [mem: 2.30e+04]
NaN or Inf in gradient of model.patch_embed.proj.weight
NaN or Inf in gradient of model.patch_embed.proj.bias
NaN or Inf in gradient of model.blocks.0.norm1.weight
NaN or Inf in gradient of model.blocks.0.norm1.bias
NaN or Inf in gradient of model.blocks.0.attn.qkv.weight
NaN or Inf in gradient of model.blocks.0.attn.qkv.bias
NaN or Inf in gradient of model.blocks.0.attn.proj.weight
NaN or Inf in gradient of model.blocks.0.attn.proj.bias
NaN or Inf in gradient of model.blocks.0.norm2.weight
NaN or Inf in gradient of model.blocks.0.norm2.bias
NaN or Inf in gradient of model.blocks.0.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.0.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.0.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.0.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.1.norm1.weight
NaN or Inf in gradient of model.blocks.1.norm1.bias
NaN or Inf in gradient of model.blocks.1.attn.qkv.weight
NaN or Inf in gradient of model.blocks.1.attn.qkv.bias
NaN or Inf in gradient of model.blocks.1.attn.proj.weight
NaN or Inf in gradient of model.blocks.1.attn.proj.bias
NaN or Inf in gradient of model.blocks.1.norm2.weight
NaN or Inf in gradient of model.blocks.1.norm2.bias
NaN or Inf in gradient of model.blocks.1.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.1.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.1.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.1.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.2.norm1.weight
NaN or Inf in gradient of model.blocks.2.norm1.bias
NaN or Inf in gradient of model.blocks.2.attn.qkv.weight
NaN or Inf in gradient of model.blocks.2.attn.qkv.bias
NaN or Inf in gradient of model.blocks.2.attn.proj.weight
NaN or Inf in gradient of model.blocks.2.attn.proj.bias
NaN or Inf in gradient of model.blocks.2.norm2.weight
NaN or Inf in gradient of model.blocks.2.norm2.bias
NaN or Inf in gradient of model.blocks.2.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.2.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.2.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.2.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.3.norm1.weight
NaN or Inf in gradient of model.blocks.3.norm1.bias
NaN or Inf in gradient of model.blocks.3.attn.qkv.weight
NaN or Inf in gradient of model.blocks.3.attn.qkv.bias
NaN or Inf in gradient of model.blocks.3.attn.proj.weight
NaN or Inf in gradient of model.blocks.3.attn.proj.bias
NaN or Inf in gradient of model.blocks.3.norm2.weight
NaN or Inf in gradient of model.blocks.3.norm2.bias
NaN or Inf in gradient of model.blocks.3.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.3.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.3.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.3.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.4.norm1.weight
NaN or Inf in gradient of model.blocks.4.norm1.bias
NaN or Inf in gradient of model.blocks.4.attn.qkv.weight
NaN or Inf in gradient of model.blocks.4.attn.qkv.bias
NaN or Inf in gradient of model.blocks.4.attn.proj.weight
NaN or Inf in gradient of model.blocks.4.attn.proj.bias
NaN or Inf in gradient of model.blocks.4.norm2.weight
NaN or Inf in gradient of model.blocks.4.norm2.bias
NaN or Inf in gradient of model.blocks.4.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.4.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.4.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.4.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.5.norm1.weight
NaN or Inf in gradient of model.blocks.5.norm1.bias
NaN or Inf in gradient of model.blocks.5.attn.qkv.weight
NaN or Inf in gradient of model.blocks.5.attn.qkv.bias
NaN or Inf in gradient of model.blocks.5.attn.proj.weight
NaN or Inf in gradient of model.blocks.5.attn.proj.bias
NaN or Inf in gradient of model.blocks.5.norm2.weight
NaN or Inf in gradient of model.blocks.5.norm2.bias
NaN or Inf in gradient of model.blocks.5.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.5.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.5.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.5.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.6.norm1.weight
NaN or Inf in gradient of model.blocks.6.norm1.bias
NaN or Inf in gradient of model.blocks.6.attn.qkv.weight
NaN or Inf in gradient of model.blocks.6.attn.qkv.bias
NaN or Inf in gradient of model.blocks.6.attn.proj.weight
NaN or Inf in gradient of model.blocks.6.attn.proj.bias
NaN or Inf in gradient of model.blocks.6.norm2.weight
NaN or Inf in gradient of model.blocks.6.norm2.bias
NaN or Inf in gradient of model.blocks.6.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.6.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.6.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.6.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.7.norm1.weight
NaN or Inf in gradient of model.blocks.7.norm1.bias
NaN or Inf in gradient of model.blocks.7.attn.qkv.weight
NaN or Inf in gradient of model.blocks.7.attn.qkv.bias
NaN or Inf in gradient of model.blocks.7.attn.proj.weight
NaN or Inf in gradient of model.blocks.7.attn.proj.bias
NaN or Inf in gradient of model.blocks.7.norm2.weight
NaN or Inf in gradient of model.blocks.7.norm2.bias
NaN or Inf in gradient of model.blocks.7.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.7.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.7.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.7.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.8.norm1.weight
NaN or Inf in gradient of model.blocks.8.norm1.bias
NaN or Inf in gradient of model.blocks.8.attn.qkv.weight
NaN or Inf in gradient of model.blocks.8.attn.qkv.bias
NaN or Inf in gradient of model.blocks.8.attn.proj.weight
NaN or Inf in gradient of model.blocks.8.attn.proj.bias
NaN or Inf in gradient of model.blocks.8.norm2.weight
NaN or Inf in gradient of model.blocks.8.norm2.bias
NaN or Inf in gradient of model.blocks.8.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.8.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.8.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.8.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.9.norm1.weight
NaN or Inf in gradient of model.blocks.9.norm1.bias
NaN or Inf in gradient of model.blocks.9.attn.qkv.weight
NaN or Inf in gradient of model.blocks.9.attn.qkv.bias
NaN or Inf in gradient of model.blocks.9.attn.proj.weight
NaN or Inf in gradient of model.blocks.9.attn.proj.bias
NaN or Inf in gradient of model.blocks.9.norm2.weight
NaN or Inf in gradient of model.blocks.9.norm2.bias
NaN or Inf in gradient of model.blocks.9.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.9.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.9.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.9.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.10.norm1.weight
NaN or Inf in gradient of model.blocks.10.norm1.bias
NaN or Inf in gradient of model.blocks.10.attn.qkv.weight
NaN or Inf in gradient of model.blocks.10.attn.qkv.bias
NaN or Inf in gradient of model.blocks.10.attn.proj.weight
NaN or Inf in gradient of model.blocks.10.attn.proj.bias
NaN or Inf in gradient of model.blocks.10.norm2.weight
NaN or Inf in gradient of model.blocks.10.norm2.bias
NaN or Inf in gradient of model.blocks.10.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.10.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.10.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.10.mlp.fc2.bias
NaN or Inf in gradient of model.blocks.11.norm1.weight
NaN or Inf in gradient of model.blocks.11.norm1.bias
NaN or Inf in gradient of model.blocks.11.attn.qkv.weight
NaN or Inf in gradient of model.blocks.11.attn.qkv.bias
NaN or Inf in gradient of model.blocks.11.attn.proj.weight
NaN or Inf in gradient of model.blocks.11.attn.proj.bias
NaN or Inf in gradient of model.blocks.11.norm2.weight
NaN or Inf in gradient of model.blocks.11.norm2.bias
NaN or Inf in gradient of model.blocks.11.mlp.fc1.weight
NaN or Inf in gradient of model.blocks.11.mlp.fc1.bias
NaN or Inf in gradient of model.blocks.11.mlp.fc2.weight
NaN or Inf in gradient of model.blocks.11.mlp.fc2.bias
NaN or Inf in gradient of model.norm.weight
NaN or Inf in gradient of model.norm.bias
NaN or Inf in gradient of module.pooler.query_tokens
NaN or Inf in gradient of module.pooler.cross_attention_block.norm1.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.norm1.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.q.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.q.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.kv.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.xattn.kv.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.norm2.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.norm2.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc1.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc1.bias
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc2.weight
NaN or Inf in gradient of module.pooler.cross_attention_block.mlp.fc2.bias
NaN or Inf in gradient of module.linear.weight
NaN or Inf in gradient of module.linear.bias
INFO:root:[  280] 98.416% (loss: 0.122) [mem: 2.30e+04]
INFO:root:[  300] 98.472% (loss: 0.118) [mem: 2.30e+04]
INFO:root:[  320] 98.505% (loss: 0.117) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.048% (loss: 0.053) [mem: 2.30e+04]
INFO:root:[   40] 99.512% (loss: 0.027) [mem: 2.30e+04]
INFO:root:[   60] 99.590% (loss: 0.023) [mem: 2.30e+04]
INFO:root:[   80] 99.691% (loss: 0.017) [mem: 2.30e+04]
INFO:root:[   95] train: 98.476% test: 99.702% (loss: 0.117, grad_norm: nan)
Epoch 95: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 96
INFO:root:[    0] 95.000% (loss: 0.699) [mem: 2.30e+04]
INFO:root:[   20] 98.095% (loss: 0.125) [mem: 2.30e+04]
INFO:root:[   40] 98.171% (loss: 0.165) [mem: 2.30e+04]
INFO:root:[   60] 97.951% (loss: 0.175) [mem: 2.30e+04]
INFO:root:[   80] 97.840% (loss: 0.162) [mem: 2.30e+04]
INFO:root:[  100] 97.822% (loss: 0.157) [mem: 2.30e+04]
INFO:root:[  120] 97.810% (loss: 0.165) [mem: 2.30e+04]
INFO:root:[  140] 97.943% (loss: 0.153) [mem: 2.30e+04]
INFO:root:[  160] 97.919% (loss: 0.154) [mem: 2.30e+04]
INFO:root:[  180] 97.983% (loss: 0.151) [mem: 2.30e+04]
INFO:root:[  200] 97.985% (loss: 0.148) [mem: 2.30e+04]
INFO:root:[  220] 98.054% (loss: 0.147) [mem: 2.30e+04]
INFO:root:[  240] 98.133% (loss: 0.138) [mem: 2.30e+04]
INFO:root:[  260] 98.123% (loss: 0.140) [mem: 2.30e+04]
INFO:root:[  280] 98.132% (loss: 0.140) [mem: 2.30e+04]
INFO:root:[  300] 98.123% (loss: 0.144) [mem: 2.30e+04]
INFO:root:[  320] 98.084% (loss: 0.149) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.524% (loss: 0.035) [mem: 2.30e+04]
INFO:root:[   40] 99.756% (loss: 0.018) [mem: 2.30e+04]
INFO:root:[   60] 99.754% (loss: 0.017) [mem: 2.30e+04]
INFO:root:[   80] 99.753% (loss: 0.016) [mem: 2.30e+04]
INFO:root:[   96] train: 98.092% test: 99.762% (loss: 0.149, grad_norm: 20185.703)
Epoch 96: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 97
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 98.810% (loss: 0.080) [mem: 2.30e+04]
INFO:root:[   40] 99.024% (loss: 0.078) [mem: 2.30e+04]
INFO:root:[   60] 98.443% (loss: 0.139) [mem: 2.30e+04]
INFO:root:[   80] 98.395% (loss: 0.141) [mem: 2.30e+04]
INFO:root:[  100] 98.069% (loss: 0.162) [mem: 2.30e+04]
INFO:root:[  120] 98.182% (loss: 0.155) [mem: 2.30e+04]
INFO:root:[  140] 98.050% (loss: 0.168) [mem: 2.30e+04]
INFO:root:[  160] 98.106% (loss: 0.162) [mem: 2.30e+04]
INFO:root:[  180] 98.122% (loss: 0.157) [mem: 2.30e+04]
INFO:root:[  200] 98.209% (loss: 0.145) [mem: 2.30e+04]
INFO:root:[  220] 98.258% (loss: 0.145) [mem: 2.30e+04]
INFO:root:[  240] 98.237% (loss: 0.143) [mem: 2.30e+04]
INFO:root:[  260] 98.218% (loss: 0.142) [mem: 2.30e+04]
INFO:root:[  280] 98.221% (loss: 0.142) [mem: 2.30e+04]
INFO:root:[  300] 98.256% (loss: 0.142) [mem: 2.30e+04]
INFO:root:[  320] 98.271% (loss: 0.142) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.762% (loss: 0.028) [mem: 2.30e+04]
INFO:root:[   40] 99.756% (loss: 0.024) [mem: 2.30e+04]
INFO:root:[   60] 99.836% (loss: 0.016) [mem: 2.30e+04]
INFO:root:[   80] 99.877% (loss: 0.013) [mem: 2.30e+04]
INFO:root:[   97] train: 98.260% test: 99.881% (loss: 0.142, grad_norm: 21982.564)
Epoch 97: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 98
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.048% (loss: 0.060) [mem: 2.30e+04]
INFO:root:[   40] 98.659% (loss: 0.113) [mem: 2.30e+04]
INFO:root:[   60] 98.607% (loss: 0.127) [mem: 2.30e+04]
INFO:root:[   80] 98.333% (loss: 0.154) [mem: 2.30e+04]
INFO:root:[  100] 98.366% (loss: 0.149) [mem: 2.30e+04]
INFO:root:[  120] 98.264% (loss: 0.149) [mem: 2.30e+04]
INFO:root:[  140] 98.262% (loss: 0.144) [mem: 2.30e+04]
INFO:root:[  160] 98.230% (loss: 0.146) [mem: 2.30e+04]
INFO:root:[  180] 98.260% (loss: 0.138) [mem: 2.30e+04]
INFO:root:[  200] 98.184% (loss: 0.151) [mem: 2.30e+04]
INFO:root:[  220] 98.100% (loss: 0.152) [mem: 2.30e+04]
INFO:root:[  240] 98.154% (loss: 0.146) [mem: 2.30e+04]
INFO:root:[  260] 98.161% (loss: 0.146) [mem: 2.30e+04]
INFO:root:[  280] 98.149% (loss: 0.144) [mem: 2.30e+04]
INFO:root:[  300] 98.206% (loss: 0.140) [mem: 2.30e+04]
INFO:root:[  320] 98.146% (loss: 0.145) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.286% (loss: 0.064) [mem: 2.30e+04]
INFO:root:[   40] 99.634% (loss: 0.033) [mem: 2.30e+04]
INFO:root:[   60] 99.672% (loss: 0.029) [mem: 2.30e+04]
INFO:root:[   80] 99.753% (loss: 0.022) [mem: 2.30e+04]
INFO:root:[   98] train: 98.169% test: 99.762% (loss: 0.143, grad_norm: 28289.254)
Epoch 98: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 99
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 98.095% (loss: 0.166) [mem: 2.30e+04]
INFO:root:[   40] 98.171% (loss: 0.110) [mem: 2.30e+04]
INFO:root:[   60] 98.197% (loss: 0.124) [mem: 2.30e+04]
INFO:root:[   80] 98.395% (loss: 0.113) [mem: 2.30e+04]
INFO:root:[  100] 98.465% (loss: 0.118) [mem: 2.30e+04]
INFO:root:[  120] 98.512% (loss: 0.120) [mem: 2.30e+04]
INFO:root:[  140] 98.511% (loss: 0.123) [mem: 2.30e+04]
INFO:root:[  160] 98.540% (loss: 0.125) [mem: 2.30e+04]
INFO:root:[  180] 98.508% (loss: 0.123) [mem: 2.30e+04]
INFO:root:[  200] 98.458% (loss: 0.126) [mem: 2.30e+04]
INFO:root:[  220] 98.394% (loss: 0.128) [mem: 2.30e+04]
INFO:root:[  240] 98.465% (loss: 0.122) [mem: 2.30e+04]
INFO:root:[  260] 98.487% (loss: 0.118) [mem: 2.30e+04]
INFO:root:[  280] 98.488% (loss: 0.117) [mem: 2.30e+04]
INFO:root:[  300] 98.538% (loss: 0.113) [mem: 2.30e+04]
INFO:root:[  320] 98.567% (loss: 0.111) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   40] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   60] 99.918% (loss: 0.007) [mem: 2.30e+04]
INFO:root:[   80] 99.938% (loss: 0.005) [mem: 2.30e+04]
INFO:root:[   99] train: 98.521% test: 99.940% (loss: 0.114, grad_norm: 15208.023)
Epoch 99: Encoder weights changed: True, Classifier weights changed: True
INFO:root:Epoch 100
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.048% (loss: 0.082) [mem: 2.30e+04]
INFO:root:[   40] 98.293% (loss: 0.144) [mem: 2.30e+04]
INFO:root:[   60] 98.361% (loss: 0.137) [mem: 2.30e+04]
INFO:root:[   80] 98.457% (loss: 0.130) [mem: 2.30e+04]
INFO:root:[  100] 98.465% (loss: 0.123) [mem: 2.30e+04]
INFO:root:[  120] 98.388% (loss: 0.121) [mem: 2.30e+04]
INFO:root:[  140] 98.440% (loss: 0.115) [mem: 2.30e+04]
INFO:root:[  160] 98.230% (loss: 0.129) [mem: 2.30e+04]
INFO:root:[  180] 98.287% (loss: 0.124) [mem: 2.30e+04]
INFO:root:[  200] 98.209% (loss: 0.128) [mem: 2.30e+04]
INFO:root:[  220] 98.077% (loss: 0.135) [mem: 2.30e+04]
INFO:root:[  240] 98.091% (loss: 0.135) [mem: 2.30e+04]
INFO:root:[  260] 98.103% (loss: 0.137) [mem: 2.30e+04]
INFO:root:[  280] 98.149% (loss: 0.134) [mem: 2.30e+04]
INFO:root:[  300] 98.140% (loss: 0.138) [mem: 2.30e+04]
INFO:root:[  320] 98.162% (loss: 0.140) [mem: 2.30e+04]
INFO:root:[    0] 100.000% (loss: 0.000) [mem: 2.30e+04]
INFO:root:[   20] 99.762% (loss: 0.012) [mem: 2.30e+04]
INFO:root:[   40] 99.878% (loss: 0.006) [mem: 2.30e+04]
INFO:root:[   60] 99.918% (loss: 0.004) [mem: 2.30e+04]
INFO:root:[   80] 99.938% (loss: 0.003) [mem: 2.30e+04]
INFO:root:[  100] train: 98.168% test: 99.940% (loss: 0.141, grad_norm: 30646.133)
[1;34mwandb[0m: 
[1;34mwandb[0m:  View run [33mK400_RGB[0m at: [34mhttps://wandb.ai/voxel-jepa/voxel-jepa-ad-bind-base-scratch/runs/lw2eae3s[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250910_120706-lw2eae3s/logs[0m
Epoch 100: Encoder weights changed: True, Classifier weights changed: True
