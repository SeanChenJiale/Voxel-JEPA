{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VideoMAE GradCAM Visualization\n",
        "\n",
        "This notebook implements GradCAM for VideoMAE models to visualize which spatial-temporal regions the model focuses on for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "from pathlib import Path\n",
        "from collections import OrderedDict\n",
        "from typing import List, Tuple, Optional\n",
        "import pandas as pd\n",
        "\n",
        "# VideoMAE imports\n",
        "import sys\n",
        "sys.path.append('/home/tianze/Code/VideoMAE')\n",
        "from timm.models import create_model\n",
        "import modeling_finetune\n",
        "from datasets import build_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "### Two-Model Comparison Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda:0\n",
            "Checkpoint 1: /home/tianze/Code/VideoMAE/checkpoints/final_experiments/K400-MRI-cov_loss_comb_diag-linear_probe/checkpoint-best.pth\n",
            "Checkpoint 2: /home/tianze/Code/VideoMAE/checkpoints/final_experiments/K400-MRI-MCI_CN-finetune-linear_probe/checkpoint-49.pth\n",
            "Test CSV: /home/tianze/DATA_2T/MRI/csv_for_finetuning/Finalized_all/Downstreamtask_csv/A_MCI_CN_allscantest_split.csv\n",
            "Output directory: /home/tianze/Code/VideoMAE/gradcam_comparison_results_bestvsbad\n"
          ]
        }
      ],
      "source": [
        "# Model configuration for comparison\n",
        "CHECKPOINT_PATH_1 = '/home/tianze/Code/VideoMAE/checkpoints/final_experiments/K400-MRI-cov_loss_comb_diag-linear_probe/checkpoint-best.pth'\n",
        "CHECKPOINT_PATH_2 = '/home/tianze/Code/VideoMAE/checkpoints/final_experiments/K400-MRI-MCI_CN-finetune-linear_probe/checkpoint-49.pth'\n",
        "# CHECKPOINT_PATH_2 = '/home/tianze/Code/VideoMAE/checkpoints/final_experiments/K400-MRI-cov_loss_comb_trace-linear_probe-NIFD/checkpoint-best.pth'\n",
        "\n",
        "MODEL_NAME = 'vit_base_patch16_224'\n",
        "NUM_CLASSES = 2  # MCI_CN is binary classification\n",
        "NUM_FRAMES = 16\n",
        "SAMPLING_RATE = 4\n",
        "INPUT_SIZE = 224\n",
        "PATCH_SIZE = 16\n",
        "TUBELET_SIZE = 2\n",
        "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Dataset configuration\n",
        "DATA_PATH = '/home/tianze/DATA_2T/MRI/csv_for_finetuning/Finalized_all/Downstreamtask_csv'\n",
        "TEST_CSV = 'A_MCI_CN_allscantest_split.csv'\n",
        "TEST_CSV_PATH = os.path.join(DATA_PATH, TEST_CSV)\n",
        "\n",
        "# Output directory for saving results\n",
        "OUTPUT_DIR = '/home/tianze/Code/VideoMAE/gradcam_comparison_results_bestvsbad'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "print(f\"Checkpoint 1: {CHECKPOINT_PATH_1}\")\n",
        "print(f\"Checkpoint 2: {CHECKPOINT_PATH_2}\")\n",
        "print(f\"Test CSV: {TEST_CSV_PATH}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GradCAM Implementation for Vision Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ViTGradCAMHook:\n",
        "    \"\"\"Hook class for storing activations and gradients, similar to vision_transformer_gradcam.py\"\"\"\n",
        "    def __init__(self):\n",
        "        self.activations = None\n",
        "        self.gradients = None\n",
        "\n",
        "    def save_activation(self, module, input, output):\n",
        "        \"\"\"Save activations during forward pass\"\"\"\n",
        "        self.activations = output.detach()\n",
        "\n",
        "    def save_gradient(self, grad):\n",
        "        \"\"\"Save gradients during backward pass\"\"\"\n",
        "        self.gradients = grad\n",
        "\n",
        "\n",
        "class GradCAM:\n",
        "    \"\"\"GradCAM implementation for Vision Transformers, following vision_transformer_gradcam.py pattern\"\"\"\n",
        "    \n",
        "    def __init__(self, model: nn.Module, target_layer: nn.Module):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.hook = ViTGradCAMHook()\n",
        "        self.hook_handle = None\n",
        "        \n",
        "        # Register forward hook to save activations\n",
        "        def forward_hook(module, input, output):\n",
        "            # Save activations (detached, similar to vision_transformer_gradcam.py)\n",
        "            self.hook.save_activation(module, input, output)\n",
        "            \n",
        "            # Register backward hook on the output tensor to capture gradients\n",
        "            # This is done during forward pass, similar to vision_transformer_gradcam.py\n",
        "            output.register_hook(self.hook.save_gradient)\n",
        "        \n",
        "        self.hook_handle = self.target_layer.register_forward_hook(forward_hook)\n",
        "    \n",
        "    def __del__(self):\n",
        "        \"\"\"Remove hooks when object is deleted\"\"\"\n",
        "        if self.hook_handle is not None:\n",
        "            self.hook_handle.remove()\n",
        "    \n",
        "    def generate_cam(self, input_tensor: torch.Tensor, target_class: Optional[int] = None) -> Tuple[np.ndarray, int, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Generate GradCAM heatmap\n",
        "        \n",
        "        Args:\n",
        "            input_tensor: Input video tensor [B, C, T, H, W]\n",
        "            target_class: Target class index. If None, uses the predicted class.\n",
        "        \n",
        "        Returns:\n",
        "            Tuple of (cam, target_class, output)\n",
        "            cam: Heatmap as numpy array [num_patches]\n",
        "            target_class: Target class index used\n",
        "            output: Model output logits\n",
        "        \"\"\"\n",
        "        # Reset gradients and activations\n",
        "        self.hook.activations = None\n",
        "        self.hook.gradients = None\n",
        "        \n",
        "        # Set model to eval mode but enable gradients\n",
        "        self.model.eval()\n",
        "        # Ensure all parameters require grad for gradient computation\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = True\n",
        "        \n",
        "        # Ensure input requires grad\n",
        "        if not input_tensor.requires_grad:\n",
        "            input_tensor = input_tensor.requires_grad_(True)\n",
        "        \n",
        "        # Zero gradients before forward pass\n",
        "        self.model.zero_grad()\n",
        "        if input_tensor.grad is not None:\n",
        "            input_tensor.grad.zero_()\n",
        "        \n",
        "        # Forward pass\n",
        "        output = self.model(input_tensor)\n",
        "        \n",
        "        # Get target class\n",
        "        if target_class is None:\n",
        "            target_class = output.argmax(dim=1).item()\n",
        "        \n",
        "        # Backward pass using same logic as eval.py\n",
        "        # Create one-hot vector for target class\n",
        "        one_hot = torch.zeros_like(output)\n",
        "        one_hot[0, target_class] = 1  # For single sample, sample_idx=0\n",
        "        \n",
        "        # Clear previous gradients\n",
        "        if self.hook.gradients is not None:\n",
        "            self.hook.gradients = None\n",
        "        \n",
        "        # Backward pass\n",
        "        output.backward(gradient=one_hot, retain_graph=False)\n",
        "        \n",
        "        # Get gradients and activations from hook\n",
        "        gradients = self.hook.gradients  # [B, N, C] where N is num_patches (including CLS token if present)\n",
        "        activations = self.hook.activations  # [B, N, C] (already detached)\n",
        "        \n",
        "        if gradients is None or activations is None:\n",
        "            raise ValueError(f\"Gradients or activations not captured. Gradients: {gradients is not None}, Activations: {activations is not None}\")\n",
        "        \n",
        "        # Detach gradients from graph for CAM computation\n",
        "        gradients = gradients.detach()\n",
        "\n",
        "        # Remove batch dimension for single sample\n",
        "        # activations, gradients: [B, N, C] -> [N, C]\n",
        "        activations = activations[0]\n",
        "        gradients = gradients[0]\n",
        "\n",
        "        # Note: VideoMAE typically doesn't use CLS tokens, so we use all tokens\n",
        "        # If your model uses CLS tokens, you would drop the first token here:\n",
        "        # activations = activations[1:, :]\n",
        "        # gradients = gradients[1:, :]\n",
        "\n",
        "        # Compute channel-wise weights by averaging gradients over \"spatial\" tokens\n",
        "        # This mirrors the CNN Grad-CAM pattern: mean over HxW -> weight per channel.\n",
        "        # gradients: [N_patches, C] -> weights: [C]\n",
        "        weights = gradients.mean(dim=0)  # [C]\n",
        "\n",
        "        # Weighted sum of features over channels to get importance per patch\n",
        "        # activations: [N_patches, C], weights: [C]\n",
        "        cam = (activations * weights.unsqueeze(0)).sum(dim=1)  # [N_patches]\n",
        "        cam = F.relu(cam)  # Apply ReLU to get positive contributions only\n",
        "\n",
        "        # Normalize\n",
        "        cam_np = cam.cpu().numpy()\n",
        "        if cam_np.max() > cam_np.min():\n",
        "            cam_np = (cam_np - cam_np.min()) / (cam_np.max() - cam_np.min() + 1e-8)\n",
        "        else:\n",
        "            cam_np = np.zeros_like(cam_np)\n",
        "\n",
        "        return cam_np, target_class, output\n",
        "    \n",
        "    def get_activations_and_gradients(self, input_tensor: torch.Tensor, target_class: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor, int, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Get raw activations and gradients for saving (similar to eval.py)\n",
        "        \n",
        "        Args:\n",
        "            input_tensor: Input video tensor [B, C, T, H, W]\n",
        "            target_class: Target class index. If None, uses the predicted class.\n",
        "        \n",
        "        Returns:\n",
        "            Tuple of (activations, gradients, target_class, output)\n",
        "            activations: [N, C] tensor (already detached)\n",
        "            gradients: [N, C] tensor (detached)\n",
        "            target_class: Target class index used\n",
        "            output: Model output logits\n",
        "        \"\"\"\n",
        "        # Reset gradients and activations\n",
        "        self.hook.activations = None\n",
        "        self.hook.gradients = None\n",
        "        \n",
        "        # Set model to eval mode but enable gradients\n",
        "        self.model.eval()\n",
        "        # Ensure all parameters require grad for gradient computation\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = True\n",
        "        \n",
        "        # Ensure input requires grad\n",
        "        if not input_tensor.requires_grad:\n",
        "            input_tensor = input_tensor.requires_grad_(True)\n",
        "        \n",
        "        # Zero gradients before forward pass\n",
        "        self.model.zero_grad()\n",
        "        if input_tensor.grad is not None:\n",
        "            input_tensor.grad.zero_()\n",
        "        \n",
        "        # Forward pass\n",
        "        output = self.model(input_tensor)\n",
        "        \n",
        "        # Get target class\n",
        "        if target_class is None:\n",
        "            target_class = output.argmax(dim=1).item()\n",
        "        \n",
        "        # Backward pass using same logic as eval.py\n",
        "        # Create one-hot vector for target class\n",
        "        one_hot = torch.zeros_like(output)\n",
        "        one_hot[0, target_class] = 1  # For single sample, sample_idx=0\n",
        "        \n",
        "        # Clear previous gradients\n",
        "        if self.hook.gradients is not None:\n",
        "            self.hook.gradients = None\n",
        "        \n",
        "        # Backward pass\n",
        "        output.backward(gradient=one_hot, retain_graph=False)\n",
        "        \n",
        "        # Get gradients and activations from hook\n",
        "        gradients = self.hook.gradients  # [B, N, C]\n",
        "        activations = self.hook.activations  # [B, N, C] (already detached)\n",
        "        \n",
        "        if gradients is None or activations is None:\n",
        "            raise ValueError(f\"Gradients or activations not captured. Gradients: {gradients is not None}, Activations: {activations is not None}\")\n",
        "        \n",
        "        # Detach gradients and remove batch dimension\n",
        "        gradients = gradients[0].detach()  # [N, C]\n",
        "        activations = activations[0]  # [N, C] (already detached)\n",
        "        \n",
        "        return activations, gradients, target_class, output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model(checkpoint_path: str, model_name: str, num_classes: int, \n",
        "               num_frames: int, tubelet_size: int) -> nn.Module:\n",
        "    \"\"\"Load VideoMAE model from checkpoint\"\"\"\n",
        "    \n",
        "    # Create model\n",
        "    model = create_model(\n",
        "        model_name,\n",
        "        pretrained=False,\n",
        "        num_classes=num_classes,\n",
        "        all_frames=num_frames,\n",
        "        tubelet_size=tubelet_size,\n",
        "        fc_drop_rate=0.0,\n",
        "        drop_rate=0.0,\n",
        "        drop_path_rate=0.1,\n",
        "    )\n",
        "    \n",
        "    # Load checkpoint\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "        \n",
        "        # Handle different checkpoint formats\n",
        "        if 'model' in checkpoint:\n",
        "            checkpoint_model = checkpoint['model']\n",
        "        elif 'module' in checkpoint:\n",
        "            checkpoint_model = checkpoint['module']\n",
        "        else:\n",
        "            checkpoint_model = checkpoint\n",
        "        \n",
        "        # Remove 'head' if shape mismatch\n",
        "        state_dict = model.state_dict()\n",
        "        for k in ['head.weight', 'head.bias']:\n",
        "            if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:\n",
        "                print(f\"Removing key {k} from pretrained checkpoint\")\n",
        "                del checkpoint_model[k]\n",
        "        \n",
        "        # Remove 'backbone.' or 'encoder.' prefix if present\n",
        "        all_keys = list(checkpoint_model.keys())\n",
        "        new_dict = OrderedDict()\n",
        "        for key in all_keys:\n",
        "            if key.startswith('backbone.'):\n",
        "                new_dict[key[9:]] = checkpoint_model[key]\n",
        "            elif key.startswith('encoder.'):\n",
        "                new_dict[key[8:]] = checkpoint_model[key]\n",
        "            else:\n",
        "                new_dict[key] = checkpoint_model[key]\n",
        "        checkpoint_model = new_dict\n",
        "        \n",
        "        # Load state dict\n",
        "        model.load_state_dict(checkpoint_model, strict=False)\n",
        "        print(\"Checkpoint loaded successfully\")\n",
        "    else:\n",
        "        print(f\"Warning: Checkpoint not found at {checkpoint_path}\")\n",
        "    \n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Note: Models will be loaded one by one to save GPU memory\n",
        "# Model 1 will be loaded first, then Model 2 after processing Model 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Preprocess Video Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading test dataset...\n",
            "Dataset initialized with 292 samples for mode: test\n",
            "Number of the class = 2\n",
            "Test dataset loaded. Total samples: 292\n"
          ]
        }
      ],
      "source": [
        "# Load MCI_CN test dataset using build_dataset (same as finetune notebook)\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.data_set = 'MCI_CN'\n",
        "        self.data_path = DATA_PATH\n",
        "        self.nb_classes = NUM_CLASSES\n",
        "        self.num_frames = NUM_FRAMES\n",
        "        self.sampling_rate = SAMPLING_RATE\n",
        "        self.input_size = INPUT_SIZE\n",
        "        self.short_side_size = INPUT_SIZE\n",
        "        self.test_num_segment = 1\n",
        "        self.test_num_crop = 1\n",
        "        self.reprob = 0.0\n",
        "        self.aa = None\n",
        "        self.smoothing = 0.0\n",
        "\n",
        "args = Args()\n",
        "\n",
        "# Build test dataset\n",
        "print('Loading test dataset...')\n",
        "test_dataset, _ = build_dataset(is_train=False, test_mode=True, args=args)\n",
        "print(f'Test dataset loaded. Total samples: {len(test_dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combine Results and Visualize Comparisons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "SAVING CLASS-SPECIFIC GRAD-CAM FOR MODEL 1\n",
            "============================================================\n",
            "Loading Model 1...\n",
            "Loading checkpoint from /home/tianze/Code/VideoMAE/checkpoints/final_experiments/K400-MRI-cov_loss_comb_diag-linear_probe/checkpoint-best.pth\n",
            "Checkpoint loaded successfully\n",
            "Model 1 loaded and GradCAM initialized\n",
            "Processing 292 samples to collect class-specific gradients/activations...\n",
            "Processed 50/292 samples\n",
            "Processed 100/292 samples\n",
            "Processed 150/292 samples\n",
            "Processed 200/292 samples\n",
            "Processed 250/292 samples\n",
            "\n",
            "Computing class-conditional average Grad-CAM for 2 classes...\n",
            "Processing class 0 with 213 samples\n",
            "Saved class 0 Grad-CAM to /home/tianze/Code/VideoMAE/gradcam_comparison_results_bestvsbad/gradcam_model1_class_0.pt\n",
            "  - avg_activations shape: torch.Size([1568, 768])\n",
            "  - avg_gradients shape: torch.Size([1568, 768])\n",
            "  - num_samples: 213\n",
            "\n",
            "Processing class 1 with 79 samples\n",
            "Saved class 1 Grad-CAM to /home/tianze/Code/VideoMAE/gradcam_comparison_results_bestvsbad/gradcam_model1_class_1.pt\n",
            "  - avg_activations shape: torch.Size([1568, 768])\n",
            "  - avg_gradients shape: torch.Size([1568, 768])\n",
            "  - num_samples: 79\n",
            "\n",
            "Model 1 cleared from GPU memory\n",
            "\n",
            "============================================================\n",
            "SAVING CLASS-SPECIFIC GRAD-CAM FOR MODEL 2\n",
            "============================================================\n",
            "Loading Model 2...\n",
            "Loading checkpoint from /home/tianze/Code/VideoMAE/checkpoints/final_experiments/K400-MRI-MCI_CN-finetune-linear_probe/checkpoint-49.pth\n",
            "Checkpoint loaded successfully\n",
            "Model 2 loaded and GradCAM initialized\n",
            "Processing 292 samples to collect class-specific gradients/activations...\n",
            "Processed 50/292 samples\n",
            "Processed 100/292 samples\n",
            "Processed 150/292 samples\n",
            "Processed 200/292 samples\n",
            "Processed 250/292 samples\n",
            "\n",
            "Computing class-conditional average Grad-CAM for 2 classes...\n",
            "Processing class 0 with 265 samples\n",
            "Saved class 0 Grad-CAM to /home/tianze/Code/VideoMAE/gradcam_comparison_results_bestvsbad/gradcam_model2_class_0.pt\n",
            "  - avg_activations shape: torch.Size([1568, 768])\n",
            "  - avg_gradients shape: torch.Size([1568, 768])\n",
            "  - num_samples: 265\n",
            "\n",
            "Processing class 1 with 27 samples\n",
            "Saved class 1 Grad-CAM to /home/tianze/Code/VideoMAE/gradcam_comparison_results_bestvsbad/gradcam_model2_class_1.pt\n",
            "  - avg_activations shape: torch.Size([1568, 768])\n",
            "  - avg_gradients shape: torch.Size([1568, 768])\n",
            "  - num_samples: 27\n",
            "\n",
            "Model 2 cleared from GPU memory\n",
            "\n",
            "============================================================\n",
            "All class-specific Grad-CAM data saved!\n",
            "Saved files can be interpreted using read_tensor.py\n",
            "Output directory: /home/tianze/Code/VideoMAE/gradcam_comparison_results_bestvsbad\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "## Save Class-Specific Gradients/Activations for Later Interpretation\n",
        "\n",
        "# This cell processes all test samples and saves class-specific averaged gradients/activations\n",
        "# similar to eval.py, which can then be interpreted using read_tensor.py\n",
        "\n",
        "def save_class_specific_gradcam(test_dataset, gradcam, model_name, output_dir, num_samples=None):\n",
        "    \"\"\"\n",
        "    Process test samples and save class-specific averaged gradients/activations\n",
        "    \n",
        "    Args:\n",
        "        test_dataset: Test dataset\n",
        "        gradcam: GradCAM instance\n",
        "        model_name: Name identifier for the model (e.g., 'model1', 'model2')\n",
        "        output_dir: Directory to save the results\n",
        "        num_samples: Number of samples to process (None for all)\n",
        "    \"\"\"\n",
        "    import torch\n",
        "    import os\n",
        "    \n",
        "    num_samples = num_samples or len(test_dataset)\n",
        "    \n",
        "    # Initialize accumulators per class for class-conditional Grad-CAM\n",
        "    class_gradients = {}  # {class_id: [list of gradients]}\n",
        "    class_activations = {}  # {class_id: [list of activations]}\n",
        "    class_counts = {}\n",
        "    class_inputs = {}  # Store representative inputs per class\n",
        "    \n",
        "    print(f\"Processing {num_samples} samples to collect class-specific gradients/activations...\")\n",
        "    \n",
        "    for idx in range(min(num_samples, len(test_dataset))):\n",
        "        try:\n",
        "            # Load sample\n",
        "            sample = test_dataset[idx]\n",
        "            video_tensor = sample[0]  # Already in [C, T, H, W] format\n",
        "            if len(video_tensor.shape) == 4:\n",
        "                video_tensor = video_tensor.unsqueeze(0)  # Add batch dimension [1, C, T, H, W]\n",
        "            video_tensor = video_tensor.to(DEVICE)\n",
        "            label = sample[1] if len(sample) > 1 else None\n",
        "            \n",
        "            # Get activations and gradients for the predicted class\n",
        "            activations, gradients, pred_class, output = gradcam.get_activations_and_gradients(\n",
        "                video_tensor, target_class=None\n",
        "            )\n",
        "            \n",
        "            # Use predicted class for grouping (you can change to label if you want true class)\n",
        "            target_class = pred_class.item() if isinstance(pred_class, torch.Tensor) else pred_class\n",
        "            \n",
        "            # Initialize class storage if needed\n",
        "            if target_class not in class_gradients:\n",
        "                class_gradients[target_class] = []\n",
        "                class_activations[target_class] = []\n",
        "                class_counts[target_class] = 0\n",
        "                class_inputs[target_class] = None\n",
        "            \n",
        "            # Store sample data (move to CPU)\n",
        "            class_activations[target_class].append(activations.cpu())\n",
        "            class_gradients[target_class].append(gradients.cpu())\n",
        "            class_counts[target_class] += 1\n",
        "            \n",
        "            # Store a representative input for this class (first occurrence)\n",
        "            if class_inputs[target_class] is None:\n",
        "                class_inputs[target_class] = video_tensor[0].detach().cpu()\n",
        "            \n",
        "            if (idx + 1) % 50 == 0:\n",
        "                print(f\"Processed {idx + 1}/{num_samples} samples\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing sample {idx}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            continue\n",
        "    \n",
        "    # Compute class-conditional average Grad-CAM after processing all samples\n",
        "    print(f\"\\nComputing class-conditional average Grad-CAM for {len(class_gradients)} classes...\")\n",
        "    \n",
        "    for class_id in sorted(class_gradients.keys()):\n",
        "        if len(class_gradients[class_id]) > 0:\n",
        "            print(f\"Processing class {class_id} with {class_counts[class_id]} samples\")\n",
        "            \n",
        "            # Stack all activations and gradients for this class\n",
        "            stacked_activations = torch.stack(class_activations[class_id])  # [num_samples, N, C]\n",
        "            stacked_gradients = torch.stack(class_gradients[class_id])  # [num_samples, N, C]\n",
        "            \n",
        "            # Compute average for this class\n",
        "            avg_activations = torch.mean(stacked_activations, dim=0)  # [N, C]\n",
        "            avg_gradients = torch.mean(stacked_gradients, dim=0)  # [N, C]\n",
        "            \n",
        "            # Save class-specific Grad-CAM (compatible with read_tensor.py format)\n",
        "            class_save_path = os.path.join(output_dir, f'gradcam_{model_name}_class_{class_id}.pt')\n",
        "            torch.save({\n",
        "                'avg_activations': avg_activations,  # [N, C] - compatible with read_tensor.py\n",
        "                'avg_gradients': avg_gradients,      # [N, C] - compatible with read_tensor.py\n",
        "                'input': class_inputs[class_id],      # Representative input [C, T, H, W]\n",
        "                'class_id': class_id,\n",
        "                'num_samples': class_counts[class_id],\n",
        "                'individual_activations': stacked_activations,  # [num_samples, N, C]\n",
        "                'individual_gradients': stacked_gradients       # [num_samples, N, C]\n",
        "            }, class_save_path)\n",
        "            \n",
        "            print(f\"Saved class {class_id} Grad-CAM to {class_save_path}\")\n",
        "            print(f\"  - avg_activations shape: {avg_activations.shape}\")\n",
        "            print(f\"  - avg_gradients shape: {avg_gradients.shape}\")\n",
        "            print(f\"  - num_samples: {class_counts[class_id]}\\n\")\n",
        "\n",
        "# Process Model 1\n",
        "print(\"=\" * 60)\n",
        "print(\"SAVING CLASS-SPECIFIC GRAD-CAM FOR MODEL 1\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load Model 1 if needed\n",
        "if 'model1' not in locals() or model1 is None:\n",
        "    print(\"Loading Model 1...\")\n",
        "    model1 = load_model(CHECKPOINT_PATH_1, MODEL_NAME, NUM_CLASSES, NUM_FRAMES, TUBELET_SIZE)\n",
        "    target_layer1 = model1.blocks[-1]\n",
        "    gradcam1 = GradCAM(model1, target_layer1)\n",
        "    print(\"Model 1 loaded and GradCAM initialized\")\n",
        "\n",
        "save_class_specific_gradcam(\n",
        "    test_dataset, gradcam1, 'model1', OUTPUT_DIR, num_samples=None\n",
        ")\n",
        "\n",
        "# Clear Model 1 from GPU memory\n",
        "del model1, gradcam1\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Model 1 cleared from GPU memory\")\n",
        "\n",
        "# Process Model 2\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SAVING CLASS-SPECIFIC GRAD-CAM FOR MODEL 2\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load Model 2\n",
        "print(\"Loading Model 2...\")\n",
        "model2 = load_model(CHECKPOINT_PATH_2, MODEL_NAME, NUM_CLASSES, NUM_FRAMES, TUBELET_SIZE)\n",
        "target_layer2 = model2.blocks[-1]\n",
        "gradcam2 = GradCAM(model2, target_layer2)\n",
        "print(\"Model 2 loaded and GradCAM initialized\")\n",
        "\n",
        "save_class_specific_gradcam(\n",
        "    test_dataset, gradcam2, 'model2', OUTPUT_DIR, num_samples=None\n",
        ")\n",
        "\n",
        "# Clear Model 2 from GPU memory\n",
        "del model2, gradcam2\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Model 2 cleared from GPU memory\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"All class-specific Grad-CAM data saved!\")\n",
        "print(f\"Saved files can be interpreted using read_tensor.py\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_tensor(tensor_path,class_id=None,channels=768):\n",
        "    if class_id is None:\n",
        "        class_id = 'not_specified_class'\n",
        "    parentdir = os.path.dirname(tensor_path)\n",
        "    # Load the saved Grad-CAM tensors\n",
        "    data = torch.load(tensor_path)\n",
        "    # print(data.keys())\n",
        "    # print(data['input'].shape)\n",
        "\n",
        "    #%%\n",
        "    activations = data['avg_activations']  # shape: [1568, 768]\n",
        "    gradients = data['avg_gradients']      # shape: [1568, 768]\n",
        "    print('activations shape:', activations.shape)\n",
        "    print('gradients shape:', gradients.shape)\n",
        "\n",
        "    # Set your parameters (adjust as needed)\n",
        "    num_frames = 8\n",
        "    height = 14\n",
        "    width = 14\n",
        "\n",
        "    # Reshape to [num_frames, height, width, channels]\n",
        "    activations = activations.view(num_frames, height, width, channels)\n",
        "    gradients = gradients.view(num_frames, height, width, channels)\n",
        "\n",
        "    # Average gradients over spatial dimensions (frames, height, width)\n",
        "    weights = gradients.mean(dim=(0, 1, 2))  # shape: [channels]\n",
        "\n",
        "    # Weighted sum of activations\n",
        "    cam = (weights * activations).sum(dim=3)  # shape: [num_frames, height, width]\n",
        "\n",
        "    # Take mean over frames if you want a single heatmap\n",
        "    cam = cam.cpu().numpy() if hasattr(cam, 'cpu') else np.array(cam)\n",
        "    print('min cam:', cam.min(), 'max cam:', cam.max())\n",
        "    # cam = np.maximum(cam, 0)  # ReLU\n",
        "\n",
        "    # Visualize Grad-CAM overlayed on input patches\n",
        "    input_tensor = data['input']  # shape: [3, 16, 224, 224]\n",
        "    print('input shape:', input_tensor.shape)\n",
        "\n",
        "    # Convert input to numpy\n",
        "    input_np = input_tensor.cpu().numpy() if hasattr(input_tensor, 'cpu') else np.array(input_tensor)\n",
        "\n",
        "    frames_per_patch = input_np.shape[1] // num_frames  # 16 // 8 = 2\n",
        "    # Compute frame importance scores\n",
        "    frame_importance = []\n",
        "\n",
        "    for i in range(num_frames):\n",
        "        ###### Grad-CAM magnitude per frame\n",
        "        # Method 1: Sum of all Grad-CAM values in the frame\n",
        "        frame_importance_sum = cam[i].sum()\n",
        "        # Store the importance score (choose one method)\n",
        "        frame_importance.append(frame_importance_sum)  # or use any other method\n",
        "\n",
        "        ##### end Grad-CAM magnitude per frame\n",
        "        # Grad-CAM heatmap\n",
        "        frame_cam = cam[i].astype(np.float32)\n",
        "        frame_cam -= frame_cam.min()\n",
        "        if frame_cam.max() > 0:\n",
        "            frame_cam /= frame_cam.max()\n",
        "        frame_cam_resized = cv2.resize(frame_cam, (224, 224))\n",
        "\n",
        "        # Corresponding input frames (mean over the patch)\n",
        "        start = i * frames_per_patch\n",
        "        end = (i + 1) * frames_per_patch\n",
        "        input_patch = input_np[:, start:end, :, :]  # shape: [3, 2, 224, 224]\n",
        "        input_patch_mean = input_patch.mean(axis=1)  # shape: [3, 224, 224]\n",
        "        input_patch_img = np.transpose(input_patch_mean, (1, 2, 0))  # [224, 224, 3]\n",
        "        input_patch_img = (input_patch_img - input_patch_img.min()) / (input_patch_img.max() - input_patch_img.min() + 1e-8)\n",
        "\n",
        "        # Rotate image and heatmap 180 degrees\n",
        "        input_patch_img = np.rot90(input_patch_img, 2)  # 180-degree rotation\n",
        "        frame_cam_resized = np.rot90(frame_cam_resized, 2)  # 180-degree rotation\n",
        "        \n",
        "        # Plot input and Grad-CAM side by side\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(input_patch_img)\n",
        "        plt.title(f'Input Patch Mean {i} ({start}-{end-1})')\n",
        "        plt.axis('off')\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(input_patch_img)\n",
        "        plt.imshow(frame_cam_resized, cmap='jet', alpha=0.5)\n",
        "        plt.title(f'Grad-CAM Patch {i}')\n",
        "        plt.axis('off')\n",
        "        plt.colorbar(fraction=0.046, pad=0.04)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{parentdir}/class_{class_id}_gradcam_patch_{i}.png')\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "def read_tensor_comparison(tensor_path_model1, tensor_path_model2, class_id=None, channels=768):\n",
        "    \"\"\"\n",
        "    Compare Grad-CAM from two models side by side\n",
        "    \n",
        "    Args:\n",
        "        tensor_path_model1: Path to model1 Grad-CAM .pt file\n",
        "        tensor_path_model2: Path to model2 Grad-CAM .pt file\n",
        "        class_id: Class identifier for naming\n",
        "        channels: Number of channels (default 768)\n",
        "    \"\"\"\n",
        "    if class_id is None:\n",
        "        class_id = 'comparison'\n",
        "    parentdir = os.path.dirname(tensor_path_model1)\n",
        "    \n",
        "    # Load both models' data\n",
        "    data1 = torch.load(tensor_path_model1)\n",
        "    data2 = torch.load(tensor_path_model2)\n",
        "    \n",
        "    # Process Model 1\n",
        "    activations1 = data1['avg_activations']  # shape: [1568, 768]\n",
        "    gradients1 = data1['avg_gradients']      # shape: [1568, 768]\n",
        "    \n",
        "    # Process Model 2\n",
        "    activations2 = data2['avg_activations']  # shape: [1568, 768]\n",
        "    gradients2 = data2['avg_gradients']      # shape: [1568, 768]\n",
        "    \n",
        "    print('Model 1 - activations shape:', activations1.shape, 'gradients shape:', gradients1.shape)\n",
        "    print('Model 2 - activations shape:', activations2.shape, 'gradients shape:', gradients2.shape)\n",
        "    \n",
        "    # Set your parameters (adjust as needed)\n",
        "    num_frames = 8\n",
        "    height = 14\n",
        "    width = 14\n",
        "    \n",
        "    # Reshape Model 1\n",
        "    activations1 = activations1.view(num_frames, height, width, channels)\n",
        "    gradients1 = gradients1.view(num_frames, height, width, channels)\n",
        "    weights1 = gradients1.mean(dim=(0, 1, 2))  # shape: [channels]\n",
        "    cam1 = (weights1 * activations1).sum(dim=3)  # shape: [num_frames, height, width]\n",
        "    cam1 = cam1.cpu().numpy() if hasattr(cam1, 'cpu') else np.array(cam1)\n",
        "    \n",
        "    # Reshape Model 2\n",
        "    activations2 = activations2.view(num_frames, height, width, channels)\n",
        "    gradients2 = gradients2.view(num_frames, height, width, channels)\n",
        "    weights2 = gradients2.mean(dim=(0, 1, 2))  # shape: [channels]\n",
        "    cam2 = (weights2 * activations2).sum(dim=3)  # shape: [num_frames, height, width]\n",
        "    cam2 = cam2.cpu().numpy() if hasattr(cam2, 'cpu') else np.array(cam2)\n",
        "    \n",
        "    print('Model 1 CAM - min:', cam1.min(), 'max:', cam1.max())\n",
        "    print('Model 2 CAM - min:', cam2.min(), 'max:', cam2.max())\n",
        "    \n",
        "    # Use input from model1 (should be same for both models)\n",
        "    input_tensor = data1['input']  # shape: [3, 16, 224, 224]\n",
        "    print('input shape:', input_tensor.shape)\n",
        "    \n",
        "    # Convert input to numpy\n",
        "    input_np = input_tensor.cpu().numpy() if hasattr(input_tensor, 'cpu') else np.array(input_tensor)\n",
        "    \n",
        "    frames_per_patch = input_np.shape[1] // num_frames  # 16 // 8 = 2\n",
        "    \n",
        "    for i in range(num_frames):\n",
        "        # Process Model 1 CAM\n",
        "        frame_cam1 = cam1[i].astype(np.float32)\n",
        "        frame_cam1 -= frame_cam1.min()\n",
        "        if frame_cam1.max() > 0:\n",
        "            frame_cam1 /= frame_cam1.max()\n",
        "        frame_cam1_resized = cv2.resize(frame_cam1, (224, 224))\n",
        "        \n",
        "        # Process Model 2 CAM\n",
        "        frame_cam2 = cam2[i].astype(np.float32)\n",
        "        frame_cam2 -= frame_cam2.min()\n",
        "        if frame_cam2.max() > 0:\n",
        "            frame_cam2 /= frame_cam2.max()\n",
        "        frame_cam2_resized = cv2.resize(frame_cam2, (224, 224))\n",
        "        \n",
        "        # Corresponding input frames (mean over the patch)\n",
        "        start = i * frames_per_patch\n",
        "        end = (i + 1) * frames_per_patch\n",
        "        input_patch = input_np[:, start:end, :, :]  # shape: [3, 2, 224, 224]\n",
        "        input_patch_mean = input_patch.mean(axis=1)  # shape: [3, 224, 224]\n",
        "        input_patch_img = np.transpose(input_patch_mean, (1, 2, 0))  # [224, 224, 3]\n",
        "        input_patch_img = (input_patch_img - input_patch_img.min()) / (input_patch_img.max() - input_patch_img.min() + 1e-8)\n",
        "        \n",
        "        # Rotate image and heatmaps 180 degrees\n",
        "        input_patch_img = np.rot90(input_patch_img, 2)  # 180-degree rotation\n",
        "        frame_cam1_resized = np.rot90(frame_cam1_resized, 2)  # 180-degree rotation\n",
        "        frame_cam2_resized = np.rot90(frame_cam2_resized, 2)  # 180-degree rotation\n",
        "        \n",
        "        # Plot: Input | Input + Model1 GradCAM | Input + Model2 GradCAM\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        \n",
        "        # Column 1: Original input\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.imshow(input_patch_img)\n",
        "        plt.title(f'Input Patch {i}\\n({start}-{end-1})')\n",
        "        plt.axis('off')\n",
        "        \n",
        "        # Column 2: Input + Model1 GradCAM\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(input_patch_img)\n",
        "        plt.imshow(frame_cam1_resized, cmap='jet', alpha=0.5)\n",
        "        plt.title(f'Model 1 GradCAM\\nPatch {i}')\n",
        "        plt.axis('off')\n",
        "        \n",
        "        # Column 3: Input + Model2 GradCAM\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(input_patch_img)\n",
        "        plt.imshow(frame_cam2_resized, cmap='jet', alpha=0.5)\n",
        "        plt.title(f'Model 2 GradCAM\\nPatch {i}')\n",
        "        plt.axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{parentdir}/class_{class_id}_comparison_patch_{i}.png', dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "    \n",
        "    print(f\"Saved comparison visualizations to {parentdir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Comparing Model 1 vs Model 2 - Class 0\n",
            "============================================================\n",
            "Model 1 - activations shape: torch.Size([1568, 768]) gradients shape: torch.Size([1568, 768])\n",
            "Model 2 - activations shape: torch.Size([1568, 768]) gradients shape: torch.Size([1568, 768])\n",
            "Model 1 CAM - min: -0.0020994954 max: 0.0028355212\n",
            "Model 2 CAM - min: -0.0016992074 max: 0.002305457\n",
            "input shape: torch.Size([3, 16, 224, 224])\n",
            "Saved comparison visualizations to /home/tianze/Code/VideoMAE/gradcam_comparison_results_bestvsbad\n",
            "\n",
            "============================================================\n",
            "Comparing Model 1 vs Model 2 - Class 1\n",
            "============================================================\n",
            "Model 1 - activations shape: torch.Size([1568, 768]) gradients shape: torch.Size([1568, 768])\n",
            "Model 2 - activations shape: torch.Size([1568, 768]) gradients shape: torch.Size([1568, 768])\n",
            "Model 1 CAM - min: -0.0031615943 max: 0.002051248\n",
            "Model 2 CAM - min: -0.002597673 max: 0.0016390154\n",
            "input shape: torch.Size([3, 16, 224, 224])\n",
            "Saved comparison visualizations to /home/tianze/Code/VideoMAE/gradcam_comparison_results_bestvsbad\n",
            "\n",
            "============================================================\n",
            "All comparison visualizations complete!\n",
            "Check /home/tianze/Code/VideoMAE/gradcam_comparison_results_bestvsbad for saved images\n",
            "Files saved as: class_class0_comparison_patch_X.png and class_class1_comparison_patch_X.png\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Run read_tensor_comparison to visualize both models side by side\n",
        "# This will generate comparison visualizations showing: Input | Input + Model1 GradCAM | Input + Model2 GradCAM\n",
        "\n",
        "# Compare Class 0\n",
        "print(\"=\" * 60)\n",
        "print(\"Comparing Model 1 vs Model 2 - Class 0\")\n",
        "print(\"=\" * 60)\n",
        "tensor_path_model1_class0 = os.path.join(OUTPUT_DIR, 'gradcam_model1_class_0.pt')\n",
        "tensor_path_model2_class0 = os.path.join(OUTPUT_DIR, 'gradcam_model2_class_0.pt')\n",
        "if os.path.exists(tensor_path_model1_class0) and os.path.exists(tensor_path_model2_class0):\n",
        "    read_tensor_comparison(\n",
        "        tensor_path_model1_class0, \n",
        "        tensor_path_model2_class0, \n",
        "        class_id='class0', \n",
        "        channels=768\n",
        "    )\n",
        "else:\n",
        "    print(f\"Files not found:\")\n",
        "    if not os.path.exists(tensor_path_model1_class0):\n",
        "        print(f\"  - {tensor_path_model1_class0}\")\n",
        "    if not os.path.exists(tensor_path_model2_class0):\n",
        "        print(f\"  - {tensor_path_model2_class0}\")\n",
        "\n",
        "# Compare Class 1\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Comparing Model 1 vs Model 2 - Class 1\")\n",
        "print(\"=\" * 60)\n",
        "tensor_path_model1_class1 = os.path.join(OUTPUT_DIR, 'gradcam_model1_class_1.pt')\n",
        "tensor_path_model2_class1 = os.path.join(OUTPUT_DIR, 'gradcam_model2_class_1.pt')\n",
        "if os.path.exists(tensor_path_model1_class1) and os.path.exists(tensor_path_model2_class1):\n",
        "    read_tensor_comparison(\n",
        "        tensor_path_model1_class1, \n",
        "        tensor_path_model2_class1, \n",
        "        class_id='class1', \n",
        "        channels=768\n",
        "    )\n",
        "else:\n",
        "    print(f\"Files not found:\")\n",
        "    if not os.path.exists(tensor_path_model1_class1):\n",
        "        print(f\"  - {tensor_path_model1_class1}\")\n",
        "    if not os.path.exists(tensor_path_model2_class1):\n",
        "        print(f\"  - {tensor_path_model2_class1}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"All comparison visualizations complete!\")\n",
        "print(f\"Check {OUTPUT_DIR} for saved images\")\n",
        "print(\"Files saved as: class_class0_comparison_patch_X.png and class_class1_comparison_patch_X.png\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "videomae",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
