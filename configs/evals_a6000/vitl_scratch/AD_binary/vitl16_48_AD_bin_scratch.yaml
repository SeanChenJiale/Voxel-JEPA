nodes: 1
tasks_per_node: 1
tag: vitl16_brainslice48_AD_bin_Scratch
eval_name: video_classification_frozen
resume_checkpoint: false
data:
  dataset_train: /media/backup_16TB/sean/VJEPA/jepa/configs/evals_a6000/pretrain_maskfix_tiny_smaller/tiny_AD_binclass_slice48_mp4/Brainslice_48_ADclassification_train_final_filtered.csv
  dataset_val: /media/backup_16TB/sean/VJEPA/jepa/configs/evals_a6000/pretrain_maskfix_tiny_smaller/tiny_AD_binclass_slice48_mp4/Brainslice_48_ADclassification_val_final_filtered.csv
  dataset_type: VideoDataset
  num_classes: 2
  frames_per_clip: 16
  num_segments: 1
  num_views_per_segment: 1
  frame_step: 4
optimization:
  attend_across_segments: false
  num_epochs: 300
  resolution: 224
  batch_size: 6
  weight_decay: 0.01
  lr: 0.001
  start_lr: 0.001
  final_lr: 0.0
  warmup: 0.
  use_bfloat16: true
pretrain:
  model_name: vit_large
  checkpoint_key: target_encoder
  clip_duration: null
  frames_per_clip: 16
  tubelet_size: 2
  uniform_power: true
  
  use_silu: false
  tight_silu: false
  use_sdpa: true
  patch_size: 16
  folder: /media/backup_16TB/sean/VJEPA/a6000_output/vitl_scratch
  checkpoint: vitl16.pth.tar  # name of pretrained model file inside folder
  write_tag: vitl16_48_AD_bin_scratch
logging:
  project: voxel-jepa-fine-tuning # Don't change this
  run_name: vitl16_48_AD_bin_scratch # Please change every time before running, this is the name for wandb run file

  ##todo run tomorrow 9th april.
  #python -m evals.main --fname /media/backup_16TB/sean/VJEPA/jepa/configs/evals_a6000/vitl_scratch/AD_binary/vitl16_48_AD_bin_scratch.yaml --devices cuda:0 cuda:2>/media/backup_16TB/sean/VJEPA/a6000_output/vitl_scratch/vitl16_brainslice48_AD_bin_Scratch/AD_bin.log 2>&1
  